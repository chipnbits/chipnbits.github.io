---
title: "Lecture 5: Autodiff and Implementing Gauss-Newton"
subtitle: "A look at some of the foundations of automatic differentiation and the Gauss-Newton optimization method."
date: 2024-10-08
author: "Simon Ghyselincks"
description: >-
    Automatic differentiation is a powerful tool for solving optimization problems that can be used to automate the process of Gauss-Newton optimization. Here we put together an implementation of the Gauss-Newton method using PyTorch.
categories:
  - Optimization
  - Gauss-Newton
  - Automatic Differentiation
  - PyTorch

image: imgs/lotka_volterra_phase_space.png
draft: true

editor: 
  render-on-save: false

filters:
  - pseudocode

pseudocode:
  caption-prefix: "Algorithm"
  reference-prefix: "Algorithm"
  caption-number: true
---

{{< include /_macros.qmd >}}


# Automatic Differentiation

Returning to the Lotka-Volterra model, we can now use automatic differentiation to compute the Jacobian matrix of the forward model. In fact, it can be shown that we can perform Gauss-Newton optimization more efficiently by using the Jacobian-vector product (JVP) and the vector-Jacobian product (VJP) instead of the full Jacobian matrix, since in the algorithm what we are truly interested in is the product of the Jacobian with a vector or its transpose. This equates to a directional derivative.

### Application to the Lotka-Volterra Model

Take a forward model $F(p)$ for which we want a Jacobian matrix at $p_k$. We can write the Taylor expansion of the forward model as:

$$ F(p_k + \epsilon v) = F(p_k) + J_k \epsilon v + \mathcal{O}(\epsilon^2)$$

where $J_k$ is the Jacobian of $F(p_k)$. If we take the derivative of both sides in this expansion with respect to $\epsilon$ we get:

$$ \frac{d}{d \epsilon} F(p_k + \epsilon v) = J_k v + \mathcal{O}(\epsilon)$$

If we make $\epsilon$ very small then the Jacobian of the forward problem can be numerically approximated and bounded by a small $\mathcal{O}(\epsilon)$. The next step to fully recover the Jacobian is to take the gradient with respect to $v$ of the left-hand side of the equation. 

$$ \nabla_v \frac{d}{d \epsilon} F(p_k + \epsilon v) = J_k$$

The gradient with respect to $v$ can be traced through with automatic differentiation. So we apply a chain of operations, the `pytorch` Jacobian vector product, followed by backpropagation on a surrogate $v$ that was passed to the function to get the Jacobian of the forward model. The same principles can be used to recover $J_k^T$.

There is also the direct method that is avaible for computing the Jacobian matrix using the torch library. Both cases are shown below. Note that the tensors have a `requires_grad=True` flag set to allow for the gradients to be computed, it indicates that the tensor is part of the computational graph for backpropagation and tracing by how much each element of $v$ contributed to the `jvp` result.

The fundamental use of the jvp or the vjp is to compute the directional derivate or its transpose. This is because the jacobian matrix encodes the directional derivatives of the function at a point.

$$d_k = J_k^T v$$

```{python}
#| label: jacobian-vector-product
#| fig-cap: "The Jacobian vector product using automatic differentiation."
import torch
from torch.autograd.functional import jvp
from torch.autograd.functional import jacobian

# Define a simple forward function
def F(p):
    return torch.stack([p[0]**2 + p[1], p[1]**3 + p[0]])

# Input point p_k
p_k = torch.tensor([1.0, 1.0])

# Arbitrary vector v, same size as p_k
v = torch.tensor([1.0,1.0], requires_grad=True)

# Compute the Jacobian-vector product (J(p) * v)
F_output, jvp_result = jvp(F, (p_k,), v, create_graph=True)
print("Function output:")
print(F_output)
print("Jacobian-vector product:")
print(jvp_result)

# Initialize a list to store each row of the Jacobian
jacobian_rows = []
# Compute the gradient of each component of the JVP result separately, retaining the graph to avoid re-computation
for i in range(F_output.shape[0]):
    v.grad = None  # Clear the gradient
    jvp_result.backward(torch.tensor([1.0 if i == j else 0.0 for j in range(F_output.shape[0])]), retain_graph=True)
    jacobian_rows.append(v.grad.clone())  # Append the gradient (row of the Jacobian)

# Stack the rows to get the full Jacobian matrix
jacobian_matrix = torch.stack(jacobian_rows, dim=0)

# Print the Jacobian matrix
print("Jacobian matrix at p_k:")
print(jacobian_matrix)

# Compute the full Jacobian matrix directly
jacobian_matrix = jacobian(F, p_k)

# Print the Jacobian matrix
print("Jacobian matrix at p_k:")
print(jacobian_matrix)
```

# A Torch Implementation

Now all the previous steps can be combined to form a PyTorch training loop that will solve the non-linear least squares problem using the Gauss-Newton method using the conjugate gradient method to solve the normal equations involved.

To make the solution components easier to understand, they are seprated into different class objects that contain the necessary components for each part of the solution.

```{python}
#| fig-cap: "RK4 ODE solver and Lotka-Volterra model with trainable parameters."
#| code-fold: true
import torch
from torch import nn
import matplotlib.pyplot as plt

class RungeKutta4:
    """
    Runge-Kutta 4th Order Integrator for solving ODEs.
    """
    def __init__(self, func, time_steps, period):
        """
        Initializes the RK4 integrator.

        Args:
            func (callable): The function defining the ODE system, f(x, i).
            time_steps (int): Number of time steps to integrate over.
            period (list or tuple): [start_time, end_time].
        """
        self.func = func
        self.time_steps = time_steps
        self.start_time, self.end_time = period
        self.dt = (self.end_time - self.start_time) / self.time_steps

    def integrate(self, x0):
        """
        Performs the RK4 integration.

        Args:
            x0 (torch.Tensor): Initial state tensor of shape (n_vars,).

        Returns:
            torch.Tensor: Tensor containing the solution at each time step of shape (n_vars, time_steps + 1).
        """
        X = torch.zeros(x0.size(0), self.time_steps + 1)
        X[:, 0] = x0

        for i in range(self.time_steps):
            k1 = self.func(X[:, i], i)
            k2 = self.func(X[:, i] + self.dt * k1 / 2, i)
            k3 = self.func(X[:, i] + self.dt * k2 / 2, i)
            k4 = self.func(X[:, i] + self.dt * k3, i)
            X[:, i + 1] = X[:, i] + (self.dt / 6) * (k1 + 2 * k2 + 2 * k3 + k4)

        return X

class LotkaVolterra(nn.Module):
    """
    Lotka-Volterra (Predator-Prey) Model with Trainable Parameters.
    """
    def __init__(self, period, n_time_steps, perturbation=None):
        """
        Initializes the Lotka-Volterra model.

        Args:
            period (list or tuple): [start_time, end_time].
            n_time_steps (int): Number of time steps for integration.
            perturbation (torch.Tensor, optional): Tensor to perturb alpha parameters. Defaults to None.
        """
        super(LotkaVolterra, self).__init__()
        self.time_steps = n_time_steps
        self.period = period

        if perturbation is None:
            perturbation = torch.zeros(n_time_steps + 1)

        # Initialize trainable parameters
        self.alpha = nn.Parameter( (2/3) * torch.ones(n_time_steps + 1) + perturbation )
        self.beta = nn.Parameter( (4/3) * torch.ones(n_time_steps + 1) )
        self.gamma = nn.Parameter( torch.ones(n_time_steps + 1) )
        self.delta = nn.Parameter( torch.ones(n_time_steps + 1) )

    def predator_prey_derivatives(self, state, i):
        """
        Computes the derivatives for the Lotka-Volterra equations.

        Args:
            state (torch.Tensor): Current state tensor [prey, predator].
            i (int): Current time step index.

        Returns:
            torch.Tensor: Derivatives [dprey/dt, dpredator/dt].
        """
        prey, predator = state
        dprey_dt = self.alpha[i] * prey - self.beta[i] * prey * predator
        dpredator_dt = -self.gamma[i] * predator + self.delta[i] * prey * predator
        derivatives = torch.stack([dprey_dt, dpredator_dt])
        return derivatives

    def forward(self, x0):
        """
        Solves the Lotka-Volterra equations using RK4.

        Args:
            x0 (torch.Tensor): Initial state tensor [prey, predator].

        Returns:
            torch.Tensor: Solution tensor over time of shape (2, time_steps + 1).
        """
        rk4 = RungeKutta4(self.predator_prey_derivatives, self.time_steps, self.period)
        return rk4.integrate(x0)
```

Now to check the implementation, we can create a model and solve the Lotka-Volterra equations using the RK4 solver.

```{python}
period = [0, 40]
n_time_steps = 2000

# Initialize the Lotka-Volterra model
model = LotkaVolterra(period=period, n_time_steps=n_time_steps)

# Initial populations: [prey, predator]
initial_state = torch.rand(2) # Example initial conditions

# Perform integration
solution = model(initial_state)

# Plotting the results
plt.figure(figsize=(12, 8))

# Time series plot
plt.subplot(2, 1, 1)
plt.plot(solution[0, :].detach().numpy(), label='Prey')
plt.plot(solution[1, :].detach().numpy(), label='Predator')
plt.title('Time Series')
plt.xlabel('Time Step')
plt.ylabel('Population')
plt.legend()

# Phase space plot
plt.subplot(2, 1, 2)
plt.plot(solution[0, :].detach().numpy(), solution[1, :].detach().numpy())
plt.title('Phase Space')
plt.xlabel('Prey Population')
plt.ylabel('Predator Population')

plt.tight_layout()
plt.show()
```

N