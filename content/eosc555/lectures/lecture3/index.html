<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.53">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Simon Ghyselincks">
<meta name="dcterms.date" content="2024-09-17">
<meta name="description" content="In continuation of Lecture 2, we now look at an alternative approach to image denoising using gradient descent and early stopping. We will derive the least squares gradient descent algorithm and analyze it as an ordinary differential equation.">

<title>Lecture 3: Image Denoising with Gradient Descent and Early Stopping – Simon Ghyselincks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../../styles.css">
<meta property="og:title" content="Lecture 3 – Simon Ghyselincks">
<meta property="og:description" content="In continuation of Lecture 2, we now look at an alternative approach to image denoising using gradient descent and early stopping. We will derive the least squares gradient descent algorithm and analyze it as an ordinary differential equation.">
<meta property="og:image" content="https://chipnbits.github.io/content/eosc555/lectures/lecture3/imgs/gradient_flow.gif">
<meta property="og:site_name" content="Simon Ghyselincks">
<meta name="twitter:title" content="Lecture 3 – Simon Ghyselincks">
<meta name="twitter:description" content="In continuation of Lecture 2, we now look at an alternative approach to image denoising using gradient descent and early stopping. We will derive the least squares gradient descent algorithm and analyze it as an ordinary differential equation.">
<meta name="twitter:image" content="https://chipnbits.github.io/content/eosc555/lectures/lecture3/imgs/gradient_flow.gif">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">Simon Ghyselincks</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../content/about/biography.html"> 
<span class="menu-text">Bio</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../content/publications/index.html"> 
<span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../content/talks/index.html"> 
<span class="menu-text">Talks &amp; Teaching</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-projects" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Projects</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-projects">    
        <li>
    <a class="dropdown-item" href="../../../../content/eosc555/index.html">
 <span class="dropdown-text">EOSC 555</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../content/projects/RLUnicycle/introduction.html">
 <span class="dropdown-text">Learning to Balance</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../../../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#derivations-of-linear-algebra-gradients" id="toc-derivations-of-linear-algebra-gradients" class="nav-link active" data-scroll-target="#derivations-of-linear-algebra-gradients">Derivations of Linear Algebra Gradients</a></li>
  <li><a href="#gradient-descent-analysis" id="toc-gradient-descent-analysis" class="nav-link" data-scroll-target="#gradient-descent-analysis">Gradient Descent Analysis</a>
  <ul class="collapse">
  <li><a href="#example-for-image-recovery-using-analytic-solution" id="toc-example-for-image-recovery-using-analytic-solution" class="nav-link" data-scroll-target="#example-for-image-recovery-using-analytic-solution">Example for Image Recovery using Analytic Solution</a></li>
  </ul></li>
  <li><a href="#recovery-of-the-adjoint-operator-using-autograd" id="toc-recovery-of-the-adjoint-operator-using-autograd" class="nav-link" data-scroll-target="#recovery-of-the-adjoint-operator-using-autograd">Recovery of the Adjoint Operator using Autograd</a>
  <ul class="collapse">
  <li><a href="#explicit-computation-of-the-adjoint" id="toc-explicit-computation-of-the-adjoint" class="nav-link" data-scroll-target="#explicit-computation-of-the-adjoint">Explicit Computation of the Adjoint</a></li>
  <li><a href="#autograd-computation-of-the-adjoint" id="toc-autograd-computation-of-the-adjoint" class="nav-link" data-scroll-target="#autograd-computation-of-the-adjoint">Autograd Computation of the Adjoint</a></li>
  </ul></li>
  <li><a href="#gradient-descent-with-adjoint" id="toc-gradient-descent-with-adjoint" class="nav-link" data-scroll-target="#gradient-descent-with-adjoint">Gradient Descent with Adjoint</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Lecture 3: Image Denoising with Gradient Descent and Early Stopping</h1>
<p class="subtitle lead">A derivation of least squares gradient descent and ODE analysis</p>
  <div class="quarto-categories">
    <div class="quarto-category">Optimization</div>
    <div class="quarto-category">Inverse Theory</div>
    <div class="quarto-category">Python</div>
    <div class="quarto-category">Torch</div>
    <div class="quarto-category">Adjoint</div>
  </div>
  </div>

<div>
  <div class="description">
    In continuation of Lecture 2, we now look at an alternative approach to image denoising using gradient descent and early stopping. We will derive the least squares gradient descent algorithm and analyze it as an ordinary differential equation.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Simon Ghyselincks </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 17, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="derivations-of-linear-algebra-gradients" class="level2">
<h2 class="anchored" data-anchor-id="derivations-of-linear-algebra-gradients">Derivations of Linear Algebra Gradients</h2>
<p>Often times we wish to find the gradient of a multi-variable function that is formulated as a linear algebra operation. In this case there are some useful “vector” derivatives and rules that can simplify the process of calculating more complex expressions. The gradient with respect to vector <span class="math inline">\(\mathbf{x}\)</span> is generally denoted as <span class="math inline">\(\nabla_{\mathbf{x}}\)</span> or alternatively <span class="math inline">\(\partial_{\mathbf{x}}\)</span>, somewhat of an abuse of notation.</p>
<section id="a-warmup" class="level4">
<h4 class="anchored" data-anchor-id="a-warmup">1. A Warmup</h4>
<p><span class="math display">\[\phi(x) = a^\top x = \sum_i a_i x_i\]</span></p>
<p>This is a vector dotproduct and the gradient is simply the vector <span class="math inline">\(a\)</span>. There is a subtlety here in that the vector is usually transposed to be a column vector, but this is not always the case. Some people in the field of statistics prefer to use row vector, this can cause some confusion. The general convention is a column vector.</p>
<p><span class="math display">\[\nabla_{\mathbf{x}} \phi = a\]</span></p>
</section>
<section id="matrix-vector-multiplication" class="level4">
<h4 class="anchored" data-anchor-id="matrix-vector-multiplication">2. Matrix Vector Multiplication</h4>
<p><span class="math display">\[\phi(x) = Ax\]</span></p>
<p>Based on the previous process we are expecting to potentially get <span class="math inline">\(A^\top\)</span> as the gradient, however the transpose does not occur in this case because we are not returning a vector that needs to be reshaped into a column form.</p>
<p><span class="math display">\[\nabla_{\mathbf{x}} \phi = A\]</span></p>
</section>
<section id="quadratic-forms" class="level4">
<h4 class="anchored" data-anchor-id="quadratic-forms">3. Quadratic Forms</h4>
<p>Often we may encounter quadratic linear functions that are of the form: <span class="math display">\[ \phi(x) = x^\top A x\]</span></p>
<p>One way to determine the gradient is to expand the expression and evaluate for a single <span class="math inline">\(\frac{\partial}{\partial x_i}\)</span> term. This method can be found at <a href="https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/linearQuadraticGradients.pdf">Mark Schmidt Notes</a> Instead we can apply a chain rule for matrix differentiation that is based on the product rule for differentiation. The chain rule for matrix differentiation is as follows:</p>
<p><span class="math display">\[\frac{d f(g,h)}{d x} = \frac{d (g(x)^\top)}{d x} \frac{\partial f(g,h)}{\partial g} + \frac{d (h(x)^\top)}{d x} \frac{\partial f(g,h)}{\partial h}\]</span></p>
<p><span class="math display">\[ \begin {align*}
\phi(x) &amp;= x^\top A x \\
\nabla_{\mathbf{x}} \phi &amp;= \nabla_{\mathbf{x}} (x^\top A x) \\
&amp;= \nabla_{\mathbf{x}} x^\top (A x) =  \nabla_{\mathbf{x}} x^\top y\\
&amp;= (\nabla_{\mathbf{x}} x) \nabla_{\mathbf{x}} x^\top y + \nabla_{\mathbf{x}} y^\top \nabla_{\mathbf{y}} x^\top y\\
&amp;= I y + \nabla_{\mathbf{x}} (x^\top A^\top) x\\
&amp;= (A x) + A^\top x\\
&amp;= (A + A^\top) x
\end {align*}
\]</span></p>
<p>This fits with the generalization for a scalar quadratic form where we end up with <span class="math inline">\((cx^2)' = (c + c^\top)x = 2cx\)</span> where <span class="math inline">\(c\)</span> is a scalar.</p>
</section>
<section id="hadamard-product" class="level4">
<h4 class="anchored" data-anchor-id="hadamard-product">4. Hadamard Product</h4>
<p>Another form of interest is the hadamard product of two vectors. <span class="math display">\[\phi(x) = (Ax)^2 = Ax \odot Ax\]</span></p>
<p>For this one let <span class="math inline">\(y=Ax\)</span> and we can index each element of the vector <span class="math inline">\(y\)</span> as <span class="math inline">\(y_i = \sum_j A_{ij} x_j\)</span>. The hadamard product is a vector <span class="math inline">\(z\)</span> where <span class="math inline">\(z_i = y_i^2\)</span>, we can compute the jacobian since now we are taking the gradient with respect to a vector.</p>
<p>The Jacobian will contain the partial derivatives:</p>
<p><span class="math display">\[\frac{d\vec{z}}{d\vec{x}} = \begin{bmatrix} \frac{\partial z_1}{\partial x_1} &amp; \frac{\partial z_1}{\partial x_2} &amp; \cdots &amp; \frac{\partial z_1}{\partial x_n} \\
\frac{\partial z_2}{\partial x_1} &amp; \frac{\partial z_2}{\partial x_2} &amp; \cdots &amp; \frac{\partial z_2}{\partial x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial z_n}{\partial x_1} &amp; \frac{\partial z_n}{\partial x_2} &amp; \cdots &amp; \frac{\partial z_n}{\partial x_n} \end{bmatrix}
\]</span></p>
<p>If we can recover this then we have the gradient of the hadamard product.</p>
<p><span class="math display">\[
\begin{align*}
z_i &amp;= y_i^2 = \left( \sum_j A_{ij} x_j \right)^2\\
\frac{\partial}{\partial x_j} y_i^2 &amp;= 2 y_i \frac{\partial y_i}{\partial x_j} = 2 y_i A_{ij}\\
\frac{d\vec{z}}{d\vec{x}} &amp;= 2 \begin{bmatrix} y_1 A_{1j} &amp; y_1 A_{2j} &amp; \cdots &amp; y_1 A_{nj} \\
y_2 A_{1j} &amp; y_2 A_{2j} &amp; \cdots &amp; y_2 A_{nj} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
y_n A_{1j} &amp; y_n A_{2j} &amp; \cdots &amp; y_n A_{nj} \end{bmatrix}\\
&amp;= 2 \cdot \text{diag}(\vec{y})A\\
&amp;= 2 \cdot \text{diag}(Ax)A
\end{align*}
\]</span></p>
</section>
<section id="least-squares-gradient" class="level4">
<h4 class="anchored" data-anchor-id="least-squares-gradient">5. Least Squares Gradient</h4>
<p>We look at taking the gradient of the expansion of least squares to find the gradient for this optimization objective.</p>
<p><span class="math display">\[\phi(x) = \frac{1}{2} ||Ax - b||^2 = \frac{1}{2} (x^\top A^\top A x - 2 b^\top A x + b^\top b)\]</span></p>
<p><span class="math display">\[ \begin{align*}
\nabla_{\mathbf{x}} \phi &amp;= \nabla_{\mathbf{x}} \left( \frac{1}{2} (x^\top A^\top A x - 2 b^\top A x + b^\top b) \right)\\
&amp;= \nabla_{\mathbf{x}} \left( \frac{1}{2} x^\top A^\top A x \right) - \nabla_{\mathbf{x}} \left( b^\top A x \right)\\
&amp;= \frac{1}{2} (A^\top A + A^\top A) x - A^\top b\\
&amp;= A^\top A x - A^\top b\\
\end{align*}
\]</span></p>
<p>Returning to the first-order optimality condition we have: <span class="math display">\[A^\top A x = A^\top b\]</span></p>
<p>At which point it is in question if <span class="math inline">\(A^\top A\)</span> is invertible. The invertibility of <span class="math inline">\(A^\top A\)</span> is determined by the rank of <span class="math inline">\(A\)</span>. The rank of A for a non-square matrix is the number of independent columns. If we examine <span class="math inline">\(A^\top Ax = 0\)</span> then we see that this is only true where the range of <span class="math inline">\(A\)</span> is in the nullspace of <span class="math inline">\(A^\top\)</span>. But <span class="math inline">\(N(A^\top) = R(A)^\perp\)</span> so they are orthogonal subspaces and will never coincide unless <span class="math inline">\(Ax=0\)</span>. So then <span class="math inline">\(A^\top A x = 0\)</span> implies that <span class="math inline">\(Ax = 0\)</span> which means that if the null space of <span class="math inline">\(A=\{0\}\)</span> then the null space of <span class="math inline">\(A^\top A = \{0\}\)</span> and <span class="math inline">\(A^\top A\)</span> is invertible. Since <span class="math inline">\(A^\top A\)</span> is symmetric and positive definite, it is invertible.</p>
<p><span class="math inline">\(A^\top A\)</span> is invertible <span class="math inline">\(\iff\)</span> <span class="math inline">\(A\)</span> is full rank, that is all the columns are independent. For non-square matrices, an <span class="math inline">\(m&gt;n\)</span> matrix that is wide will trivially not satisfy this condition. A tall matrix <span class="math inline">\(m&lt;n\)</span> will satisfy the condition if the columns are independent.</p>
</section>
</section>
<section id="gradient-descent-analysis" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent-analysis">Gradient Descent Analysis</h2>
<p>The standard form of the gradient descent algorithm comes from the field of optimization and can be written as:</p>
<p><span class="math display">\[ x_{k+1} = x_k - \alpha \nabla_x \phi(x_k)\]</span></p>
<p>Where <span class="math inline">\(\alpha\)</span> is the learning rate, which can be dependent on the problem and the gradient. Substituting the gradient of the least squares problem we have:</p>
<p><span class="math display">\[ \begin{align}
x_{k+1} &amp;= x_k - \alpha (A^\top A x_k - A^\top b)\\
\frac{x_{k+1}-x_k}{\alpha} &amp;= A^\top b - A^\top A x_k\\
\lim_{\alpha \to 0} \frac{x_{k+1}-x_k}{\alpha} &amp;= \frac{dx}{dt} = A^\top (b -A x), \quad x(0) = x_0
\end{align}
\]</span></p>
<p>This ODE is the continuous version of the gradient descent algorithm, also known as the <em>gradient flow</em>. Since this a linear first-order ODE we can solve it analytically. The general method for a linear system ODE would be to find the homogeneous solution and the particular solution:</p>
<p><span class="math display">\[ \begin{align}
x' + A^\top A x &amp;= A^\top b\\
\text{Guess:} x &amp;= v e^{\lambda t}\\
\lambda v e^{\lambda t} + A^\top A v e^{\lambda t} &amp;= A^\top b e^{\lambda t}\\
\lambda v + A^\top A v &amp;= 0 \qquad \text{Homogeneous}\\
(\lambda I + A^\top A) v &amp;= 0\\
\lambda &amp;= \text{eigenvalues of } A^\top A, \quad v = \text{eigenvectors of } A^\top A
\end{align}
\]</span></p>
<p>Before continuing further with this line, we can see that the solutions will be closely related to the SVD because it contains the information on these eigenvalues and vectors. So we can try to solve the ODE with the SVD.</p>
<section id="solving-the-ode-with-svd" class="level4">
<h4 class="anchored" data-anchor-id="solving-the-ode-with-svd">Solving the ODE with SVD</h4>
<p><span class="math display">\[\begin{align}
A &amp;= U \Sigma V^\top\\
A^TA &amp;= V \Sigma^2 V^\top\\
\frac{d}{dt}x &amp;= V \Sigma U^\top b - V \Sigma^2 V^\top x\\
\end{align}
\]</span></p>
<p>Now let <span class="math inline">\(z = V^\top x\)</span> and <span class="math inline">\(\hat b = U ^ \top b\)</span> then we have:</p>
<p><span class="math display">\[\begin{align}
\frac{d}{dt} (V^\top x) &amp;= \Sigma \hat b - \Sigma^2 (V^\top x)\\
\frac{d}{dt} z &amp;= \Sigma \hat b - \Sigma^2 z\\
z' + \Sigma^2 z &amp;= \Sigma \hat b\\
\end{align}
\]</span></p>
<p>At this stage since everything has been diagonalized, all of the equations are decoupled and independent so we can solve for the <span class="math inline">\(\lambda_i\)</span> cases independently. We find the homogeneous <span class="math inline">\(z_h\)</span> and particular <span class="math inline">\(z_p\)</span> solutions:</p>
<p><span class="math display">\[
\begin{align}
z_h' + \lambda^2 z_h &amp;= 0\\
z_h &amp;= c e^{-\lambda^2 t}\\
z_p' + \lambda^2 z_p &amp;= \lambda \hat b\\
z_p &amp;= D \hat b \\
\lambda^2 D \hat b &amp;= \lambda \hat b\\
D &amp;= \frac{1}{\lambda}\\
z_p &amp;= \frac{1}{\lambda} \hat b
\end{align}
\]</span></p>
<p>So the general solution for the <span class="math inline">\(i^{th}\)</span> component is:</p>
<p><span class="math display">\[z_i = c_i e^{-\lambda_i^2 t} + \frac{1}{\lambda_i} \hat b_i\]</span></p>
<p>Supposing that we start at <span class="math inline">\(x=0\)</span> then we have <span class="math inline">\(z=0\)</span> at all elements and can solve the coefficients <span class="math inline">\(c_i\)</span>:</p>
<p><span class="math display">\[c_i = -\frac{1}{\lambda_i} \hat b_i\]</span></p>
<p>Then putting it all back together with all the equations we have that</p>
<p><span class="math display">\[Z = \text{diag}\left( \lambda_i^{-1} (1 - \exp (-\lambda_i t)) \right) \hat b\]</span></p>
<p>Substituting back in for <span class="math inline">\(x\)</span> and <span class="math inline">\(b\)</span> we get:</p>
<p><span class="math display">\[x = V \text{diag}\left( \lambda_i^{-1} (1 - \exp (-\lambda_i t)) \right) U^\top b\]</span></p>
<p>If we stare at this long enough it begins to look a lot like the pseudoinverse of <span class="math inline">\(A\)</span> from earlier:</p>
<p><span class="math inline">\(x = V \Sigma^{-1} U^\top b\)</span> except in this case there is a time dependence. At the limit as <span class="math inline">\(t \rightarrow \infty\)</span> we have that the exponential term goes to zero and we are left with the pseudoinverse solution. This is a nice way to see that the pseudoinverse is the limit of the gradient descent algorithm. What we may be interested in is what happens at earlier stages since each decay term is dependent on the eigenvalues.</p>
<p>For a simple matrix problem we can create a matrix and plot out the time evolution of the diagonals of the matrix that are of interest. In a sense, we have singular values that are time evolving at different rates.</p>
<div id="57334044" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Seed for reproducibility</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">4</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a 5x10 matrix A with random values</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.random.randn(<span class="dv">5</span>, <span class="dv">10</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a vector b of size 5 with random values</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.random.randn(<span class="dv">5</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the SVD of A</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>U, S, Vt <span class="op">=</span> np.linalg.svd(A, full_matrices<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a time dependent vector of the singular values</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> St(t):</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    Sdim <span class="op">=</span> S[:, np.newaxis]</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (<span class="dv">1</span> <span class="op">-</span> np.exp(<span class="op">-</span>Sdim<span class="op">**</span><span class="dv">2</span><span class="op">*</span>t)) <span class="op">/</span> Sdim</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the time evolution of the values and plot them on a log scale y axis with a linear time x axis</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="fl">.6</span>, <span class="dv">100</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> t[np.newaxis, :]</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>singular_vals_t <span class="op">=</span> St(T)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the plot</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="fl">7.5</span>, <span class="dv">4</span>))</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a color palette</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>palette <span class="op">=</span> sns.color_palette(<span class="st">"husl"</span>, <span class="bu">len</span>(S))</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the singular values and their asymptotes</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(S)):</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the time evolution of each singular value</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    sns.lineplot(x<span class="op">=</span>t, y<span class="op">=</span>singular_vals_t[i, :], color<span class="op">=</span>palette[i], linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'$1/S_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">$ '</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    Sinv <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>S[i]</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add a horizontal asymptote at the original singular value</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    plt.axhline(y<span class="op">=</span>Sinv, color<span class="op">=</span>palette[i], linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Annotate the asymptote with the singular value</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    plt.text(t[<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> <span class="fl">0.02</span>, Sinv, <span class="ss">f'</span><span class="sc">{</span>Sinv<span class="sc">:.2f}</span><span class="ss">'</span>, color<span class="op">=</span>palette[i], va<span class="op">=</span><span class="st">'center'</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Configure plot aesthetics</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Time'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Inverse Singular Vals'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Time Evolution of Pseudo Inverse in Gradient Flow'</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>plt.legend(title<span class="op">=</span><span class="st">'Inverse Singular Vals'</span>, bbox_to_anchor<span class="op">=</span>(<span class="fl">1.05</span>, <span class="dv">1</span>), loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>plt.xlim(t[<span class="dv">0</span>], t[<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> <span class="fl">0.1</span>)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">'imgs/pseudo_inverse_time_evolution.png'</span>)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-2-output-1.png" width="703" height="374" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>So we can use early stopping to prevent the flow from reaching the optimal point, a very useful technique. When it comes to inverse theory, often we are not interested in the optimal solution, but more interested in getting somewhere close that is not too noisy. This method differs from the thresholded pseudoinverse from the previous lecture, in that it allows some blending of the the smaller singular values, but their propensity for blowing up is controlled by the time exponent and early stopping.</p>
</section>
<section id="example-for-image-recovery-using-analytic-solution" class="level3">
<h3 class="anchored" data-anchor-id="example-for-image-recovery-using-analytic-solution">Example for Image Recovery using Analytic Solution</h3>
<p>Referring back to the problem of estimating the original image based on a noisy point spread function. We can monitor the time evolution of the estimate using gradient flow. Some code below defines the problem again, with recovery of the SVD decomposition for the 32x32 image, which will be used to solve the ODE for the gradient flow.</p>
<div id="2e0edef4" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">#matplotlib.use('TkAgg')</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim <span class="im">import</span> Adam</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> copy</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.fft</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> gaussianConv(nn.Module):</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co">    A PyTorch module that applies a Gaussian convolution to an input image using </span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="co">    a parameterized Gaussian Point Spread Function (PSF). The PSF is derived </span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="co">    from a covariance matrix and the derivatives of the Gaussian are computed </span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="co">    for edge detection.</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="co">        C (torch.Tensor): Inverse of covariance matrix used to define the shape of the Gaussian.</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="co">        t (float, optional): Scaling factor for the Gaussian, default is np.exp(5).</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="co">        n0 (float, optional): Scaling factor for the original PSF, default is 1.</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="co">        nx (float, optional): Scaling factor for the derivative along the x-axis, default is 1.</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a><span class="co">        ny (float, optional): Scaling factor for the derivative along the y-axis, default is 1.</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, C, t<span class="op">=</span>np.exp(<span class="dv">5</span>), n0<span class="op">=</span><span class="dv">1</span>, nx<span class="op">=</span><span class="dv">1</span>, ny<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(gaussianConv, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.C <span class="op">=</span> C</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t <span class="op">=</span> t</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n0 <span class="op">=</span> n0</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.nx <span class="op">=</span> nx</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ny <span class="op">=</span> ny</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image):</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>        P, center <span class="op">=</span> <span class="va">self</span>.psfGauss(image.shape[<span class="op">-</span><span class="dv">1</span>], image.device)</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>        P_shifted <span class="op">=</span> torch.roll(P, shifts<span class="op">=</span>center, dims<span class="op">=</span>[<span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>        S <span class="op">=</span> torch.fft.fft2(P_shifted)</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>        I_fft <span class="op">=</span> torch.fft.fft2(image)</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>        B_fft <span class="op">=</span> S <span class="op">*</span> I_fft</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>        B <span class="op">=</span> torch.real(torch.fft.ifft2(B_fft))</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> B</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> psfGauss(<span class="va">self</span>, dim, device<span class="op">=</span><span class="st">'cpu'</span>):</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> dim</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>        n <span class="op">=</span> dim</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create a meshgrid of (X, Y) coordinates</span></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.arange(<span class="op">-</span>m <span class="op">//</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, m <span class="op">//</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, device<span class="op">=</span>device)</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> torch.arange(<span class="op">-</span>n <span class="op">//</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, n <span class="op">//</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, device<span class="op">=</span>device)</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>        X, Y <span class="op">=</span> torch.meshgrid(x, y, indexing<span class="op">=</span><span class="st">'ij'</span>)</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> X.unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>)  <span class="co"># Shape: (1, 1, m, n)</span></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>        Y <span class="op">=</span> Y.unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>)  <span class="co"># Shape: (1, 1, m, n)</span></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>        cx, cy, cxy <span class="op">=</span> <span class="va">self</span>.C[<span class="dv">0</span>, <span class="dv">0</span>], <span class="va">self</span>.C[<span class="dv">1</span>, <span class="dv">1</span>], <span class="va">self</span>.C[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>        PSF <span class="op">=</span> torch.exp(<span class="op">-</span><span class="va">self</span>.t <span class="op">*</span> (cx <span class="op">*</span> X <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> cy <span class="op">*</span> Y <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> cxy <span class="op">*</span> X <span class="op">*</span> Y))</span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>        PSF0 <span class="op">=</span> PSF <span class="op">/</span> torch.<span class="bu">sum</span>(PSF.<span class="bu">abs</span>())</span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>        Kdx <span class="op">=</span> torch.tensor([[<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>                            [<span class="op">-</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>],</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>                            [<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>]], dtype<span class="op">=</span>PSF0.dtype, device<span class="op">=</span>device) <span class="op">/</span> <span class="dv">4</span></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>        Kdy <span class="op">=</span> torch.tensor([[<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>                            [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>                            [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>]], dtype<span class="op">=</span>PSF0.dtype, device<span class="op">=</span>device) <span class="op">/</span> <span class="dv">4</span></span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>        Kdx <span class="op">=</span> Kdx.unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>)  <span class="co"># Shape: (1, 1, 3, 3)</span></span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>        Kdy <span class="op">=</span> Kdy.unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>)  <span class="co"># Shape: (1, 1, 3, 3)</span></span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>        PSFdx <span class="op">=</span> F.conv2d(PSF0, Kdx, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>        PSFdy <span class="op">=</span> F.conv2d(PSF0, Kdy, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>        PSF_combined <span class="op">=</span> <span class="va">self</span>.n0 <span class="op">*</span> PSF0 <span class="op">+</span> <span class="va">self</span>.nx <span class="op">*</span> PSFdx <span class="op">+</span> <span class="va">self</span>.ny <span class="op">*</span> PSFdy</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>        center <span class="op">=</span> [<span class="dv">1</span> <span class="op">-</span> m <span class="op">//</span> <span class="dv">2</span>, <span class="dv">1</span> <span class="op">-</span> n <span class="op">//</span> <span class="dv">2</span>]</span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> PSF_combined, center</span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.zeros(<span class="dv">1</span>, <span class="dv">1</span>, dim, dim)</span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a>x[:,:, <span class="dv">12</span>:<span class="dv">14</span>, <span class="dv">12</span>:<span class="dv">14</span>] <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a>x[:,:, <span class="dv">10</span>:<span class="dv">12</span>, <span class="dv">10</span>:<span class="dv">12</span>] <span class="op">=</span> <span class="op">-</span><span class="fl">1.0</span></span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> torch.tensor([[<span class="dv">1</span>, <span class="dv">0</span>],[<span class="dv">0</span>, <span class="dv">1</span>]])</span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a>Amv <span class="op">=</span> gaussianConv(C, t<span class="op">=</span><span class="fl">0.1</span>,n0<span class="op">=</span><span class="dv">1</span>, nx<span class="op">=</span><span class="fl">0.1</span>,  ny<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span>(<span class="bu">len</span>(x.flatten()))</span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a>Amat <span class="op">=</span> torch.zeros(n,n)</span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a>k<span class="op">=</span><span class="dv">0</span></span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(x.shape[<span class="op">-</span><span class="dv">2</span>]):</span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(x.shape[<span class="op">-</span><span class="dv">1</span>]):</span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a>    e_ij <span class="op">=</span> torch.zeros_like(x)</span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a>    e_ij[:,:, i, j] <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> Amv(e_ij)</span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a>    Amat[:, k] <span class="op">=</span> y.flatten()</span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> k<span class="op">+</span><span class="dv">1</span></span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a>U, S, V <span class="op">=</span> torch.svd(Amat.to(torch.float64))</span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> Amv(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now that we have the matrix form of the forward operator <code>Amat</code> defined, along with the forward result <code>b</code> and the the decomposition <code>U, S, V</code> we can run the pseudo-inverse gradient flow method as before. So in this case we will be computing:</p>
<p><span class="math display">\[ x = V \text{diag}\left( \lambda_i^{-1} (1 - \exp (-\lambda_i t)) \right) U^\top b\]</span></p>
<p>Since these represents an evolution over time, an animation can be created to show the time evolution of the image recovery, along with the effect of continuing into a region where noise is amplified and dominates.</p>
<p>Recalling the original and distorted images with a small amount of noise <span class="math inline">\(\epsilon\)</span> are as follows:</p>
<div id="f8cdd23c" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">3</span>))</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>plt.imshow(x[<span class="dv">0</span>, <span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'viridis'</span>, vmin<span class="op">=-</span><span class="dv">1</span>, vmax<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Original Image'</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>b_noisy <span class="op">=</span> b<span class="op">+</span> <span class="fl">0.01</span> <span class="op">*</span> torch.randn_like(b)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>plt.imshow(b_noisy[<span class="dv">0</span>, <span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'viridis'</span>, vmin<span class="op">=-</span><span class="dv">1</span>, vmax<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Distorted Image'</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-4-output-1.png" width="549" height="288" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The distorted image has had much of its intensity spread out diffusely, so it is only visible as a faint outline. The noise is also visible in the image as a grainy texture. The gradient flow method will attempt to recover the original image from this distorted image.</p>
<div id="7febabf8" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> animation</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>b_flat <span class="op">=</span> b.flatten().to(torch.float64)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>x_flat <span class="op">=</span> x.flatten().to(torch.float64)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>b_noisy <span class="op">=</span> b_flat <span class="op">+</span> <span class="fl">0.001</span> <span class="op">*</span> torch.randn_like(b_flat)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_xhat(t):</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    Sinv_t <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> torch.exp(<span class="op">-</span>S<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> t)) <span class="op">/</span> S</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    A_pinv <span class="op">=</span> V <span class="op">@</span> torch.diag(Sinv_t) <span class="op">@</span> U.T</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    xhat <span class="op">=</span> A_pinv <span class="op">@</span> b_noisy</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> xhat</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Time evolution parameters</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>num_frames <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>t_vals <span class="op">=</span> np.logspace(<span class="dv">0</span>, <span class="dv">6</span>, num_frames)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare the plot</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>im <span class="op">=</span> ax.imshow(np.zeros((dim, dim)), cmap<span class="op">=</span><span class="st">'viridis'</span>, vmin<span class="op">=-</span><span class="dv">1</span>, vmax<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Time Evolution of Pseudo-Inverse Gradient Flow'</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the error text</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>error_text <span class="op">=</span> ax.text(<span class="fl">0.02</span>, <span class="fl">0.95</span>, <span class="st">''</span>, transform<span class="op">=</span>ax.transAxes, color<span class="op">=</span><span class="st">'blue'</span>, fontsize<span class="op">=</span><span class="dv">12</span>,</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>                     verticalalignment<span class="op">=</span><span class="st">'top'</span>)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>time_text <span class="op">=</span> ax.text(<span class="fl">0.5</span>, <span class="fl">0.95</span>, <span class="st">''</span>, transform<span class="op">=</span>ax.transAxes, color<span class="op">=</span><span class="st">'blue'</span>, fontsize<span class="op">=</span><span class="dv">12</span>,</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>                        verticalalignment<span class="op">=</span><span class="st">'top'</span>)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize containers to track min error and best time</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>tracking <span class="op">=</span> {<span class="st">'min_error'</span>: <span class="bu">float</span>(<span class="st">'inf'</span>), <span class="st">'best_t'</span>: <span class="fl">0.0</span>}</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Animation update function</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update_frame(t):</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute time-dependent singular values</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>    Sinv_t <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> torch.exp(<span class="op">-</span>S <span class="op">**</span> <span class="dv">2</span> <span class="op">*</span> t)) <span class="op">/</span> S</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Construct the pseudoinverse of Amat at time t</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    A_pinv <span class="op">=</span> V <span class="op">@</span> torch.diag(Sinv_t) <span class="op">@</span> U.t()</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reconstruct the image estimate x(t)</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    xt <span class="op">=</span> A_pinv <span class="op">@</span> b_noisy</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the relative error</span></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> torch.norm(x_flat <span class="op">-</span> xt) <span class="op">/</span> torch.norm(x_flat)</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update min_error and best_t if current error is lower</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> error.item() <span class="op">&lt;</span> tracking[<span class="st">'min_error'</span>]:</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>        tracking[<span class="st">'min_error'</span>] <span class="op">=</span> error.item()</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>        tracking[<span class="st">'best_t'</span>] <span class="op">=</span> t</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reshape to image dimensions</span></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>    x_image <span class="op">=</span> xt.reshape(dim, dim).detach().numpy()</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update the image data</span></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>    im.set_data(x_image)</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update the error text</span></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>    error_text.set_text(<span class="ss">f'Relative Error: </span><span class="sc">{</span>error<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>    time_text.set_text(<span class="ss">f'Time: </span><span class="sc">{</span>t<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [im, error_text, time_text]</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the animation</span></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>ani <span class="op">=</span> animation.FuncAnimation(fig, update_frame, frames<span class="op">=</span>t_vals, blit<span class="op">=</span><span class="va">True</span>, interval<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>ani.save(<span class="st">'imgs/gradient_flow.gif'</span>, writer<span class="op">=</span><span class="st">'pillow'</span>, fps<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>plt.close(fig)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><img src="imgs/gradient_flow.gif" class="img-fluid" width="600"></p>
<p>And we saved the best time that was discovered for the recovery (with prior knowledge of the ground truth). So we can inspect that image, this was the best that we could do with the gradient flow method.</p>
<div id="96468aea" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>best_img <span class="op">=</span> get_xhat(tracking[<span class="st">'best_t'</span>]).reshape(dim, dim).detach().numpy()</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(best_img <span class="op">/</span> np.<span class="bu">max</span>(np.<span class="bu">abs</span>(best_img)), cmap<span class="op">=</span><span class="st">'viridis'</span>, vmin<span class="op">=-</span><span class="dv">1</span>, vmax<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'Best Reconstruction at t=</span><span class="sc">{</span>tracking[<span class="st">"best_t"</span>]<span class="sc">:.2f}</span><span class="ch">\n</span><span class="ss">Relative Error: </span><span class="sc">{</span>tracking[<span class="st">"min_error"</span>]<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-6-output-1.png" width="463" height="501" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="recovery-of-the-adjoint-operator-using-autograd" class="level2">
<h2 class="anchored" data-anchor-id="recovery-of-the-adjoint-operator-using-autograd">Recovery of the Adjoint Operator using Autograd</h2>
<p>In this case we were able to compute the matrix form of <span class="math inline">\(A\)</span> and use its transpose to compute the SVD, but in many cases this might be too expensive or there may not be a closed form analytic solution to the early stopping technique. In such cases we wish to recover the adjoint. The question then is how to recover the adjoint operator from the <code>Amv</code> operator? There are helpful tools available through the use of automatic differentiation to track the gradients of the forward operator and recover the adjoint operator. This is a very powerful tool that can be used to recover the adjoint operator in a very general way.</p>
<p>By definition the adjoint has the property that: <span class="math display">\[\langle Ax, v \rangle = \langle x, A^\top v \rangle\]</span></p>
<section id="explicit-computation-of-the-adjoint" class="level3">
<h3 class="anchored" data-anchor-id="explicit-computation-of-the-adjoint">Explicit Computation of the Adjoint</h3>
<p>We can compute the adjoint explicitly for the <code>Amv</code> operator based on its computation from earlier. The discrete fourier transform matrix operator <span class="math inline">\(F\)</span> has the property that <span class="math inline">\(F^{-1} = F^\top\)</span> so we can use this to compute the adjoint.</p>
<p><span class="math display">\[
\begin{align}
A(x) &amp;= \mathcal{F}^-1 \left( \mathcal{F}(P) \odot \mathcal{F}(x) \right)\\
&amp;= F^\top \left( \text{diag} (F(P)) F(x) \right)\\
A^\top(v) &amp;= F^\top \text{diag} (F(P))^* F v\\
\end{align}
\]</span></p>
<p>Where the hadamard operation of the two vectors has been modified to a matrix form by diagonalizing the vector <span class="math inline">\(F(P)\)</span> that is the Fourier transform of the point spread function. From this form it is posible to take the adjoint of the operator by taking the complex conjugate of the transpose of the entire operation.</p>
</section>
<section id="autograd-computation-of-the-adjoint" class="level3">
<h3 class="anchored" data-anchor-id="autograd-computation-of-the-adjoint">Autograd Computation of the Adjoint</h3>
<p>We start with a new function <span class="math inline">\(h = v^\top A(x)\)</span> and we wish to compute the gradient of <span class="math inline">\(h\)</span> with respect to <span class="math inline">\(x\)</span>.</p>
<p><span class="math display">\[ \nabla_x h = \nabla_x (v^\top A(x)) = A^\top(v)\]</span></p>
<p>The gradient of <span class="math inline">\(h\)</span> with respect to <span class="math inline">\(x\)</span> is the adjoint operator <span class="math inline">\(A^\top(v)\)</span>. We can use the <code>torch.autograd.grad</code> function to compute the gradient of <span class="math inline">\(h\)</span> with respect to <span class="math inline">\(x\)</span>.</p>
<div id="ae7a7b8e" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Amv_adjoint(v):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.zeros(<span class="dv">1</span>, <span class="dv">1</span>, dim, dim)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    x.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> Amv(x)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the dot product of the forward operator with the input vector</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> torch.<span class="bu">sum</span>(b <span class="op">*</span> v)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the gradient of the dot product with respect to the input image</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    adjoint <span class="op">=</span> torch.autograd.grad(h, x, create_graph<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> adjoint</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We can use this to recover <span class="math inline">\(A^\top\)</span> for the general case if we run the operator on the set of basis vectors in the image space. This will give us the adjoint operator in the form of a matrix. We can also use it to confirm that it recovers the matrix transpose of the forward operator if we are working with a simple matrix, reusing the <code>Amat</code> matrix from earlier to take its transpose and compare it to the adjoint operator.</p>
<div id="4dbef633" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>Amat_adj <span class="op">=</span> torch.zeros(n,n)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">32</span> <span class="co"># Same as earlier</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>k<span class="op">=</span><span class="dv">0</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(dim):</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(dim):</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    e_ij <span class="op">=</span> torch.zeros_like(x)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    e_ij[:,:, i, j] <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> Amv_adjoint(e_ij)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    Amat_adj[:, k] <span class="op">=</span> y.flatten()</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> k<span class="op">+</span><span class="dv">1</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>diff <span class="op">=</span> torch.norm(Amat_adj <span class="op">-</span> Amat.T)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Norm of difference between adjoint and transpose: </span><span class="sc">{</span>diff<span class="sc">:.2e}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Norm of difference between adjoint and transpose: 4.43e-07</code></pre>
</div>
</div>
<p>So the difference is within the bounds of numerical precison and the code appears to be working correctly.</p>
</section>
</section>
<section id="gradient-descent-with-adjoint" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent-with-adjoint">Gradient Descent with Adjoint</h2>
<p>We can now use the defined operators (functions) from earlier to setup a simple gradient descent algorithm with a step size and early stopping to produce a recovery image that bypasses the need to compute the SVD decomposition, which may be very expensive for large matrices.</p>
<div id="4d7ed9ba" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> least_squares_sol(x0, b, Amv, Amv_adjoint, max_iter<span class="op">=</span><span class="dv">1000</span>, alpha<span class="op">=</span><span class="fl">1e-3</span>, tol<span class="op">=</span><span class="fl">1e-6</span>, show_progress<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Solves the least squares problem using gradient descent with optional progress tracking.</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">    - x0 (torch.Tensor): Initial guess for the solution.</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co">    - b (torch.Tensor): Observation vector.</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co">    - Amv (callable): Function to compute A @ x.</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co">    - Amv_adjoint (callable): Function to compute A^T @ v.</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co">    - max_iter (int): Maximum number of iterations.</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co">    - alpha (float): Learning rate.</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co">    - tol (float): Tolerance for convergence.</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co">    - show_progress (bool): If True, display a progress bar; otherwise, suppress output.</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="co">    - x (torch.Tensor): Approximated solution vector.</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x0.clone()</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    x.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    b_noisy <span class="op">=</span> b.clone() <span class="op">+</span> <span class="fl">0.01</span> <span class="op">*</span> torch.randn_like(b)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize progress bar or a placeholder for quiet mode</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>    pbar <span class="op">=</span> tqdm(total<span class="op">=</span>max_iter, desc<span class="op">=</span><span class="st">'Least Squares Iteration'</span>, unit<span class="op">=</span><span class="st">'iter'</span>, disable<span class="op">=</span><span class="kw">not</span> show_progress) </span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Gradient descent update</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>        residual <span class="op">=</span> Amv(x) <span class="op">-</span> b_noisy</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>        gradient <span class="op">=</span> Amv_adjoint(residual)</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>        xnext <span class="op">=</span> x <span class="op">-</span> alpha <span class="op">*</span> gradient</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute relative error</span></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>        error <span class="op">=</span> torch.norm(xnext <span class="op">-</span> x) </span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update the progress bar with the current error</span></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> show_progress:</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>            pbar.set_postfix({<span class="st">'Error'</span>: <span class="ss">f'</span><span class="sc">{</span>error<span class="sc">.</span>item()<span class="sc">:.4e}</span><span class="ss">'</span>})</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>            pbar.update(<span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check for convergence</span></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> error <span class="op">&lt;</span> tol:</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> show_progress:</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>                pbar.write(<span class="ss">f'Converged at iteration </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> with error </span><span class="sc">{</span>error<span class="sc">.</span>item()<span class="sc">:.4e}</span><span class="ss">'</span>)</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> xnext</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> xnext</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>    pbar.close()</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> Amv(x)</span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> torch.zeros_like(x)</span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a>xhat <span class="op">=</span> least_squares_sol(x0, b, Amv, Amv_adjoint, max_iter<span class="op">=</span><span class="dv">1000</span>, alpha<span class="op">=</span><span class="dv">1</span>, tol<span class="op">=</span><span class="fl">1e-6</span>, show_progress<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Display final images</span></span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">3</span>))</span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a>plt.imshow(x[<span class="dv">0</span>, <span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'viridis'</span>, vmin<span class="op">=-</span><span class="dv">1</span>, vmax<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Original Image'</span>)</span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a>plt.imshow(xhat.detach().numpy()[<span class="dv">0</span>, <span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'viridis'</span>, vmin<span class="op">=-</span><span class="dv">1</span>, vmax<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Recovered Image'</span>)</span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-9-output-1.png" width="549" height="288" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Note that torch does have the framework to run autograd on the least squares objective itself, but for this general method we are using the adjoint to compute the gradient (and indirectly invoking autograd). This framework is the most general for when there might not be explicit analytic solutions to the least squares problem, but we have the forward operator and its adjoint.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/chipnbits\.github\.io");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2026, Simon Ghyselincks</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://chipnbits.github.io/">
      <i class="bi bi-house" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/chipnbits">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>