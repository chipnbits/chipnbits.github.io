---
title: "Lecture 3: Image Denoising with Gradient Descent and Early Stopping"
subtitle: "A derivation of least squares gradient descent and ODE analysis"
date: 2024-09-17
author: "Simon Ghyselincks"
description: >-
    In continuation of Lecture 2, we now look at an alternative approach to image denoising using gradient descent and early stopping. We will derive the least squares gradient descent algorithm and analyze it as an ordinary differential equation.
categories:
  - Optimization
  - Inverse Theory
  - Python
  - Torch
  - Adjoint

image: imgs/pseudo_inverse_time_evolution.png
draft: false

format:
  html:
    code-fold: true
    code-summary: "Show the code"

editor: 
  render-on-save: false
---

## Derivations of Linear Algebra Gradients

Often times we wish to find the gradient of a multi-variable function that is formulated as a linear algebra operation. In this case there are some useful "vector" derivatives and rules that can simplify the process of calculating more complex expressions. The gradient with respect to vector $\mathbf{x}$ is generally denoted as $\nabla_{\mathbf{x}}$ or alternatively $\partial_{\mathbf{x}}$, somewhat of an abuse of notation.

#### 1.  A Warmup

$$\phi(x) = a^\top x = \sum_i a_i x_i$$

This is a vector dotproduct and the gradient is simply the vector $a$. There is a subtlety here in that the vector is usually transposed to be a column vector, but this is not always the case. Some people in the field of statistics prefer to use row vector, this can cause some confusion. The general convention is a column vector.

$$\nabla_{\mathbf{x}} \phi = a$$

#### 2. Matrix Vector Multiplication

$$\phi(x) = Ax$$

Based on the previous process we are expecting to potentially get $A^\top$ as the gradient, however the transpose does not occur in this case because we are not returning a vector that needs to be reshaped into a column form.

$$\nabla_{\mathbf{x}} \phi = A$$

#### 3. Quadratic Forms

Often we may encounter quadratic linear functions that are of the form:
$$ \phi(x) = x^\top A x$$

One way to determine the gradient is to expand the expression and evaluate for a single $\frac{\partial}{\partial x_i}$ term. This method can be found at [Mark Schmidt Notes](https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/linearQuadraticGradients.pdf) Instead we can apply a chain rule for matrix differentiation that is based on the product rule for differentiation. The chain rule for matrix differentiation is as follows:

$$\frac{d f(g,h)}{d x} = \frac{d (g(x)^\top)}{d x} \frac{\partial f(g,h)}{\partial g} + \frac{d (h(x)^\top)}{d x} \frac{\partial f(g,h)}{\partial h}$$

$$ \begin {align*}
\phi(x) &= x^\top A x \\
\nabla_{\mathbf{x}} \phi &= \nabla_{\mathbf{x}} (x^\top A x) \\
&= \nabla_{\mathbf{x}} x^\top (A x) =  \nabla_{\mathbf{x}} x^\top y\\
&= (\nabla_{\mathbf{x}} x) \nabla_{\mathbf{x}} x^\top y + \nabla_{\mathbf{x}} y^\top \nabla_{\mathbf{y}} x^\top y\\
&= I y + \nabla_{\mathbf{x}} (x^\top A^\top) x\\
&= (A x) + A^\top x\\
&= (A + A^\top) x
\end {align*}
$$

This fits with the generalization for a scalar quadratic form where we end up with $(cx^2)' = (c + c^\top)x = 2cx$ where $c$ is a scalar.

#### 4. Hadamard Product

Another form of interest is the hadamard product of two vectors.
$$\phi(x) = (Ax)^2 = Ax \odot Ax$$

For this one let $y=Ax$ and we can index each element of the vector $y$ as $y_i = \sum_j A_{ij} x_j$. The hadamard product is a vector $z$ where $z_i = y_i^2$, we can compute the jacobian since now we are taking the gradient with respect to a vector.

The Jacobian will contain the partial derivatives:

$$\frac{d\vec{z}}{d\vec{x}} = \begin{bmatrix} \frac{\partial z_1}{\partial x_1} & \frac{\partial z_1}{\partial x_2} & \cdots & \frac{\partial z_1}{\partial x_n} \\
\frac{\partial z_2}{\partial x_1} & \frac{\partial z_2}{\partial x_2} & \cdots & \frac{\partial z_2}{\partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial z_n}{\partial x_1} & \frac{\partial z_n}{\partial x_2} & \cdots & \frac{\partial z_n}{\partial x_n} \end{bmatrix}
$$

If we can recover this then we have the gradient of the hadamard product.

$$
\begin{align*}
z_i &= y_i^2 = \left( \sum_j A_{ij} x_j \right)^2\\
\frac{\partial}{\partial x_j} y_i^2 &= 2 y_i \frac{\partial y_i}{\partial x_j} = 2 y_i A_{ij}\\
\frac{d\vec{z}}{d\vec{x}} &= 2 \begin{bmatrix} y_1 A_{1j} & y_1 A_{2j} & \cdots & y_1 A_{nj} \\
y_2 A_{1j} & y_2 A_{2j} & \cdots & y_2 A_{nj} \\
\vdots & \vdots & \ddots & \vdots \\
y_n A_{1j} & y_n A_{2j} & \cdots & y_n A_{nj} \end{bmatrix}\\
&= 2 \cdot \text{diag}(\vec{y})A\\
&= 2 \cdot \text{diag}(Ax)A
\end{align*}
$$

#### 5. Least Squares Gradient

We look at taking the gradient of the expansion of least squares to find the gradient for this optimization objective.

$$\phi(x) = \frac{1}{2} ||Ax - b||^2 = \frac{1}{2} (x^\top A^\top A x - 2 b^\top A x + b^\top b)$$

$$ \begin{align*}
\nabla_{\mathbf{x}} \phi &= \nabla_{\mathbf{x}} \left( \frac{1}{2} (x^\top A^\top A x - 2 b^\top A x + b^\top b) \right)\\
&= \nabla_{\mathbf{x}} \left( \frac{1}{2} x^\top A^\top A x \right) - \nabla_{\mathbf{x}} \left( b^\top A x \right)\\
&= \frac{1}{2} (A^\top A + A^\top A) x - A^\top b\\
&= A^\top A x - A^\top b\\
\end{align*}
$$

Returning to the first-order optimality condition we have:
$$A^\top A x = A^\top b$$

At which point it is in question if $A^\top A$ is invertible. The invertibility of $A^\top A$ is determined by the rank of $A$. The rank of A for a non-square matrix is the number of independent columns. If we examine $A^\top Ax = 0$ then we see that this is only true where the range of $A$ is in the nullspace of $A^\top$. But $N(A^\top) = R(A)^\perp$ so they are orthogonal subspaces and will never coincide unless $Ax=0$. So then $A^\top A x = 0$ implies that $Ax = 0$ which means that if the null space of $A=\{0\}$ then the null space of $A^\top A = \{0\}$ and $A^\top A$ is invertible. Since $A^\top A$ is symmetric and positive definite, it is invertible.


$A^\top A$ is invertible $\iff$ $A$ is full rank, that is all the columns are independent. For non-square matrices, an $m>n$ matrix that is wide will trivially not satisfy this condition. A tall matrix $m<n$ will satisfy the condition if the columns are independent. 

## Gradient Descent Analysis

The standard form of the gradient descent algorithm comes from the field of optimization and can be written as:

$$ x_{k+1} = x_k - \alpha \nabla_x \phi(x_k)$$

Where $\alpha$ is the learning rate, which can be dependent on the problem and the gradient. Substituting the gradient of the least squares problem we have:

$$ \begin{align}
x_{k+1} &= x_k - \alpha (A^\top A x_k - A^\top b)\\
\frac{x_{k+1}-x_k}{\alpha} &= A^\top b - A^\top A x_k\\
\lim_{\alpha \to 0} \frac{x_{k+1}-x_k}{\alpha} &= \frac{dx}{dt} = A^\top (b -A x), \quad x(0) = x_0
\end{align}
$$

This ODE is the continuous version of the gradient descent algorithm, also known as the *gradient flow*. Since this a linear first-order ODE we can solve it analytically. The general method for a linear system ODE would be to find the homogeneous solution and the particular solution:

$$ \begin{align}
x' + A^\top A x &= A^\top b\\
\text{Guess:} x &= v e^{\lambda t}\\
\lambda v e^{\lambda t} + A^\top A v e^{\lambda t} &= A^\top b e^{\lambda t}\\
\lambda v + A^\top A v &= 0 \qquad \text{Homogeneous}\\
(\lambda I + A^\top A) v &= 0\\
\lambda &= \text{eigenvalues of } A^\top A, \quad v = \text{eigenvectors of } A^\top A
\end{align}
$$

Before continuing further with this line, we can see that the solutions will be closely related to the SVD because it contains the information on these eigenvalues and vectors. So we can try to solve the ODE with the SVD.

#### Solving the ODE with SVD

$$\begin{align}
A &= U \Sigma V^\top\\
A^TA &= V \Sigma^2 V^\top\\
\frac{d}{dt}x &= V \Sigma U^\top b - V \Sigma^2 V^\top x\\
\end{align}
$$

Now let $z = V^\top x$ and $\hat b = U ^ \top b$ then we have:

$$\begin{align}
\frac{d}{dt} (V^\top x) &= \Sigma \hat b - \Sigma^2 (V^\top x)\\
\frac{d}{dt} z &= \Sigma \hat b - \Sigma^2 z\\
z' + \Sigma^2 z &= \Sigma \hat b\\
\end{align}
$$

At this stage since everything has been diagonalized, all of the equations are decoupled and independent so we can solve for the $\lambda_i$ cases independently. We find the homogeneous $z_h$ and particular $z_p$ solutions:

$$
\begin{align}
z_h' + \lambda^2 z_h &= 0\\
z_h &= c e^{-\lambda^2 t}\\
z_p' + \lambda^2 z_p &= \lambda \hat b\\
z_p &= D \hat b \\
\lambda^2 D \hat b &= \lambda \hat b\\
D &= \frac{1}{\lambda}\\
z_p &= \frac{1}{\lambda} \hat b
\end{align}
$$

So the general solution for the $i^{th}$ component is:

$$z_i = c_i e^{-\lambda_i^2 t} + \frac{1}{\lambda_i} \hat b_i$$

Supposing that we start at $x=0$ then we have $z=0$ at all elements and can solve the coefficients $c_i$:

$$c_i = -\frac{1}{\lambda_i} \hat b_i$$

Then putting it all back together with all the equations we have that 

$$Z = \text{diag}\left( \lambda_i^{-1} (1 - \exp (-\lambda_i t)) \right) \hat b$$

Substituting back in for $x$ and $b$ we get:

$$x = V \text{diag}\left( \lambda_i^{-1} (1 - \exp (-\lambda_i t)) \right) U^\top b$$

If we stare at this long enough it begins to look a lot like the pseudoinverse of $A$ from earlier:

$x = V \Sigma^{-1} U^\top b$ except in this case there is a time dependence. At the limit as $t \rightarrow \infty$ we have that the exponential term goes to zero and we are left with the pseudoinverse solution. This is a nice way to see that the pseudoinverse is the limit of the gradient descent algorithm. What we may be interested in is what happens at earlier stages since each decay term is dependent on the eigenvalues.

For a simple matrix problem we can create a matrix and plot out the time evolution of the diagonals of the matrix that are of interest. In a sense, we have singular values that are time evolving at different rates.

```{python}
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Seed for reproducibility
np.random.seed(4)
# Create a 5x10 matrix A with random values
A = np.random.randn(5, 10)
# Create a vector b of size 5 with random values
b = np.random.randn(5)

# Compute the SVD of A
U, S, Vt = np.linalg.svd(A, full_matrices=False)

# Create a time dependent vector of the singular values
def St(t):
    Sdim = S[:, np.newaxis]
    return (1 - np.exp(-Sdim**2*t)) / Sdim

# Compute the time evolution of the values and plot them on a log scale y axis with a linear time x axis
t = np.linspace(0, .6, 100)
T = t[np.newaxis, :]

singular_vals_t = St(T)

# Initialize the plot
plt.figure(figsize=(7.5, 4))

# Create a color palette
palette = sns.color_palette("husl", len(S))

# Plot the singular values and their asymptotes
for i in range(len(S)):
    # Plot the time evolution of each singular value
    sns.lineplot(x=t, y=singular_vals_t[i, :], color=palette[i], linewidth=2, label=f'$1/S_{i}$ ')
    
    Sinv = 1/S[i]

    # Add a horizontal asymptote at the original singular value
    plt.axhline(y=Sinv, color=palette[i], linestyle='--', linewidth=1)
    
    # Annotate the asymptote with the singular value
    plt.text(t[-1] + 0.02, Sinv, f'{Sinv:.2f}', color=palette[i], va='center')

# Configure plot aesthetics
plt.xlabel('Time', fontsize=14)
plt.ylabel('Inverse Singular Vals', fontsize=14)
plt.title('Time Evolution of Pseudo Inverse in Gradient Flow', fontsize=16)
plt.legend(title='Inverse Singular Vals', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.xlim(t[0], t[-1] + 0.1)
plt.tight_layout()
plt.savefig('imgs/pseudo_inverse_time_evolution.png')
plt.show()



```

So we can use early stopping to prevent the flow from reaching the optimal point, a very useful technique. When it comes to inverse theory, often we are not interested in the optimal solution, but more interested in getting somewhere close that is not too noisy. This method differs from the thresholded pseudoinverse from the previous lecture, in that it allows some blending of the the smaller singular values, but their propensity for blowing up is controlled by the time exponent and early stopping.

