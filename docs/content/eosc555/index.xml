<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Simon Ghyselincks</title>
<link>https://chipnbits.github.io/content/eosc555/</link>
<atom:link href="https://chipnbits.github.io/content/eosc555/index.xml" rel="self" type="application/rss+xml"/>
<description>A personal page for Simon Ghyselincks</description>
<generator>quarto-1.5.53</generator>
<lastBuildDate>Sun, 15 Sep 2024 07:00:00 GMT</lastBuildDate>
<item>
  <title>Lecture 2: Image Denoising with SVD</title>
  <dc:creator>Simon Ghyselincks</dc:creator>
  <link>https://chipnbits.github.io/content/eosc555/lectures/lecture2/</link>
  <description><![CDATA[ 




<section id="image-denoising-and-deblurring" class="level1">
<h1>Image Denoising and Deblurring</h1>
<p>The motivation for the exercise comes from a real world problem. The Hubble space telescope when launched had a defect in its mirror. This defect caused the images to be blurred. The problem was initially addressed by using signal processing techniques to remove the aberrations from the images.</p>
<section id="point-spread-function" class="level3">
<h3 class="anchored" data-anchor-id="point-spread-function">Point Spread Function</h3>
<p>For such an image processing problem, we can consider the continuous incoming light as striking a 2D mirror that distorts the light, followed by a 2D sensor that captures the light. In this context we suppose that we have a noise kernel or a point spread function (PSF) that describes the distortion of the light at the mirror. The point spread function, being a convolution kernel, behaves as a Greenâ€™s function for the system in the continuous case:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cvec%7Bb%7D(x,y)%20=%20%5Cint_%7B%5Cmathcal%7BX%7D%7D%20%5Cint_%7B%5Cmathcal%7BY%7D%7D%20%5Cvec%7BG%7D(x%20-%20x',%20y%20-%20y')%20%5Cvec%7Bu%7D(x',y')%20%5C,%20dx'%20dy'%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bb%7D(x,y)"> is the blurred image data that is recovered at the sensor, <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bu%7D(x',y')"> is the true image data, and <img src="https://latex.codecogs.com/png.latex?%5Cvec%7BG%7D(x,y)"> is the point spread function.</p>
<p>In the special case that the point spread function is <img src="https://latex.codecogs.com/png.latex?%5Cdelta(x-x',y-y')">, then the image data is not distorted and the sensor captures the true image data. However our experiment is to consider cases where there could be even severe distortions and see how this impacts the proposition of recovering the true image data, <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bu%7D(x',y')"> from our sensor data, <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bb%7D(x,y)">.</p>
<section id="discrete-psf" class="level4">
<h4 class="anchored" data-anchor-id="discrete-psf">Discrete PSF</h4>
<p>The discrete analog of the continuous PSF can be more conveniently treated with we essentially flatten the the 2D mesh into a 1D vector, a common operation for signal processing. The unflattened case we have:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20b_%7Bij%7D%20=%20%5Csum_%7Bk=1%7D%5E%7Bn%7D%20%5Csum_%7Bl=1%7D%5E%7Bm%7D%20%5CDelta%20x%20%5CDelta%20y%20G(x_i%20-%20x_k,%20y_j%20-%20y_l)%20u_%7Bkl%7D%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?b"> is the blurred image data at the sensor, <img src="https://latex.codecogs.com/png.latex?u"> is the true image data, and <img src="https://latex.codecogs.com/png.latex?G"> is the discrete point spread function. If we flatten the 2D mesh into a 1D vector we can represent this as a 1D convolution operation: <img src="https://latex.codecogs.com/png.latex?%20%5Cvec%7Bb%7D%20=%20%5Cvec%7BG%7D%20*%20%5Cvec%7Bu%7D%20"></p>
<p>Since this is a convolution operation, we can process it much more quickly by leveraging the convolution theorem.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%5Cmathcal%7BF%7D(%5Cvec%7Bb%7D)%20&amp;=%20%5Cmathcal%7BF%7D(%5Cvec%7BG%7D%20*%20%5Cvec%7Bu%7D)%20%5C%5C%0A%5Cmathcal%7BF%7D(%5Cvec%7Bb%7D)%20&amp;=%20%5Cmathcal%7BF%7D(%5Cvec%7BG%7D)%20%5Cmathcal%7BF%7D(%5Cvec%7Bu%7D)%20%5C%5C%0A%5Cvec%7Bb%7D%20&amp;=%20%5Cmathcal%7BF%7D%5E%7B-1%7D(%5Cmathcal%7BF%7D(%5Cvec%7BG%7D)%20%5Codot%20%5Cmathcal%7BF%7D(%5Cvec%7Bu%7D))%0A%5Cend%7Balign%7D%0A"></p>
<p>The <img src="https://latex.codecogs.com/png.latex?%5Codot"> hadamard product is element-wise multiplication, the discrete analog of multiplication of two functions except over an array.</p>
</section>
</section>
<section id="matrix-representation-of-convolution-operation" class="level3">
<h3 class="anchored" data-anchor-id="matrix-representation-of-convolution-operation">Matrix Representation of Convolution Operation</h3>
<p>If we flatten the data down into a 1D vector then it is possible to construct a matrix operator that performs the convolution. This is a Toeplitz matrix, a matrix where each descending diagonal from left to right is constant, so that the row vectors represent a sliding window of the convolution kernel. We can flatten out the PSF and construct the matrix using it as the first row entry and then shifting the PSF to the right to fill out the rest of the rows.</p>
</section>
</section>
<section id="code-implementation" class="level1">
<h1>Code Implementation</h1>
<div id="e6d07c3b" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib</span>
<span id="cb1-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#matplotlib.use('TkAgg')</span></span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch.optim</span>
<span id="cb1-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb1-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch.nn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> nn</span>
<span id="cb1-8"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch.nn.functional <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> F</span>
<span id="cb1-9"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torch.optim <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Adam</span>
<span id="cb1-10"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> copy</span>
<span id="cb1-11"></span>
<span id="cb1-12"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb1-13"></span>
<span id="cb1-14"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> math</span>
<span id="cb1-15"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> os</span>
<span id="cb1-16"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> time</span>
<span id="cb1-17"></span>
<span id="cb1-18"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-19"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-20"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch.fft</span></code></pre></div>
</details>
</div>
<p>We start off by introducing a point spread function within the torch framework. In the case we work with a parameterized gaussian kernel.</p>
<section id="gaussian-example" class="level3">
<h3 class="anchored" data-anchor-id="gaussian-example">Gaussian Example</h3>
<p>The multivariate extension of the gaussian function is given by: <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cexp%5Cleft(-%5Cfrac%7B1%7D%7B2%7D%20(x-%5Cmu)%5ET%20%5CSigma%5E%7B-1%7D%20(x-%5Cmu)%5Cright)"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmu"> is the mean vector, <img src="https://latex.codecogs.com/png.latex?x"> is a position vector, and <img src="https://latex.codecogs.com/png.latex?%5CSigma"> is the covariance matrix. The covariance matrix essentially encodes the eigenvectors and corresponding postive eigenvalues of the matrix. The covariance matrix is always symmetric and positive definite. In the context of the code, we are using <img src="https://latex.codecogs.com/png.latex?C"> as the inverse of the covariance matrix and working with a <img src="https://latex.codecogs.com/png.latex?%5Cmu=0"> value.</p>
<div id="cell-mv-plot" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> multivariate_gaussian(pos, mean, cov):</span>
<span id="cb2-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Return the multivariate Gaussian distribution on array pos."""</span></span>
<span id="cb2-3">    n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mean.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb2-4">    diff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pos <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> mean</span>
<span id="cb2-5">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compute the exponent</span></span>
<span id="cb2-6">    exponent <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.einsum(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'...k,kl,...l-&gt;...'</span>, diff, np.linalg.inv(cov), diff)</span>
<span id="cb2-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compute the normalization factor</span></span>
<span id="cb2-8">    norm_factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.sqrt((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.pi) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.linalg.det(cov))</span>
<span id="cb2-9">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Return the Gaussian function</span></span>
<span id="cb2-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.exp(exponent) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> norm_factor</span>
<span id="cb2-11"></span>
<span id="cb2-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define the grid limits and resolution</span></span>
<span id="cb2-13">X, Y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mgrid[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>:<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>:<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>]</span>
<span id="cb2-14">pos <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.dstack((X, Y))</span>
<span id="cb2-15"></span>
<span id="cb2-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Parameters</span></span>
<span id="cb2-17">mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb2-18">eigenvalues <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>])  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Example eigenvalues</span></span>
<span id="cb2-19">principal_axis <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Example principal axis</span></span>
<span id="cb2-20"></span>
<span id="cb2-21"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Normalize the principal axis</span></span>
<span id="cb2-22">principal_axis <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> principal_axis <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> np.linalg.norm(principal_axis)</span>
<span id="cb2-23"></span>
<span id="cb2-24"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Create the covariance matrix</span></span>
<span id="cb2-25">D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.diag(eigenvalues)</span>
<span id="cb2-26">orthogonal_complement <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>principal_axis[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], principal_axis[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]])</span>
<span id="cb2-27">Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.column_stack((principal_axis, orthogonal_complement))</span>
<span id="cb2-28">cov <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> D <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> Q.T</span>
<span id="cb2-29"></span>
<span id="cb2-30"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compute the Gaussian function over the grid</span></span>
<span id="cb2-31">Z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> multivariate_gaussian(pos, mean, cov)</span>
<span id="cb2-32"></span>
<span id="cb2-33"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Contour plot</span></span>
<span id="cb2-34">plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>))</span>
<span id="cb2-35">plt.contourf(X, Y, Z, levels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>, cmap<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'viridis'</span>)</span>
<span id="cb2-36">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Continuous Multivariate Gaussian Distribution with Eigenvectors'</span>)</span>
<span id="cb2-37">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'X-axis'</span>)</span>
<span id="cb2-38">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Y-axis'</span>)</span>
<span id="cb2-39">plt.axis(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'equal'</span>)</span>
<span id="cb2-40"></span>
<span id="cb2-41"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Plot the eigenvectors</span></span>
<span id="cb2-42">eigvals, eigvecs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linalg.eigh(cov)</span>
<span id="cb2-43"></span>
<span id="cb2-44"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Scale factor for visualization</span></span>
<span id="cb2-45">scale_factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb2-46"></span>
<span id="cb2-47"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Origin at the mean</span></span>
<span id="cb2-48">origin <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mean</span>
<span id="cb2-49"></span>
<span id="cb2-50"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Plot eigenvectors</span></span>
<span id="cb2-51"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(eigvals)):</span>
<span id="cb2-52">    eigval <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> eigvals[i]</span>
<span id="cb2-53">    eigvec <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> eigvecs[:, i]</span>
<span id="cb2-54">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Scale eigenvector by the square root of the eigenvalue</span></span>
<span id="cb2-55">    vector <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> eigvec <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.sqrt(eigval) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> scale_factor</span>
<span id="cb2-56">    plt.quiver(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>origin, vector[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], vector[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], angles<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'xy'</span>, scale_units<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'xy'</span>, scale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb2-57"></span>
<span id="cb2-58">plt.savefig(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'imgs/gaussian_plot.png'</span>, dpi<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">300</span>, bbox_inches<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tight'</span>)</span>
<span id="cb2-59">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="mv-plot" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://chipnbits.github.io/content/eosc555/lectures/lecture2/index_files/figure-html/mv-plot-output-1.png" width="547" height="523" class="figure-img"></p>
<figcaption>Plot</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="extending-to-combination-of-gaussian-and-derivative" class="level3">
<h3 class="anchored" data-anchor-id="extending-to-combination-of-gaussian-and-derivative">Extending to Combination of Gaussian and Derivative</h3>
<p>We can compute the MV gaussian from the inverse covariance matrix <img src="https://latex.codecogs.com/png.latex?C"> with a mean of <img src="https://latex.codecogs.com/png.latex?%5Cmu=0"> along with a dimensional scaling metric <img src="https://latex.codecogs.com/png.latex?t">. For the purposes of forming interesting and varied PSFs, we include the linear combination of the gaussian and a Sobel operator to axpproximate the derivative of the gaussian.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0AS_x%20&amp;=%20%5Cfrac%7B1%7D%7B4%7D%20%5Cbegin%7Bbmatrix%7D%20-1%20&amp;%200%20&amp;%201%20%5C%5C%20-2%20&amp;%200%20&amp;%202%20%5C%5C%20-1%20&amp;%200%20&amp;%201%20%5Cend%7Bbmatrix%7D%20%5C%5C%0AS_y%20&amp;=%20%5Cfrac%7B1%7D%7B4%7D%20%5Cbegin%7Bbmatrix%7D%20-1%20&amp;%20-2%20&amp;%20-1%20%5C%5C%200%20&amp;%200%20&amp;%200%20%5C%5C%201%20&amp;%202%20&amp;%201%20%5Cend%7Bbmatrix%7D%0A%5Cend%7Balign%7D"></p>
<p>These operators act like edge detection or derivatives. The <img src="https://latex.codecogs.com/png.latex?n_0">, <img src="https://latex.codecogs.com/png.latex?n_x">, and <img src="https://latex.codecogs.com/png.latex?n_y"> parameters are used to scale the gaussian and the derivatives.</p>
<div id="cbc998ee" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> gaussianConv(nn.Module):</span>
<span id="cb3-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb3-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    A PyTorch module that applies a Gaussian convolution to an input image using </span></span>
<span id="cb3-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    a parameterized Gaussian Point Spread Function (PSF). The PSF is derived </span></span>
<span id="cb3-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    from a covariance matrix and the derivatives of the Gaussian are computed </span></span>
<span id="cb3-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    for edge-detection.</span></span>
<span id="cb3-7"></span>
<span id="cb3-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Args:</span></span>
<span id="cb3-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        C (torch.Tensor): Covariance matrix used to define the shape of the Gaussian.</span></span>
<span id="cb3-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        t (float, optional): The scaling factor for the Gaussian, default is np.exp(5).</span></span>
<span id="cb3-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        n0 (float, optional): Scaling factor for the original PSF, default is 1.</span></span>
<span id="cb3-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        nx (float, optional): Scaling factor for the derivative along the x-axis, default is 1.</span></span>
<span id="cb3-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        ny (float, optional): Scaling factor for the derivative along the y-axis, default is 1.</span></span>
<span id="cb3-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb3-15">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, C, t<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.exp(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>), n0<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, nx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,  ny<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb3-16">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>(gaussianConv, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>).<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb3-17"></span>
<span id="cb3-18">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.C  <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> C</span>
<span id="cb3-19">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.t  <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> t</span>
<span id="cb3-20">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> n0</span>
<span id="cb3-21">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.nx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nx</span>
<span id="cb3-22">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.ny <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ny</span>
<span id="cb3-23"></span>
<span id="cb3-24">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, I):</span>
<span id="cb3-25"></span>
<span id="cb3-26">        P, center <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.psfGauss(I.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], I.device)</span>
<span id="cb3-27"></span>
<span id="cb3-28">        S <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.fft.fft2(torch.roll(P, shifts<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>center, dims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>]))</span>
<span id="cb3-29">        B <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.real(torch.fft.ifft2(S <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> torch.fft.fft2(I)))</span>
<span id="cb3-30"></span>
<span id="cb3-31">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> B</span>
<span id="cb3-32"></span>
<span id="cb3-33">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> psfGauss(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, dim, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cpu'</span>):</span>
<span id="cb3-34">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Square conv kenel mxn</span></span>
<span id="cb3-35">        m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dim</span>
<span id="cb3-36">        n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dim</span>
<span id="cb3-37"></span>
<span id="cb3-38">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Create a meshgrid then expand to B,C,X,Y</span></span>
<span id="cb3-39">        x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.arange(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>m<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,m<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>device)</span>
<span id="cb3-40">        y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.arange(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>device)</span>
<span id="cb3-41">        X, Y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.meshgrid(x,y, indexing<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ij'</span>)</span>
<span id="cb3-42">        X    <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>).unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb3-43">        Y    <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Y.unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>).unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb3-44"></span>
<span id="cb3-45">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Use C as inverse covariance matrix</span></span>
<span id="cb3-46">        cx, cy, cxy <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.C[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.C[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.C[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb3-47"></span>
<span id="cb3-48">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Apply gaussian across the mesh</span></span>
<span id="cb3-49">        PSF <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.exp(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.t<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>(cx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>X<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> cy<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>Y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>cxy<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>X<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>Y) )</span>
<span id="cb3-50">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Normalize to sum to 1</span></span>
<span id="cb3-51">        PSF0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PSF <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(PSF.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>())</span>
<span id="cb3-52"></span>
<span id="cb3-53">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Derivative kernels over gaussian</span></span>
<span id="cb3-54">        Kdx   <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor([[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>],[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>],[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span></span>
<span id="cb3-55">        Kdy   <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor([[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>],[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>],[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span></span>
<span id="cb3-56">        Kdx   <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Kdx.unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>).unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>).to(device)</span>
<span id="cb3-57">        Kdy   <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Kdy.unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>).unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>).to(device)</span>
<span id="cb3-58"></span>
<span id="cb3-59">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Perform the convolution to get the derivatives</span></span>
<span id="cb3-60">        PSFdx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> F.conv2d(PSF0, Kdx, padding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb3-61">        PSFdy <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> F.conv2d(PSF0, Kdy, padding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb3-62"></span>
<span id="cb3-63">        PSF <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n0<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>PSF0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.nx<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>PSFdx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.ny<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>PSFdy</span>
<span id="cb3-64">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Get center ready for output.</span></span>
<span id="cb3-65">        center <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>m<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>]</span>
<span id="cb3-66"></span>
<span id="cb3-67">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> PSF, center</span></code></pre></div>
</details>
</div>


</section>
</section>

 ]]></description>
  <category>Optimization</category>
  <category>Inverse Theory</category>
  <category>Python</category>
  <category>Torch</category>
  <category>SVD</category>
  <guid>https://chipnbits.github.io/content/eosc555/lectures/lecture2/</guid>
  <pubDate>Sun, 15 Sep 2024 07:00:00 GMT</pubDate>
  <media:content url="https://chipnbits.github.io/content/eosc555/lectures/lecture2/imgs/gaussian_plot.png" medium="image" type="image/png" height="138" width="144"/>
</item>
<item>
  <title>Lecture 1: Introduction to Inverse Theory</title>
  <dc:creator>Simon Ghyselincks</dc:creator>
  <link>https://chipnbits.github.io/content/eosc555/lectures/lecture1-2/</link>
  <description><![CDATA[ 




<section id="what-is-inverse-theory" class="level1">
<h1>What is Inverse Theory?</h1>
<p>Inverse theory is a set of mathematical techniques used to infer the properties of a physical system from observations of its output. It is a fundamental tool in many scientific disciplines, including geophysics, seismology, and medical imaging. Inverse theory is used to solve a wide range of problems, such as:</p>
<ul>
<li><strong>Parameter Estimation</strong>: Determining the values of unknown parameters in a model that best fit the observed data.</li>
<li><strong>System Identification</strong>: Identifying the structure and dynamics of a system from input-output data.</li>
<li><strong>Image Reconstruction</strong>: Reconstructing an image or object from noisy or incomplete measurements.</li>
</ul>
<p>What many of these tasks have in common is that we are working with incomplete information. There is a <em>forward</em> problem that has generated the data that we observe <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bb%7D"> from a set of input data <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bx%7D">, and we want to infer the <em>inverse</em> problem that generated the data. However the inverse problem is often ill-posed, meaning that there are multiple solutions that can fit the data equally well. Inverse theory provides a framework for finding the best solution to these problems.</p>
<p>The forward problem can be described for example as a differetial equation or operator <img src="https://latex.codecogs.com/png.latex?L"> that takes in some measured parameters <img src="https://latex.codecogs.com/png.latex?u"> with model parameters <img src="https://latex.codecogs.com/png.latex?x"> :</p>
<p><img src="https://latex.codecogs.com/png.latex?%20L(x)%5Bu%5D%20=%20q%20%5Ciff%20u%20=%20L%5E%7B-1%7D(x)%5Bq%5D%20"></p>
<p>For example making measurements of an electromagnetic field in correspondence to conductivity values that are underground we have:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cnabla%20%5Csigma%20%5Cnabla%20u%20=%20q%20+%20%5Ctext%7BBC%7D"></p>
<p>We measure the <img src="https://latex.codecogs.com/png.latex?u"> at some points and use that to try and form an estimate of the conductivity <img src="https://latex.codecogs.com/png.latex?%5Csigma">. The forward problem is to solve for <img src="https://latex.codecogs.com/png.latex?u"> given <img src="https://latex.codecogs.com/png.latex?%5Csigma"> and the inverse problem is to solve for <img src="https://latex.codecogs.com/png.latex?%5Csigma"> given <img src="https://latex.codecogs.com/png.latex?u">. The forward problem is often well-posed and the inverse problem is often ill-posed.</p>
<p>For a computational framework we can discretize the the equation so that the operator is a matrix <img src="https://latex.codecogs.com/png.latex?A"> and the data is a vector <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bb%7D">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cunderbrace%7BA%7D_%7B%5Ctext%7BForward%20Map%7D%7D%20%5Cunderbrace%7B%5Cvec%7Bx%7D%7D_%7B%5Ctext%7BModel%20Parameters%7D%7D%20+%20%5Cepsilon%20=%20%5Cunderbrace%7B%5Cvec%7Bb%7D%7D_%7B%5Ctext%7BObserved%20Data%7D%7D%20"></p>
<p>In this case we may have a sparse set of measurements <img src="https://latex.codecogs.com/png.latex?b"> and a large set of <img src="https://latex.codecogs.com/png.latex?x"> making the problem underdetermined. The goal of inverse theory is to find the best estimate of <img src="https://latex.codecogs.com/png.latex?x"> given <img src="https://latex.codecogs.com/png.latex?b">.</p>
<section id="example-the-triathlon-problem" class="level3">
<h3 class="anchored" data-anchor-id="example-the-triathlon-problem">Example: The Triathlon Problem</h3>
<p>To illustrate the concept of inverse theory, consider the following example:</p>
<blockquote class="blockquote">
<p>Suppose that you have agreed to meet a friend to watch them during a triathlon race but you showed up late and missed the start. They are expecting for you to have been there at some point during the time at which they were changing from a running phase to a cycle phase. They expect you to know the time at which they made the transition. However you only know the overall start time and finish time of the race.</p>
<p>If the race starts at time <img src="https://latex.codecogs.com/png.latex?t=0"> and then ends at time <img src="https://latex.codecogs.com/png.latex?t=b"> how do you use this information to deduce the actual time <img src="https://latex.codecogs.com/png.latex?t_r%20%5Cin%20%5B0,b%5D"> at which they crossed the transition zone of the race?</p>
</blockquote>
<p>The first restriction on feasible solutions is the domain <img src="https://latex.codecogs.com/png.latex?%5B0,b%5D"> so that we know that <img src="https://latex.codecogs.com/png.latex?0%3Ct_r%3Cb">.</p>
<p>After this there are some other techniquest that we could use to better inform the probability of the occurence at different times. For example, we might have a good idea of their fitness level or average running speed from previous experience. Or in the abscence of this information there might be average times for the competitors that are available to further inform the problem and reduce the amount of error in the estimate.</p>
</section>
<section id="the-singular-value-decomposition" class="level2">
<h2 class="anchored" data-anchor-id="the-singular-value-decomposition">The Singular Value Decomposition</h2>
<p>For cases where the matrix <img src="https://latex.codecogs.com/png.latex?A"> is not full rank, the singular value decomposition (SVD) provides a more general framework for solving the least squares problem. The SVD decomposes the matrix <img src="https://latex.codecogs.com/png.latex?A"> into three matrices <img src="https://latex.codecogs.com/png.latex?U">, <img src="https://latex.codecogs.com/png.latex?%5CSigma">, and <img src="https://latex.codecogs.com/png.latex?V"></p>
<p><img src="https://latex.codecogs.com/png.latex?%20A%20=%20U%20%5CSigma%20V%5ET%20"></p>
<p>The matrices have the following special properties:</p>
<ul>
<li><em>Orthogonal Subspaces</em>: <img src="https://latex.codecogs.com/png.latex?U"> and <img src="https://latex.codecogs.com/png.latex?V"> are orthogonal matrices, meaning that <img src="https://latex.codecogs.com/png.latex?U%5ETU%20=%20I"> and <img src="https://latex.codecogs.com/png.latex?V%5ETV%20=%20I">, that is <img src="https://latex.codecogs.com/png.latex?U%5ET%20=%20U%5E%7B-1%7D"> and $V^T = V^{-1}.</li>
<li><em>Ordered Singular Values</em>: <img src="https://latex.codecogs.com/png.latex?%5CSigma"> is a diagonal matrix with non-negative values on the diagonal, known as the singular values of <img src="https://latex.codecogs.com/png.latex?A">. The singular values are ordered such that <img src="https://latex.codecogs.com/png.latex?%5Csigma_1%20%5Cgeq%20%5Csigma_2%20%5Cgeq%20%5Cldots%20%5Cgeq%20%5Csigma_r">. The number of non-zero singular values is equal to the rank of <img src="https://latex.codecogs.com/png.latex?A">.</li>
</ul>
<p>Supposed that we have a <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Brank%7D(A)%20=%20r"> matrix <img src="https://latex.codecogs.com/png.latex?A"> which maps from <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5Em%5Crightarrow%20%5Cmathbb%7BR%7D%5En">. A fundamental way to view this mapping is as a composition of three linear transformations: a rotation <img src="https://latex.codecogs.com/png.latex?V">, a scaling <img src="https://latex.codecogs.com/png.latex?%5CSigma">, and another rotation <img src="https://latex.codecogs.com/png.latex?U">. The orthogonal matrix <img src="https://latex.codecogs.com/png.latex?V"> has the property that all of its rows and columns are orthogonal to each other, and the vectors themselves are normalized to <img src="https://latex.codecogs.com/png.latex?1">. To see this property of the orthogonal matrix consider that <img src="https://latex.codecogs.com/png.latex?V%5ET%20V%20=%20I"> and <img src="https://latex.codecogs.com/png.latex?V%20V%5ET%20=%20I">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cbegin%7Balign%7D%0AZ%20=%20V%5ET%20V%20&amp;=%20I%20%5C%5C%0Az_%7Bij%7D%20=%20%5Clangle%20v_i,%20v_j%20%5Crangle%20&amp;=%20%5Cdelta_%7Bij%7D%20%5Cend%7Balign%7D%20"></p>
<p>Each of the elements of the matrix <img src="https://latex.codecogs.com/png.latex?V%5ET"> is the dot product of the <img src="https://latex.codecogs.com/png.latex?i">th and <img src="https://latex.codecogs.com/png.latex?j">th columns of <img src="https://latex.codecogs.com/png.latex?V">. The dotproduct of all vectors against themselves is <img src="https://latex.codecogs.com/png.latex?1"> and the dotproduct of any two different vectors is <img src="https://latex.codecogs.com/png.latex?0">. So from this we can see that all of the columns of <img src="https://latex.codecogs.com/png.latex?V"> are orthogonal to each other. The same property holds for <img src="https://latex.codecogs.com/png.latex?U">.</p>
<p><img src="https://latex.codecogs.com/png.latex?V%5ET"> by our definition of <img src="https://latex.codecogs.com/png.latex?A"> must accept a vector from <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5Em"> and the matrix is square, indicating an <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20m"> matrix. The matrix <img src="https://latex.codecogs.com/png.latex?U"> must output a vector in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En"> and the matrix is square, indicating an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> matrix. The matrix <img src="https://latex.codecogs.com/png.latex?%5CSigma"> must be <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20m"> to map from <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5Em"> to <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En">.</p>
<p>In all its glory:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AA_%7Bn%20%5Ctimes%20m%7D%20&amp;=%20U_%7Bn%20%5Ctimes%20n%7D%20%5C,%20%5CSigma_%7Bn%20%5Ctimes%20m%7D%20%5C,%20V%5ET_%7Bm%20%5Ctimes%20m%7D%20%5C%5C%0A&amp;=%20%5Cleft%5B%20%5Cbegin%7Barray%7D%7Bccc%7Cccc%7D%0A%5Cmathbf%7Bu%7D_1%20&amp;%20%5Ccdots%20&amp;%20%5Cmathbf%7Bu%7D_r%20&amp;%20%5Cmathbf%7Bu%7D_%7Br+1%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cmathbf%7Bu%7D_n%0A%5Cend%7Barray%7D%20%5Cright%5D_%7Bn%20%5Ctimes%20n%7D%0A%5Cleft%5B%20%5Cbegin%7Barray%7D%7Bccc%7D%0A%5Csigma_1%20&amp;%20%20&amp;%20%20%5C%5C%0A&amp;%20%5Cddots%20&amp;%20%20%5C%5C%0A&amp;%20%20&amp;%20%5Csigma_r%20%5C%5C%0A0%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0A0%20&amp;%20%5Ccdots%20&amp;%200%0A%5Cend%7Barray%7D%20%5Cright%5D_%7Bn%20%5Ctimes%20m%7D%0A%5Cleft%5B%20%5Cbegin%7Barray%7D%7Bccc%7Cccc%7D%0A%5Cmathbf%7Bv%7D%5ET_1%20%5C%5C%0A%5Cvdots%20%5C%5C%0A%5Cmathbf%7Bv%7D%5ET_r%20%5C%5C%0A%5Cmathbf%7Bv%7D%5ET_%7Br+1%7D%20%5C%5C%0A%5Cvdots%20%5C%5C%0A%20%20%5Cmathbf%7Bv%7D%5ET_m%0A%5Cend%7Barray%7D%20%5Cright%5D_%7Bm%20%5Ctimes%20m%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>In this case the first <img src="https://latex.codecogs.com/png.latex?r"> columns of <img src="https://latex.codecogs.com/png.latex?U"> are the range of <img src="https://latex.codecogs.com/png.latex?A">, the rest of <img src="https://latex.codecogs.com/png.latex?U"> is filled with its orthogonal complement. The first <img src="https://latex.codecogs.com/png.latex?r"> columns of <img src="https://latex.codecogs.com/png.latex?V"> are the domain of <img src="https://latex.codecogs.com/png.latex?A">, the rest of <img src="https://latex.codecogs.com/png.latex?V"> is filled with its orthogonal complement. These are the four fundamental subspaces of the matrix <img src="https://latex.codecogs.com/png.latex?A">, more information on this can be found at: <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">Wikipedia: SVD</a></p>
<p>The matrices as shown above are for a rectangular <img src="https://latex.codecogs.com/png.latex?A"> where <img src="https://latex.codecogs.com/png.latex?n%3Em"> but the same properties hold for all <img src="https://latex.codecogs.com/png.latex?n,m">. Some of the singular values <img src="https://latex.codecogs.com/png.latex?%5Csigma_i"> may be zero, in which case the matrix <img src="https://latex.codecogs.com/png.latex?A"> is not full rank.</p>
<p>Another way to decompose the SVD is to write it as a sum of outer products that are scaled by the diagonal matrix of singular values:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20A%20=%20%5Csum_%7Bi=1%7D%5Er%20%5Csigma_i%20%5Cmathbf%7Bu%7D_i%20%5Cmathbf%7Bv%7D_i%5ET%20"></p>
<p>If <img src="https://latex.codecogs.com/png.latex?%5Csigma_i%3E0"> then <img src="https://latex.codecogs.com/png.latex?v_i"> is not in the null space of <img src="https://latex.codecogs.com/png.latex?A"> because <img src="https://latex.codecogs.com/png.latex?A%20v_i%20=%20%5Csigma_i%20u_i">. If <img src="https://latex.codecogs.com/png.latex?%5Csigma_i%20=%200"> then <img src="https://latex.codecogs.com/png.latex?v_i"> is in the null space of <img src="https://latex.codecogs.com/png.latex?A"> because <img src="https://latex.codecogs.com/png.latex?A%20v_i%20=%200">.</p>
<section id="the-pseudoinverse" class="level3">
<h3 class="anchored" data-anchor-id="the-pseudoinverse">The Pseudoinverse</h3>
<p>Back to the task of inverting <img src="https://latex.codecogs.com/png.latex?Ax%20+%20%5Cepsilon%20=%20b"> we can apply the SVD decomposition:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0AU%20%5CSigma%20V%5ET%20x%20+%20%5Cepsilon%20&amp;=%20b%20%5C%5C%0A%5CSigma%20V%5ET%20x%20+&amp;=%20U%5ET%20(b-%5Cepsilon)%20%5C%5C%0AV%20%5CSigma%5E%7B-1%7D%20U%5ET%20(b-%5Cepsilon)%20&amp;=%20x%5C%5C%0AA%5E+%20(b-%5Cepsilon)%20&amp;=%20%5Chat%7Bx%7D%0A%5Cend%7Balign%7D"></p>
<p>Where <img src="https://latex.codecogs.com/png.latex?A%5E+%20=%20V%20%5CSigma%5E%7B-1%7D%20U%5ET"> is the pseudoinverse of <img src="https://latex.codecogs.com/png.latex?A">. The pseudoinverse is a generalization of the matrix inverse for non-square matrices. We recover a square matrix by removing all of the absent or zero singular values from <img src="https://latex.codecogs.com/png.latex?%5CSigma"> and inverting the rest, giving an <img src="https://latex.codecogs.com/png.latex?r%20%5Ctimes%20r"> diagonal matrix whose inverse is simply the inverse of each element.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cleft%5B%20%5Cbegin%7Barray%7D%7Bccc%7D%0A%5Csigma_1%20&amp;%20%20&amp;%20%20%5C%5C%0A&amp;%20%5Cddots%20&amp;%20%20%5C%5C%0A&amp;%20%20&amp;%20%5Csigma_r%20%5C%5C%0A0%20&amp;%20%5Ccdots%20&amp;%200%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0A0%20&amp;%20%5Ccdots%20&amp;%200%0A%5Cend%7Barray%7D%20%5Cright%5D_%7Bn%20%5Ctimes%20m%7D%0A%5Crightarrow%20%5Cleft%5B%20%5Cbegin%7Barray%7D%7Bccc%7D%0A%5Csigma_1%5E%7B-1%7D%20&amp;%20%20&amp;%20%20%5C%5C%0A%20%20&amp;%20%5Cddots%20&amp;%20%20%5C%5C%0A%20%20&amp;%20%20&amp;%20%5Csigma_r%5E%7B-1%7D%20%5C%5C%0A%20%20%5Cend%7Barray%7D%20%5Cright%5D_%7Br%20%5Ctimes%20r%7D"></p>
<p>Then <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bx%7D%20=%20%5Csum_i%5EN%20%5Csigma_i%5E%7B-1%7D%20%5Cmathbf%7Bu%7D_i%5ET%20(b-%5Cepsilon)%20%5Cmathbf%7Bv%7D_i"> is the solution to the least squares problem. This can be solved also as a truncated sum since <img src="https://latex.codecogs.com/png.latex?0%3CN%3Cr">. In actual practice with real world measurement we end up with many singular values that may be effectively <img src="https://latex.codecogs.com/png.latex?0"> by nature of being very small relative to the noise in the data and the largest single value. We have that the solution <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bx%7D"> is a sum of <img src="https://latex.codecogs.com/png.latex?v_i"> components that form an orthogonal basis <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bx%7D%20=%20%5Csum_i%20%5Cbeta_i%20v_i"> where <img src="https://latex.codecogs.com/png.latex?%5Cbeta_i%20=%20%5Cfrac%7Bu_i%5ET%20(b-%5Cepsilon)%7D%7B%5Csigma_i%7D">. These small singular values blow up in size when inverted and so extra truncation is often necessary to avoid numerical instability and excessive amplification of noise <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">.</p>
</section>
</section>
<section id="least-squares" class="level2">
<h2 class="anchored" data-anchor-id="least-squares">Least Squares</h2>
<p>Least squares and matrix inversion is a classic starting point for understanding inverse theory. Suppose that we have input data <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bx%7D"> and output data <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bb%7D"> that are related by a linear system of equations: <img src="https://latex.codecogs.com/png.latex?Ax%20=%20b"> where <img src="https://latex.codecogs.com/png.latex?A"> is a matrix of coefficients. In many cases, the system is overdetermined, meaning that there are more equations than unknowns. In this case, there is no exact solution to the system, and we must find the best solution that minimizes the error between the observed data <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bb%7D"> and the predicted data <img src="https://latex.codecogs.com/png.latex?A%5Cvec%7Bx%7D">. In the simplest form of inversion that we can attempt, we can solve the least squares solution. In this case we reject all of the observed data that is from the null space of <img src="https://latex.codecogs.com/png.latex?A"> assuming a zero value for each of those parameters.</p>
<section id="example" class="level3">
<h3 class="anchored" data-anchor-id="example">Example</h3>
<p>Let <img src="https://latex.codecogs.com/png.latex?A"> be a <img src="https://latex.codecogs.com/png.latex?3%20%5Ctimes%202"> matrix and <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bb%7D"> be a <img src="https://latex.codecogs.com/png.latex?3%20%5Ctimes%201"> vector. The <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bx%7D"> that we are trying to solve for is a <img src="https://latex.codecogs.com/png.latex?2%20%5Ctimes%201"> vector. The system of equations is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20A%20=%20%5Cbegin%7Bbmatrix%7D%20%20%5Cvec%7Ba%7D_1%20&amp;%20%5Cvec%7Ba%7D_2%20%5Cend%7Bbmatrix%7D%20%5Cquad%20%5Cvec%7Bx%7D%20=%20%5Cbegin%7Bbmatrix%7D%20x_1%20%5C%5C%20x_2%20%20%5Cend%7Bbmatrix%7D%20%20%5Cquad%20%5Cvec%7Bb%7D%20=%20%5Cbegin%7Bbmatrix%7D%20b_1%20%5C%5C%20b_2%20%5C%5C%20b_3%20%5Cend%7Bbmatrix%7D%20"></p>
<p>In this case we have an <em>overdetermined</em> system with three equations, two unknowns, and three data samples. If the system of equations is full rank then we are trying to map from a 2D space to a 3D space: <img src="https://latex.codecogs.com/png.latex?A:%20%5Cmathbb%7BR%7D%5E2%20%5Crightarrow%20%5Cmathbb%7BR%7D%5E3">. In this case there is no exact solution to the system for any <img src="https://latex.codecogs.com/png.latex?b"> that is not in the column space of <img src="https://latex.codecogs.com/png.latex?A">.</p>
<p>Instead we can solve for the least squares solution <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bx%7D_%7BLS%7D"> by minimizing the error between the observed data <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bb%7D"> and the predicted data <img src="https://latex.codecogs.com/png.latex?A%5Cvec%7Bx%7D"> from the forward model.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cvec%7Bx%7D_%7BLS%7D%20=%20%5Carg%20%5Cmin_%7B%5Cvec%7Bx%7D%7D%20%7C%7CA%5Cvec%7Bx%7D%20-%20%5Cvec%7Bb%7D%7C%7C_2%5E2%20"></p>
<p>We want to find the argument that minimizes the function <img src="https://latex.codecogs.com/png.latex?f(%5Cvec%7Bx%7D)%20=%20%7C%7CA%5Cvec%7Bx%7D%20-%20%5Cvec%7Bb%7D%7C%7C_2%5E2">. By first order optimality conditions, the gradient of the function must be zero at the minimum.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cbegin%7Balign%7D%0A%5Cnabla%20f(%5Cvec%7Bx%7D)%20&amp;=%200%20%5C%5C%0A%5Cnabla%20%7C%7CA%5Cvec%7Bx%7D%20-%20%5Cvec%7Bb%7D%7C%7C_2%5E2%20&amp;=%200%20%5C%5C%0A%5Cnabla%20(A%5Cvec%7Bx%7D%20-%20%5Cvec%7Bb%7D)%5ET%20(A%5Cvec%7Bx%7D%20-%20%5Cvec%7Bb%7D)%20&amp;=%200%20%5C%5C%0A%5Cnabla%20%5Cleft(%20%5Cvec%7Bx%7D%5ET%20A%5ET%20A%20%5Cvec%7Bx%7D%20-%202%20%5Cvec%7Bb%7D%5ET%20A%20%5Cvec%7Bx%7D%20+%20%5Cvec%7Bb%7D%5ET%20%5Cvec%7Bb%7D%20%5Cright)%20&amp;=%200%20%5C%5C%0A2%20A%5ET%20A%20%5Cvec%7Bx%7D%20-%202%20A%5ET%20%5Cvec%7Bb%7D%20&amp;=%200%20%5C%5C%0AA%5ET%20A%20%5Cvec%7Bx%7D%20&amp;=%20A%5ET%20%5Cvec%7Bb%7D%20%5C%5C%0A%5Cvec%7Bx%7D_%7BLS%7D%20&amp;=%20(A%5ET%20A)%5E%7B-1%7D%20A%5ET%20%5Cvec%7Bb%7D%0A%5Cend%7Balign%7D%20"></p>
<p>This is known as the normal equations for the least squares solution. We take a note of caution here that <img src="https://latex.codecogs.com/png.latex?A%5ET%20A"> must be invertible for this solution to exist. If <img src="https://latex.codecogs.com/png.latex?A"> is not full rank then the matrix <img src="https://latex.codecogs.com/png.latex?A%5ET%20A"> will not be invertible and other methods must be used.</p>
<p>We call the difference between the observed data and the predicted data the residual.</p>
<p><img src="https://latex.codecogs.com/png.latex?r%20=%20%5Cvec%7Bb%7D%20-%20A%5Cvec%7Bx%7D_%7BLS%7D"></p>
<p>Using this information, what we really want to minimize is the sum of the squares of the residuals: <img src="https://latex.codecogs.com/png.latex?%7C%7Cr%7C%7C_2%5E2">. This is the same as the sum of the squares of the errors in the data.</p>
<p>There is an altogether informative way to think about the minimization problem purely in terms of linear algebra and subspaces to derive the same normal equations.</p>
<div style="display: block; margin-left: auto; margin-right: auto; width: 50%; text-align: center;">
<img src="https://chipnbits.github.io/content/eosc555/lectures/lecture1-2/imgs/ls-sol.svg" alt="" width="300">
<p>
<em>Least Squares Visual</em>
</p>
</div>
<p>We have the range of <img src="https://latex.codecogs.com/png.latex?A"> or image of <img src="https://latex.codecogs.com/png.latex?A"> as the subspace of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E3"> that is spanned by the columns of <img src="https://latex.codecogs.com/png.latex?A">. This subspace is rank <img src="https://latex.codecogs.com/png.latex?2"> because there are only two columns in <img src="https://latex.codecogs.com/png.latex?A">, <img src="https://latex.codecogs.com/png.latex?R(A)%20%5Csubset%20%5Cmathbb%7BR%7D%5E3">. The inaccessible parts of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E3"> are in the orthogonal complement of <img src="https://latex.codecogs.com/png.latex?R(A)">, <img src="https://latex.codecogs.com/png.latex?R(A)%5E%5Cperp">. Recalling that <img src="https://latex.codecogs.com/png.latex?R(A)%5E%5Cperp%20=%20N(A%5ET)"> we can diagram the solution to least squares as a minimization of the error vector <img src="https://latex.codecogs.com/png.latex?r"> in the orthogonal complement of <img src="https://latex.codecogs.com/png.latex?R(A)">.</p>
<p>As seen the <img src="https://latex.codecogs.com/png.latex?r"> vector is perpendicular to the <img src="https://latex.codecogs.com/png.latex?x_%7BLS%7D"> solution, the projection of <img src="https://latex.codecogs.com/png.latex?r"> onto <img src="https://latex.codecogs.com/png.latex?R(A)"> is zero. Since it is in a null space of <img src="https://latex.codecogs.com/png.latex?A%5ET"> then <img src="https://latex.codecogs.com/png.latex?A%5ET%20r%20=%200">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cbegin%7Balign%7D%20A%5ET%20%5Cleft%20(%20Ax_%7BLS%7D%20-%20b%20%5Cright%20)%20&amp;=%200%5C%5C%0AA%5ET%20A%20x_%7BLS%7D%20&amp;=%20A%5ET%20b%20%5C%5C%0A%5Cend%20%7Balign%7D%20"></p>
<p>So we recover the normal equations without using any of the machinery of calculus.</p>
<p>For a review on the four fundamental subspaces of a matrix see the UBC Math 307 notes on the topic: <a href="https://ubcmath.github.io/MATH307/orthogonality/complement.html">Math 307</a></p>


</section>
</section>
</section>

 ]]></description>
  <category>Optimization</category>
  <category>Inverse Theory</category>
  <category>Python</category>
  <category>Torch</category>
  <category>SVD</category>
  <guid>https://chipnbits.github.io/content/eosc555/lectures/lecture1-2/</guid>
  <pubDate>Sat, 14 Sep 2024 07:00:00 GMT</pubDate>
  <media:content url="https://chipnbits.github.io/content/eosc555/lectures/lecture1-2/imgs/ls-sol.svg" medium="image" type="image/svg+xml"/>
</item>
</channel>
</rss>
