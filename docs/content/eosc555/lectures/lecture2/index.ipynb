{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lecture 2: Image Denoising with SVD\n",
        "\n",
        "Applications of Least Squares and SVD\n",
        "\n",
        "Simon Ghyselincks  \n",
        "2024-09-15\n",
        "\n",
        "# Image Denoising and Deblurring\n",
        "\n",
        "The motivation for the exercise comes from a real world problem. The\n",
        "Hubble space telescope when launched had a defect in its mirror. This\n",
        "defect caused the images to be blurred. The problem was initially\n",
        "addressed by using signal processing techniques to remove the\n",
        "aberrations from the images.\n",
        "\n",
        "### Point Spread Function\n",
        "\n",
        "For such an image processing problem, we can consider the continuous\n",
        "incoming light as striking a 2D mirror that distorts the light, followed\n",
        "by a 2D sensor that captures the light. In this context we suppose that\n",
        "we have a noise kernel or a point spread function (PSF) that describes\n",
        "the distortion of the light at the mirror. The point spread function,\n",
        "being a convolution kernel, behaves as a Greenâ€™s function for the system\n",
        "in the continuous case:\n",
        "\n",
        "$$ \\vec{b}(x,y) = \\int_{\\mathcal{X}} \\int_{\\mathcal{Y}} \\vec{G}(x - x', y - y') \\vec{u}(x',y') \\, dx' dy' $$\n",
        "\n",
        "where $\\vec{b}(x,y)$ is the blurred image data that is recovered at the\n",
        "sensor, $\\vec{u}(x',y')$ is the true image data, and $\\vec{G}(x,y)$ is\n",
        "the point spread function.\n",
        "\n",
        "In the special case that the point spread function is\n",
        "$\\delta(x-x',y-y')$, then the image data is not distorted and the sensor\n",
        "captures the true image data. However our experiment is to consider\n",
        "cases where there could be even severe distortions and see how this\n",
        "impacts the proposition of recovering the true image data,\n",
        "$\\vec{u}(x',y')$ from our sensor data, $\\vec{b}(x,y)$.\n",
        "\n",
        "#### Discrete PSF\n",
        "\n",
        "The discrete analog of the continuous PSF can be more conveniently\n",
        "treated with we essentially flatten the the 2D mesh into a 1D vector, a\n",
        "common operation for signal processing. The unflattened case we have:\n",
        "\n",
        "$$ b_{ij} = \\sum_{k=1}^{n} \\sum_{l=1}^{m} \\Delta x \\Delta y G(x_i - x_k, y_j - y_l) u_{kl} $$\n",
        "\n",
        "where $b$ is the blurred image data at the sensor, $u$ is the true image\n",
        "data, and $G$ is the discrete point spread function. If we flatten the\n",
        "2D mesh into a 1D vector we can represent this as a 1D convolution\n",
        "operation: $$ \\vec{b} = \\vec{G} * \\vec{u} $$\n",
        "\n",
        "Since this is a convolution operation, we can process it much more\n",
        "quickly by leveraging the convolution theorem.\n",
        "\n",
        "$$\\begin{align}\n",
        "\\mathcal{F}(\\vec{b}) &= \\mathcal{F}(\\vec{G} * \\vec{u}) \\\\\n",
        "\\mathcal{F}(\\vec{b}) &= \\mathcal{F}(\\vec{G}) \\mathcal{F}(\\vec{u}) \\\\\n",
        "\\vec{b} &= \\mathcal{F}^{-1}(\\mathcal{F}(\\vec{G}) \\odot \\mathcal{F}(\\vec{u}))\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "The $\\odot$ hadamard product is element-wise multiplication, the\n",
        "discrete analog of multiplication of two functions except over an array.\n",
        "\n",
        "### Matrix Representation of Convolution Operation\n",
        "\n",
        "If we flatten the data down into a 1D vector then it is possible to\n",
        "construct a matrix operator that performs the convolution. This is a\n",
        "Toeplitz matrix, a matrix where each descending diagonal from left to\n",
        "right is constant, so that the row vectors represent a sliding window of\n",
        "the convolution kernel. We can flatten out the PSF and construct the\n",
        "matrix using it as the first row entry and then shifting the PSF to the\n",
        "right to fill out the rest of the rows.\n",
        "\n",
        "# Code Implementation"
      ],
      "id": "05d080f5-ce2f-4acf-9afa-fed3374ecfde"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "#matplotlib.use('TkAgg')\n",
        "import numpy as np\n",
        "import torch.optim\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "import copy\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.fft"
      ],
      "id": "0cc82028"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start off by introducing a point spread function within the torch\n",
        "framework. In the case we work with a parameterized gaussian kernel.\n",
        "\n",
        "### Gaussian Example\n",
        "\n",
        "The multivariate extension of the gaussian function is given by:\n",
        "$$f(x) = \\exp\\left(-\\frac{1}{2} (x-\\mu)^T \\Sigma^{-1} (x-\\mu)\\right)$$\n",
        "\n",
        "where $\\mu$ is the mean vector, $x$ is a position vector, and $\\Sigma$\n",
        "is the covariance matrix. The covariance matrix essentially encodes the\n",
        "eigenvectors and corresponding postive eigenvalues of the matrix. The\n",
        "covariance matrix is always symmetric and positive definite. In the\n",
        "context of the code, we are using $C$ as the inverse of the covariance\n",
        "matrix and working with a $\\mu=0$ value."
      ],
      "id": "7f3bfd84-4508-43bf-be38-bd5785bf2426"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.ndimage import convolve\n",
        "\n",
        "def multivariate_gaussian(pos, mean, cov):\n",
        "    \"\"\"Return the multivariate Gaussian distribution on array pos without using einsum notation.\"\"\"\n",
        "    n = mean.shape[0]\n",
        "    diff = pos - mean\n",
        "    cov_inv = np.linalg.inv(cov)\n",
        "    \n",
        "    # Compute the exponent\n",
        "    diff_cov_inv = diff @ cov_inv\n",
        "    exponent = -0.5 * np.sum(diff * diff_cov_inv, axis=-1)\n",
        "    \n",
        "    # Compute the normalization factor\n",
        "    norm_factor = np.sqrt((2 * np.pi) ** n * np.linalg.det(cov))\n",
        "    \n",
        "    # Return the Gaussian function\n",
        "    return np.exp(exponent) / norm_factor\n",
        "\n",
        "# Define the grid limits and resolution\n",
        "X, Y = np.mgrid[-5:5:0.05, -5:5:0.05]\n",
        "pos = np.dstack((X, Y))\n",
        "\n",
        "# Parameters\n",
        "mean = np.array([0, 0])\n",
        "eigenvalues = np.array([1, 2])  # Example eigenvalues\n",
        "principal_axis = np.array([1, 1])  # Example principal axis\n",
        "\n",
        "# Normalize the principal axis\n",
        "principal_axis = principal_axis / np.linalg.norm(principal_axis)\n",
        "\n",
        "# Create the covariance matrix\n",
        "D = np.diag(eigenvalues)\n",
        "orthogonal_complement = np.array([-principal_axis[1], principal_axis[0]])\n",
        "Q = np.column_stack((principal_axis, orthogonal_complement))\n",
        "cov = Q @ D @ Q.T\n",
        "\n",
        "# Compute the Gaussian function over the grid\n",
        "Z = multivariate_gaussian(pos, mean, cov)\n",
        "\n",
        "# Define the Sobel operators for x and y derivatives\n",
        "Kdx = np.array([[-1, 0, 1],\n",
        "                [-2, 0, 2],\n",
        "                [-1, 0, 1]]) / 4.0\n",
        "\n",
        "Kdy = np.array([[-1, -2, -1],\n",
        "                [0,  0,  0],\n",
        "                [1,  2,  1]]) / 4.0\n",
        "\n",
        "# Apply the Sobel filters to compute the derivatives\n",
        "Zdx = convolve(Z, Kdx, mode='constant', cval=0.0)\n",
        "Zdy = convolve(Z, Kdy, mode='constant', cval=0.0)\n",
        "\n",
        "\n",
        "plt.contourf(X, Y, Z, levels=20, cmap='viridis')\n",
        "plt.title('Gaussian Distribution')\n",
        "plt.xlabel('X-axis')\n",
        "plt.ylabel('Y-axis')\n",
        "plt.axis('equal')\n",
        "plt.savefig('figure.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "# Plot the Gaussian and its derivatives\n",
        "plt.figure(figsize=(7.5, 2.5))\n",
        "\n",
        "# Plot the Gaussian\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.contourf(X, Y, Z, levels=20, cmap='viridis')\n",
        "plt.title('Gaussian Distribution')\n",
        "plt.xlabel('X-axis')\n",
        "plt.ylabel('Y-axis')\n",
        "plt.axis('equal')\n",
        "\n",
        "# Plot the derivative in x\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.contourf(X, Y, Zdx, levels=20, cmap='RdBu')\n",
        "plt.title('Derivative in X (Sobel Filter)')\n",
        "plt.xlabel('X-axis')\n",
        "plt.ylabel('Y-axis')\n",
        "plt.axis('equal')\n",
        "\n",
        "# Plot the derivative in y\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.contourf(X, Y, Zdy, levels=20, cmap='RdBu')\n",
        "plt.title('Derivative in Y (Sobel Filter)')\n",
        "plt.xlabel('X-axis')\n",
        "plt.ylabel('Y-axis')\n",
        "plt.axis('equal')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "mv-plot"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extending to Combination of Gaussian and Derivative\n",
        "\n",
        "We can compute the MV gaussian from the inverse covariance matrix $C$\n",
        "with a mean of $\\mu=0$ along with a dimensional scaling metric $t$. For\n",
        "the purposes of forming interesting and varied PSFs, we include the\n",
        "linear combination of the gaussian and a Sobel operator to axpproximate\n",
        "the derivative of the gaussian.\n",
        "\n",
        "$$\\begin{align}\n",
        "S_x &= \\frac{1}{4} \\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1 \\end{bmatrix} \\\\\n",
        "S_y &= \\frac{1}{4} \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{bmatrix}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "These operators act like edge detection or derivatives. The $n_0$,\n",
        "$n_x$, and $n_y$ parameters in the code are used to scale the gaussian\n",
        "and the derivatives."
      ],
      "id": "11f9b03a-b152-490f-a823-77e0eaecb53b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class gaussianConv(nn.Module):\n",
        "    \"\"\"\n",
        "    A PyTorch module that applies a Gaussian convolution to an input image using \n",
        "    a parameterized Gaussian Point Spread Function (PSF). The PSF is derived \n",
        "    from a covariance matrix and the derivatives of the Gaussian are computed \n",
        "    for edge detection.\n",
        "\n",
        "    Args:\n",
        "        C (torch.Tensor): Inverse of covariance matrix used to define the shape of the Gaussian.\n",
        "        t (float, optional): Scaling factor for the Gaussian, default is np.exp(5).\n",
        "        n0 (float, optional): Scaling factor for the original PSF, default is 1.\n",
        "        nx (float, optional): Scaling factor for the derivative along the x-axis, default is 1.\n",
        "        ny (float, optional): Scaling factor for the derivative along the y-axis, default is 1.\n",
        "    \"\"\"\n",
        "    def __init__(self, C, t=np.exp(5), n0=1, nx=1, ny=1):\n",
        "        super(gaussianConv, self).__init__()\n",
        "\n",
        "        self.C = C\n",
        "        self.t = t\n",
        "        self.n0 = n0\n",
        "        self.nx = nx\n",
        "        self.ny = ny\n",
        "\n",
        "    def forward(self, image):\n",
        "        \"\"\"\n",
        "        Apply the Gaussian convolution and derivatives to an input image.\n",
        "\n",
        "        This method performs convolution of the input image with a Gaussian\n",
        "        Point Spread Function (PSF) that includes the original Gaussian and\n",
        "        its derivatives along x and y axes. The convolution is performed\n",
        "        using the Fourier Transform for efficiency.\n",
        "\n",
        "        Args:\n",
        "            image (torch.Tensor): Input image tensor of shape (Batch, Channels, Height, Width).\n",
        "        \n",
        "        Returns:\n",
        "            torch.Tensor: The convolved image of the same shape as the input.\n",
        "        \"\"\"\n",
        "\n",
        "        # Generate the PSF and calculate the center shift required for alignment\n",
        "        P, center = self.psfGauss(image.shape[-1], image.device)\n",
        "\n",
        "        # Shift the PSF so that its center aligns with the origin (top-left corner)\n",
        "        P_shifted = torch.roll(P, shifts=center, dims=[2, 3])\n",
        "\n",
        "        # Compute the Fourier Transform of the shifted PSF\n",
        "        S = torch.fft.fft2(P_shifted)\n",
        "\n",
        "        # Compute the Fourier Transform of the input image\n",
        "        I_fft = torch.fft.fft2(image)\n",
        "\n",
        "        # Multiply the Fourier Transforms element-wise (convolution theorem with Hadamard product)\n",
        "        B_fft = S * I_fft\n",
        "\n",
        "        # Compute the inverse Fourier Transform to get back to the spatial domain\n",
        "        B = torch.real(torch.fft.ifft2(B_fft))\n",
        "\n",
        "        # Return the convolved image\n",
        "        return B\n",
        "\n",
        "    def psfGauss(self, dim, device='cpu'):\n",
        "        \"\"\"\n",
        "        Generate the Gaussian PSF and its derivatives.\n",
        "\n",
        "        Args:\n",
        "            dim (int): Dimension size (assumes square dimensions).\n",
        "            device (str, optional): Device to create tensors on, default is 'cpu'.\n",
        "\n",
        "        Returns:\n",
        "            tuple:\n",
        "                - PSF (torch.Tensor): The combined PSF including derivatives.\n",
        "                - center (list): Shifts required to align the PSF with the origin.\n",
        "        \"\"\"\n",
        "        # Define the size of the PSF kernel (assumed to be square)\n",
        "        m = dim\n",
        "        n = dim\n",
        "\n",
        "        # Create a meshgrid of (X, Y) coordinates\n",
        "        x = torch.arange(-m // 2 + 1, m // 2 + 1, device=device)\n",
        "        y = torch.arange(-n // 2 + 1, n // 2 + 1, device=device)\n",
        "        X, Y = torch.meshgrid(x, y, indexing='ij')\n",
        "        X = X.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, m, n)\n",
        "        Y = Y.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, m, n)\n",
        "\n",
        "        # Extract elements from the covariance matrix\n",
        "        # Assuming self.C is a 2x2 tensor\n",
        "        cx, cy, cxy = self.C[0, 0], self.C[1, 1], self.C[0, 1]\n",
        "\n",
        "        # Compute the Gaussian PSF using the meshgrid and covariance elements\n",
        "        PSF = torch.exp(-self.t * (cx * X ** 2 + cy * Y ** 2 + 2 * cxy * X * Y))\n",
        "\n",
        "        # Normalize the PSF so that its absolute sum is 1\n",
        "        PSF0 = PSF / torch.sum(PSF.abs())\n",
        "\n",
        "        # Define derivative kernels (Sobel operators) for edge detection\n",
        "        Kdx = torch.tensor([[-1, 0, 1],\n",
        "                            [-2, 0, 2],\n",
        "                            [-1, 0, 1]], dtype=PSF0.dtype, device=device) / 4\n",
        "        Kdy = torch.tensor([[-1, -2, -1],\n",
        "                            [0, 0, 0],\n",
        "                            [1, 2, 1]], dtype=PSF0.dtype, device=device) / 4\n",
        "\n",
        "        # Reshape kernels to match convolution requirements\n",
        "        Kdx = Kdx.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, 3, 3)\n",
        "        Kdy = Kdy.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, 3, 3)\n",
        "\n",
        "        # Convolve the PSF with the derivative kernels to obtain derivatives\n",
        "        # Padding ensures the output size matches the input size\n",
        "        PSFdx = F.conv2d(PSF0, Kdx, padding=1)\n",
        "        PSFdy = F.conv2d(PSF0, Kdy, padding=1)\n",
        "\n",
        "        # Combine the original PSF and its derivatives using the scaling factors\n",
        "        PSF_combined = self.n0 * PSF0 + self.nx * PSFdx + self.ny * PSFdy\n",
        "\n",
        "        # Calculate the center shift required to align the PSF with the origin\n",
        "        center = [1 - m // 2, 1 - n // 2]\n",
        "\n",
        "        # Return the combined PSF and center shift\n",
        "        return PSF_combined, center"
      ],
      "id": "f5a3b693"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating a Toy Dataset\n",
        "\n",
        "Often in computational science we test our strategies on toy datasets,\n",
        "simplified data that allows for easier debugging and understanding of\n",
        "the problem at task. In this case, rather than use a real image, we\n",
        "construct a geometric image that will be easier to analyse visually for\n",
        "its correctness when it comes to denoising and deblurring. The dataset\n",
        "is also dimensioned to have a batch and color channel to follow some of\n",
        "the conventions for working with torch tensors, and later some machine\n",
        "learning frameworks. That is $B \\times C \\times H \\times W$, with a\n",
        "single sample, single channel, and a 256x256 image having dimensions\n",
        "$1 \\times 1 \\times 256 \\times 256$."
      ],
      "id": "af20f1f0-70be-43ec-b5a5-377101aea61e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.zeros(1, 1, 256, 256)\n",
        "x[:,:, 120:140, 120:140] = 1.0\n",
        "x[:,:, 100:120, 100:120] = -1.0\n",
        "\n",
        "plt.figure(figsize=(3,3))\n",
        "plt.imshow(x[0,0,:,:])"
      ],
      "id": "toy-dataset"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This simple image is a high and a low signal shown as two square\n",
        "regions, which we will try to recover after applying a point spread\n",
        "function to it (the forward model). The forward model is the convolution\n",
        "of the image with the PSF."
      ],
      "id": "47c590a9-e5ce-4f91-9308-20b06e85e19f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "C = torch.tensor([[1, 0],[0, 1]])\n",
        "Amv = gaussianConv(C, t=0.001,n0=0, nx=1,  ny=-1)\n",
        "\n",
        "y = Amv(x)\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(x[0,0,:,:])\n",
        "plt.colorbar()\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(y[0,0,:,:])\n",
        "plt.colorbar()\n",
        "print()"
      ],
      "id": "forward-model"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Forming a Convolution Matrix\n",
        "\n",
        "Back to the idea of forming a Toeplitz matrix, we first flatten the data\n",
        "to 1D and then recover the matrix in one of two ways. We can work in the\n",
        "spatial domain where the first row of the matrix is determined by the 1D\n",
        "convolution for the first element, then slide the row by one to form the\n",
        "matrix. The matrix can be quite large, since an $n\\times m$ image will\n",
        "have $n \\times m$ elements once flattened, requiring a\n",
        "$(n\\times m) \\times (n\\times m)$ matrix. A reduction in dimension to the\n",
        "$32 \\times 32$ image will help with the computation.\n",
        "\n",
        "Note that we are working with a rolling PSF which has a strange effect\n",
        "in that it assumes a periodic boundary condition in both $x$ and $y$.\n",
        "When it comes to convolution, there are many different ways to treat the\n",
        "boundary condition, such as using zero padding or mirroring the\n",
        "boundary. Coding this by hand is a good exercise to understand the\n",
        "convolution operation, but not the purpose of this exercise.\n",
        "\n",
        "#### Direct Recovery of Convolution Matrix"
      ],
      "id": "70c59ec2-9b6f-438b-959e-8c8804500dcb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dim = 32\n",
        "x = torch.zeros(1, 1, dim, dim)\n",
        "x[:,:, 12:14, 12:14] = 1.0\n",
        "x[:,:, 10:12, 10:12] = -1.0\n",
        "\n",
        "Amv = gaussianConv(C, t=0.1,n0=1, nx=0.1,  ny=0.1)\n",
        "\n",
        "# Flatten the image and the PSF\n",
        "x_flat = x.flatten()\n",
        "\n",
        "kernel, center = Amv.psfGauss(x.shape[-1]) # Get a square conv kernel \n",
        "\n",
        "# Since we are using the conv kernel as a filter operation, we use the transpose of the kernel\n",
        "# to fill the convolution matrix. \n",
        "\n",
        "kernel = kernel.transpose(2,3) \n",
        "# Roll shifts the kernel from the center of the box to the top left corner\n",
        "kernel_shifted = torch.roll(kernel, shifts=center, dims=[2, 3])\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(kernel[0,0,:,:])\n",
        "plt.title('PSF Centered')\n",
        "plt.subplot(1,3,2)\n",
        "plt.title('PSF Shifted with Roll')\n",
        "plt.imshow(kernel_shifted[0,0,:,:])\n",
        "\n",
        "# Flatten the kernel\n",
        "kernel_flat = kernel_shifted.flatten()\n",
        "\n",
        "# Form the convolution matrix\n",
        "n = x_flat.shape[0]\n",
        "m = kernel_flat.shape[0]\n",
        "A_conv = torch.zeros(n, n)\n",
        "\n",
        "for i in range(n):\n",
        "    A_conv[i, :] = torch.roll(kernel_flat, shifts=i, dims=[0])\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(A_conv)\n",
        "plt.title('Convolution Matrix');"
      ],
      "id": "convolution-matrix"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Recovery Using Linearity of Operator\n",
        "\n",
        "Since the convolution operation that is being performed is linear, one\n",
        "way to recover the matrix operator under this assumption is to pass\n",
        "through the basis vectors and recover the column vectors in this\n",
        "fashion:\n",
        "\n",
        "$$\\begin{bmatrix} a_1 \\mid a_2 \\mid \\ldots \\mid a_n \\end{bmatrix} \\mathbf{e}_i = \\mathbf{A} \\mathbf{e}_i  = \\mathbf{a}_i$$\n",
        "\n",
        "where $\\mathbf{e}_i$ is the $i$th basis vector."
      ],
      "id": "e25742d2-bac6-4ebb-9c13-057b3360c9ad"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "A_conv_lin = torch.zeros(n, n)\n",
        "\n",
        "k=0\n",
        "for i in range(x.shape[-2]):\n",
        "  for j in range(x.shape[-1]):\n",
        "    e_ij = torch.zeros_like(x)\n",
        "    e_ij[:,:, i, j] = 1.0\n",
        "    y = Amv(e_ij)\n",
        "    A_conv_lin[:, k] = y.flatten()\n",
        "    k = k+1\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(A_conv_lin)\n",
        "plt.title('Convolution Matrix (Linear)')\n",
        "plt.colorbar()\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(A_conv-A_conv_lin)\n",
        "plt.title('Difference from Direct')\n",
        "plt.colorbar()"
      ],
      "id": "convolution-matrix-2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now comparing this method against the known convolution result using the\n",
        "class defined earlier with the forward model:"
      ],
      "id": "24870089-9c06-4982-8cfd-c4d5f8fa1bd5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "b_forward = Amv(x)\n",
        "\n",
        "b_mat_toeplitz = A_conv @ x_flat\n",
        "b_mat_linear = A_conv_lin @ x_flat\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(b_forward[0,0,:,:])\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(b_mat_toeplitz.reshape(x.shape[-2:]))\n",
        "plt.subplot(1,3,3) \n",
        "plt.imshow(b_mat_linear.reshape(x.shape[-2:]))"
      ],
      "id": "e665072a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that there are some differences between the two methods but\n",
        "in principle they should be the same, (Not sure where the difference is\n",
        "coming from). The important method is actually the one which extracts\n",
        "the columns, as it is more generalizable. So we will continue with that."
      ],
      "id": "450ee5ab-55f3-458c-ab5d-1f23444cf8f1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Amat = A_conv_lin"
      ],
      "id": "final-conv-matrix"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Least Squares Recovery with SVD and Pseudoinverse\n",
        "\n",
        "Now that we have a matrix operator recovered we can formulate the\n",
        "forward problem as $A\\mathbf{x} = \\mathbf{b}$ with our known $A$ and\n",
        "$\\mathbf{b}$, and we want to recover $\\mathbf{x}$. To do this we use the\n",
        "SVD decomposition to gather the pseudo inverse. We can decide to filter\n",
        "out some of the singular values that are very small to improve the\n",
        "conditioning on the matrix as well, using a cutoff value for example.\n",
        "\n",
        "### SVD Decomposition"
      ],
      "id": "773ac535-9962-4402-9cae-8ec635308787"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "U, S, V = torch.svd(Amat.to(torch.float64))\n",
        "b = Amv(x)"
      ],
      "id": "07eb38bd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we make a log plot of the singular values to see how they decay,\n",
        "noting that we lose numerical precision around the $10^{-6}$ mark. We\n",
        "can also asses what the frobenius norm of the difference between the\n",
        "original matrix and the reconstructed matrix is to get a sense of the\n",
        "error in the decomposition and reconstruction."
      ],
      "id": "a288009f-4f55-4954-9acc-200cb607c13c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.semilogy(S)\n",
        "plt.xlabel('Singular Value Index')\n",
        "plt.ylabel('Singular Value')\n",
        "\n",
        "loss = F.mse_loss(Amat, U @ torch.diag(S) @ V.T)\n",
        "print(f\"The loss is {loss}\")"
      ],
      "id": "svd-decomposition"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The loss is quite small which is a good sign that the decomposition is\n",
        "working well within the numerical precision of the machine.\n",
        "\n",
        "### Initial Attempt at Pseudoinverse\n",
        "\n",
        "To recover the original image data we first naively try to invert the\n",
        "matrix to see what happens."
      ],
      "id": "f272bd0a-7bb0-498d-9c2c-c7a20b320515"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xhat = torch.linalg.solve(Amat,b.reshape(dim**2))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(xhat.reshape(x.shape[-2:]))\n",
        "plt.title('Naive Inverse')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(x.reshape(x.shape[-2:]))\n",
        "plt.title('Original Image');"
      ],
      "id": "naive-pseudoinverse"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wow, not even close! This is because the matrix is so ill conditioned\n",
        "that it is effectively low rank and not invertible. We can improve the\n",
        "situation by filtering out the singular values that are very small.\n",
        "\n",
        "### Pseudoinverse with Filtering\n",
        "\n",
        "We can filter out the poor conditioning singular values and exclude\n",
        "those values from the inversion. To get an idea of what the values are\n",
        "doing, we can plot the first few singular values and the corresponding\n",
        "singular vector that they project onto. In the case of the SVD the most\n",
        "important information about the matrix is captured in the left-most\n",
        "vectors of the matrix $U$."
      ],
      "id": "acfa3138-26a8-4e87-a1db-ead1df9c4189"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n= 5\n",
        "for i in range(n):\n",
        "  plt.subplot(1,n,i+1)\n",
        "  plt.imshow(U[:,i+1].reshape(x.shape[-2:]))\n",
        "  plt.title(f'Mode {i}')"
      ],
      "id": "a01e1849"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the inverse problem, the most import singular values are conversely\n",
        "found in the left-most vectors of the matrix $V$. We can also check what\n",
        "the right-most vectors are doing, as they will blow up in value when\n",
        "inverting small singular values. They are high frequency modes of the\n",
        "image, creating the reconstruction issues when they are subjected to\n",
        "error in numerical precision."
      ],
      "id": "2d864541-d6cf-4b33-ba95-7260f7a3c411"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n= 5\n",
        "for i in range(n):\n",
        "  plt.subplot(1,n,i+1)\n",
        "  plt.imshow(V[:,i+1].reshape(x.shape[-2:]))\n",
        "  plt.title(f'Mode {i}')\n",
        "plt.show()\n",
        "\n",
        "for i in range(n):\n",
        "  plt.subplot(1,n,i+1)\n",
        "  plt.imshow(V[:,-(i+1)].reshape(x.shape[-2:]))\n",
        "  plt.title(f'Mode {V.shape[1]-i}')\n",
        "plt.show()"
      ],
      "id": "a9ae512a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These modes are the most important ones, as they contain the big-picture\n",
        "detail without the high frequency noise. We can now filter out the\n",
        "singular values that are very small and invert the matrix to recover the\n",
        "original image."
      ],
      "id": "14a6d9de-92f5-46cb-9e48-98ba80b23e05"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "b_flat = b.flatten().to(torch.float64)\n",
        "x_flat = x.flatten().to(torch.float64)\n",
        "thresholds = [1e-1, 1e-3, 1e-6, 1e-7, 1e-8, 1e-10]\n",
        "\n",
        "plt.figure(figsize=(7,5))  # Adjust the figure size as needed\n",
        "\n",
        "for idx, threshold in enumerate(thresholds):\n",
        "    # Filter the singular values\n",
        "    S_filtered = S.clone()\n",
        "    S_filtered[S_filtered < threshold] = 0\n",
        "\n",
        "    # Compute the reciprocal of the filtered singular values\n",
        "    S_inv = torch.zeros_like(S_filtered)\n",
        "    non_zero_mask = S_filtered > 0\n",
        "    S_inv[non_zero_mask] = 1 / S_filtered[non_zero_mask]\n",
        "\n",
        "    # Construct the pseudoinverse of Amat\n",
        "    A_pinv = V @ torch.diag(S_inv) @ U.T\n",
        "\n",
        "    # Reconstruct the original image\n",
        "    xhat = A_pinv @ b_flat\n",
        "\n",
        "    # Compute the reconstruction error\n",
        "    error = torch.norm(xhat - x_flat, p='fro').item()\n",
        "\n",
        "    # Plot the reconstructed image in the appropriate subplot\n",
        "    plt.subplot(2, 3, idx + 1)  # idx + 1 because subplot indices start at 1\n",
        "    plt.imshow(xhat.reshape(x.shape[-2:]))\n",
        "    plt.title(f'Threshold {threshold}\\nError: {error:.4f}')\n",
        "    plt.colorbar()\n",
        "    plt.axis('off')  # Optionally turn off axis ticks and labels\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "pseudoinverse-filter"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the results, around the $10^{-7}$ mark we start to a peak\n",
        "level of recovery, as measured by the error in the Frobenius norm of the\n",
        "reconstruction. But what happens when we add noise to the data signal?\n",
        "\n",
        "### Adding Noise to the Signal\n",
        "\n",
        "Now we add some noise to the signal and try least squares again for the\n",
        "direct solution"
      ],
      "id": "5c8db622-ae57-41a0-828c-42de3368e040"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "b_flat = b.flatten().to(torch.float64)\n",
        "x_flat = x.flatten().to(torch.float64)\n",
        "Amat = Amat.to(torch.float64)\n",
        "\n",
        "alpha = .01\n",
        "noise = torch.randn_like(b_flat) * alpha\n",
        "\n",
        "H = Amat.T @ Amat + alpha**2 * torch.eye(Amat.shape[0])\n",
        "xhat = torch.linalg.solve(H, Amat.T @ (b_flat + noise))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(x[0,0])\n",
        "plt.title('Original Image')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(xhat.reshape(x.shape[-2:]))\n",
        "plt.title('Reconstructed Image');"
      ],
      "id": "pseudoinverse-filter-noised"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The reconstruction is not very good, the noise has been amplifed all\n",
        "over the image. We can try the pseudoinverse method again with the noise\n",
        "added to the signal."
      ],
      "id": "2541625e-43ba-421d-b6b3-e5f7903c97a5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Amat_noisy = Amat + alpha * torch.eye(Amat.shape[0])\n",
        "Un, Sn, Vn = torch.svd(Amat_noisy)\n",
        "\n",
        "thresholds = [.5, .1, .05, .03, .005, .001]\n",
        "\n",
        "plt.figure(figsize=(7,5))  # Adjust the figure size as needed\n",
        "\n",
        "for idx, threshold in enumerate(thresholds):\n",
        "    # Filter the singular values\n",
        "    S_filtered = Sn.clone()\n",
        "    S_filtered[S_filtered < threshold] = 0\n",
        "\n",
        "    # Compute the reciprocal of the filtered singular values\n",
        "    S_inv = torch.zeros_like(S_filtered)\n",
        "    non_zero_mask = S_filtered > 0\n",
        "    S_inv[non_zero_mask] = 1 / S_filtered[non_zero_mask]\n",
        "\n",
        "    # Construct the pseudoinverse of Amat\n",
        "    A_pinv = Vn @ torch.diag(S_inv) @ Un.T\n",
        "\n",
        "    # Reconstruct the original image\n",
        "    xhat = A_pinv @ (b_flat + noise)\n",
        "\n",
        "    # Compute the reconstruction error\n",
        "    error = torch.norm(xhat - x_flat, p='fro').item()\n",
        "\n",
        "    # Plot the reconstructed image in the appropriate subplot\n",
        "    plt.subplot(2, 3, idx + 1)  # idx + 1 because subplot indices start at 1\n",
        "    plt.imshow(xhat.reshape(x.shape[-2:]))\n",
        "    plt.title(f'Threshold {threshold}\\nError: {error:.4f}')\n",
        "    plt.colorbar()\n",
        "    plt.axis('off')  # Optionally turn off axis ticks and labels\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "pseudoinverse-filter-noised-recovery"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The small addition of noise is quite significant in the recovery\n",
        "threshold for reconstruction. Using a higher threshold for the singular\n",
        "values becomes important when dealing with noise in the signal.\n",
        "Previously numerical precision was the main issue, but now the\n",
        "measurement noise is the main issue."
      ],
      "id": "167c56eb-6e43-476c-b8d4-0e1d075de37e"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "C:"
    }
  }
}