[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Simon Ghyselincks Personal Website",
    "section": "",
    "text": "Welcome to my personal site! I’m Simon Ghyselincks, currently a 5th-year Engineering Physics student at the University of British Columbia (UBC), with a minor in Computer Science. I am studying a cross-disciplinary blend of engineering, computer science, and applied mathematics. What I really love is coding to solve tough problems in robotics, machine learning, signal processing, and more."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Simon Ghyselincks Personal Website",
    "section": "",
    "text": "Welcome to my personal site! I’m Simon Ghyselincks, currently a 5th-year Engineering Physics student at the University of British Columbia (UBC), with a minor in Computer Science. I am studying a cross-disciplinary blend of engineering, computer science, and applied mathematics. What I really love is coding to solve tough problems in robotics, machine learning, signal processing, and more."
  },
  {
    "objectID": "index.html#academics-and-projects",
    "href": "index.html#academics-and-projects",
    "title": "Simon Ghyselincks Personal Website",
    "section": "Academics and Projects",
    "text": "Academics and Projects\nI am currently working with Eldad Haber at UBC Earth and Ocean Sciences on generative AI for geophysical applications. Our work explores the application of recent advances in normalizing flows with stochastic interpolants to generate 3d models of the earth’s crust. I am also continuing to develop our Engineering Physics capstone project “Learning to Balance” which explores the application of reinforcement learning to a reaction wheel robot with complex dynamics. Read more about my projects here.\n\nFeel free to connect with me on LinkedIn or check out my GitHub."
  },
  {
    "objectID": "index.html#my-journey",
    "href": "index.html#my-journey",
    "title": "Simon Ghyselincks Personal Website",
    "section": "My Journey",
    "text": "My Journey\nRead more about my journey and past pursuits here."
  },
  {
    "objectID": "content/projects/RLUnicycle/rtkernel/rtpatch.html",
    "href": "content/projects/RLUnicycle/rtkernel/rtpatch.html",
    "title": "RT Kernel on Jetson Nano",
    "section": "",
    "text": "The following guide is intended to provide step-by-step instructions on how to compile a real-time (RT) Linux kernel for the NVIDIA Jetson Nano. The RT kernel is based on the PREEMPT_RT patch, which adds real-time capabilities to the Linux kernel by making it fully preemptible and reducing the latency of the kernel’s interrupt handling.\nThis guide has been modified from some valuable instructions found at: https://forums.developer.nvidia.com/t/applying-a-preempt-rt-patch-to-jetpack-4-5-on-jetson-nano/168428/4\n\n\nFirst download the BSP from the NVIDIA website. The BSP contains the kernel source code, device tree files, and other necessary files for building the kernel. The BSP also contains the sample root filesystem, which is used to create the final image for the Jetson Nano. You may wish to look up the most recent version of the Tegra for Linux, in this case we are using R32.7.4.\nYou can download all of these files onto a Linux machine specifically running Ubuntu 18.04. Another option that has been tested is compiling on the Jetson Nano itself which is running the correct version of Linux by default. For our project we installed 18.04 on a laptop and compiled the kernel there.\n\n\n\n\n\n\nNote\n\n\n\nSource Files:\nhttps://developer.nvidia.com/embedded/linux-tegra-r3274\n\n\nDownload:\n\nDriver Package (BSP)\nSample Root File System\nDriver Package (BSP) Sources\nGCC Tool Chain can also be obtained via the command line:\nwget http://releases.linaro.org/components/toolchain/binaries/7.3-2018.05/aarch64-linux-gnu/gcc-linaro-7.3.1-2018.05-x86_64_aarch64-linux-gnu.tar.xz\n\nPile all the files into a single directory and install packages\nsudo apt-get update \nsudo apt-get install libncurses5-dev \nsudo apt-get install build-essential \nsudo apt-get install bc \nsudo apt-get install lbzip2 \nsudo apt-get install qemu-user-static \nsudo apt-get install python\n\nmkdir $HOME/jetson_nano \ncd $HOME/jetson_nano\nExtract all of the files\nsudo tar xpf jetson-210_linux_r32.7.4_aarch64.tbz2\ncd Linux_for_Tegra/rootfs/ \nsudo tar xpf ../../tegra_linux_sample-root-filesystem_r32.7.4_aarch64.tbz2\ncd ../../ \ntar -xvf gcc-linaro-7.3.1-2018.05-x86_64_aarch64-linux-gnu.tar.xz \nsudo tar -xjf public_sources.tbz2 \ntar -xjf Linux_for_Tegra/source/public/kernel_src.tbz2\n\n\n\nGo into extracted kernel source and apply RT patch\ncd kernel/kernel-4.9/\n./scripts/rt-patch.sh apply-patches\nConfigure and compile:\nTEGRA_KERNEL_OUT=jetson_nano_kernel \nmkdir $TEGRA_KERNEL_OUT \nexport CROSS_COMPILE=$HOME/jetson_nano/gcc-linaro-7.3.1-2018.05-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu- \nmake ARCH=arm64 O=$TEGRA_KERNEL_OUT tegra_defconfig \nmake ARCH=arm64 O=$TEGRA_KERNEL_OUT menuconfig\nThe menu config opens an old school BIOS menu. Set the proper settings for the RT kernel:\n\nGeneral setup → Timer subsystem → Timer tick handling → Full dynticks system (tickless)\nKernel Features → Preemption Model: Fully Preemptible Kernel (RT)\nKernel Features → Timer frequency: 1000 HZ\n\nAt this point you can go tamper with device tree files (.dtsi) or other things, next step is the compile stage!\n\n\nI tried to modify\ntegra210-porg-gpio-p3448-0000-b00.dtsi \nthe source file, found using a find file function in terminal. It did not fix things. In general the P3450 model requires the p3448-0000-3449-b00 series of files. This was confirmed by looking at all the source configs and scripts.\n\n\n\nmake ARCH=arm64 O=$TEGRA_KERNEL_OUT -j4\n\nsudo cp jetson_nano_kernel/arch/arm64/boot/Image $HOME/jetson_nano/Linux_for_Tegra/kernel/Image\nsudo cp -r jetson_nano_kernel/arch/arm64/boot/dts/* $HOME/jetson_nano/Linux_for_Tegra/kernel/dtb/\nsudo make ARCH=arm64 O=$TEGRA_KERNEL_OUT modules_install INSTALL_MOD_PATH=$HOME/jetson_nano/Linux_for_Tegra/rootfs/\n\ncd $HOME/jetson_nano/Linux_for_Tegra/rootfs/\nsudo tar --owner root --group root -cjf kernel_supplements.tbz2 lib/modules\nsudo mv kernel_supplements.tbz2  ../kernel/\n\ncd ..\nsudo ./apply_binaries.sh\nThe image creator requires the device model. For the 4GB Jetson nano it is -r 300. This will select the correct dtb:\ncd tools\nsudo ./jetson-disk-image-creator.sh -o jetson_nano.img -b jetson-nano -r 300\nIt is crucial to select the correct device tree since it will not boot otherwise. If you are unsure of which to select, follow through the source cocde in the jetson-disk-image-creator.sh to find what the different flags do. Or try the NVIDIA forums but good luck over there!\nUse Balena etcher to put image in $HOME/jetson_nano/Linux_for_Tegra/tools/jetson_nano.img onto the SD card",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "RT Kernel on Jetson Nano"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/rtkernel/rtpatch.html#download-source-files-and-install-packages",
    "href": "content/projects/RLUnicycle/rtkernel/rtpatch.html#download-source-files-and-install-packages",
    "title": "RT Kernel on Jetson Nano",
    "section": "",
    "text": "First download the BSP from the NVIDIA website. The BSP contains the kernel source code, device tree files, and other necessary files for building the kernel. The BSP also contains the sample root filesystem, which is used to create the final image for the Jetson Nano. You may wish to look up the most recent version of the Tegra for Linux, in this case we are using R32.7.4.\nYou can download all of these files onto a Linux machine specifically running Ubuntu 18.04. Another option that has been tested is compiling on the Jetson Nano itself which is running the correct version of Linux by default. For our project we installed 18.04 on a laptop and compiled the kernel there.\n\n\n\n\n\n\nNote\n\n\n\nSource Files:\nhttps://developer.nvidia.com/embedded/linux-tegra-r3274\n\n\nDownload:\n\nDriver Package (BSP)\nSample Root File System\nDriver Package (BSP) Sources\nGCC Tool Chain can also be obtained via the command line:\nwget http://releases.linaro.org/components/toolchain/binaries/7.3-2018.05/aarch64-linux-gnu/gcc-linaro-7.3.1-2018.05-x86_64_aarch64-linux-gnu.tar.xz\n\nPile all the files into a single directory and install packages\nsudo apt-get update \nsudo apt-get install libncurses5-dev \nsudo apt-get install build-essential \nsudo apt-get install bc \nsudo apt-get install lbzip2 \nsudo apt-get install qemu-user-static \nsudo apt-get install python\n\nmkdir $HOME/jetson_nano \ncd $HOME/jetson_nano\nExtract all of the files\nsudo tar xpf jetson-210_linux_r32.7.4_aarch64.tbz2\ncd Linux_for_Tegra/rootfs/ \nsudo tar xpf ../../tegra_linux_sample-root-filesystem_r32.7.4_aarch64.tbz2\ncd ../../ \ntar -xvf gcc-linaro-7.3.1-2018.05-x86_64_aarch64-linux-gnu.tar.xz \nsudo tar -xjf public_sources.tbz2 \ntar -xjf Linux_for_Tegra/source/public/kernel_src.tbz2",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "RT Kernel on Jetson Nano"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/rtkernel/rtpatch.html#apply-rt-patch",
    "href": "content/projects/RLUnicycle/rtkernel/rtpatch.html#apply-rt-patch",
    "title": "RT Kernel on Jetson Nano",
    "section": "",
    "text": "Go into extracted kernel source and apply RT patch\ncd kernel/kernel-4.9/\n./scripts/rt-patch.sh apply-patches\nConfigure and compile:\nTEGRA_KERNEL_OUT=jetson_nano_kernel \nmkdir $TEGRA_KERNEL_OUT \nexport CROSS_COMPILE=$HOME/jetson_nano/gcc-linaro-7.3.1-2018.05-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu- \nmake ARCH=arm64 O=$TEGRA_KERNEL_OUT tegra_defconfig \nmake ARCH=arm64 O=$TEGRA_KERNEL_OUT menuconfig\nThe menu config opens an old school BIOS menu. Set the proper settings for the RT kernel:\n\nGeneral setup → Timer subsystem → Timer tick handling → Full dynticks system (tickless)\nKernel Features → Preemption Model: Fully Preemptible Kernel (RT)\nKernel Features → Timer frequency: 1000 HZ\n\nAt this point you can go tamper with device tree files (.dtsi) or other things, next step is the compile stage!\n\n\nI tried to modify\ntegra210-porg-gpio-p3448-0000-b00.dtsi \nthe source file, found using a find file function in terminal. It did not fix things. In general the P3450 model requires the p3448-0000-3449-b00 series of files. This was confirmed by looking at all the source configs and scripts.\n\n\n\nmake ARCH=arm64 O=$TEGRA_KERNEL_OUT -j4\n\nsudo cp jetson_nano_kernel/arch/arm64/boot/Image $HOME/jetson_nano/Linux_for_Tegra/kernel/Image\nsudo cp -r jetson_nano_kernel/arch/arm64/boot/dts/* $HOME/jetson_nano/Linux_for_Tegra/kernel/dtb/\nsudo make ARCH=arm64 O=$TEGRA_KERNEL_OUT modules_install INSTALL_MOD_PATH=$HOME/jetson_nano/Linux_for_Tegra/rootfs/\n\ncd $HOME/jetson_nano/Linux_for_Tegra/rootfs/\nsudo tar --owner root --group root -cjf kernel_supplements.tbz2 lib/modules\nsudo mv kernel_supplements.tbz2  ../kernel/\n\ncd ..\nsudo ./apply_binaries.sh\nThe image creator requires the device model. For the 4GB Jetson nano it is -r 300. This will select the correct dtb:\ncd tools\nsudo ./jetson-disk-image-creator.sh -o jetson_nano.img -b jetson-nano -r 300\nIt is crucial to select the correct device tree since it will not boot otherwise. If you are unsure of which to select, follow through the source cocde in the jetson-disk-image-creator.sh to find what the different flags do. Or try the NVIDIA forums but good luck over there!\nUse Balena etcher to put image in $HOME/jetson_nano/Linux_for_Tegra/tools/jetson_nano.img onto the SD card",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "RT Kernel on Jetson Nano"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/rtkernel/rtpatch.html#setting-python-scheduling-privileges",
    "href": "content/projects/RLUnicycle/rtkernel/rtpatch.html#setting-python-scheduling-privileges",
    "title": "RT Kernel on Jetson Nano",
    "section": "2.1 Setting Python Scheduling Privileges",
    "text": "2.1 Setting Python Scheduling Privileges\nNote that for this description our team is using Python 3.8 in a virtual environment, the instructions path files may change slightly if using a different version.\nThe scheduling priority is a top-level system command and is usually locked behind ‘sudo’. This is problematic when running a Python script because we don’t want to run it as sudo allowing it full access to wreak havoc on the OS. The solution is to grant only the scheduling part of ‘sudo’ to the Python interpreter:\nThis command only needs to be set once after Python 3.8 is installed (the same in use in our venv): sudo setcap 'cap_sys_nice=eip' /usr/bin/python3.8\n\nsetcap: This is a utility that sets or changes the capabilities of a file/executable. Capabilities are a Linux feature that allow for more fine-grained access control; they provide a way to grant specific privileges to executables that normally only the root user would have.\n'cap_sys_nice=eip': This argument specifies the capabilities to be set on the file, in this case, /usr/bin/python3.8. It’s composed of three parts:\n\ncap_sys_nice: This is the specific capability being set. cap_sys_nice allows the program to raise process nice values (which can deprioritize processes) and change real-time scheduling priorities and policies, without requiring full root privileges.\ne: This stands for “effective” and means the capability is “activated” and can be used by the executable.\ni: This stands for “inheritable”, meaning this capability can be inherited by child processes created by the executable.\np: This stands for “permitted”, which means the capability is allowed for the executable. It’s a part of the set of capabilities that the executable is permitted to use.\n\n/usr/bin/python3.8: This is the path to the Python 3.8 executable. The command sets the specified capabilities on this specific file.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "RT Kernel on Jetson Nano"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/rtkernel/rtpatch.html#setting-script-specific-rt",
    "href": "content/projects/RLUnicycle/rtkernel/rtpatch.html#setting-script-specific-rt",
    "title": "RT Kernel on Jetson Nano",
    "section": "2.2 Setting Script Specific RT",
    "text": "2.2 Setting Script Specific RT\nThe ‘RT’ scheduling priority is code 99. Some imported C implementation allows for resetting the scheduling for the process. The function is wrapped in try/except block to ensure it activates.\n# Define constants for the scheduling policy\nSCHED_FIFO = 1  # FIFO real-time policy\n\nclass SchedParam(ctypes.Structure):\n    _fields_ = [('sched_priority', ctypes.c_int)]\n\ndef set_realtime_priority(priority=99):\n    libc = ctypes.CDLL('libc.so.6')\n    param = SchedParam(priority)\n    # Set the scheduling policy to FIFO and priority for the entire process (0 refers to the current process)\n    if libc.sched_setscheduler(0, SCHED_FIFO, ctypes.byref(param)) != 0:\n        raise ValueError(\"Failed to set real-time priority. Check permissions.\") \nWe run this function at the start of the script which will reassign the scheduling priority to the highest level. This can be verified to work by opening the system monitor and checking the priority of the script such as with htop.\n\n\n\nRT Priority Enabled",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "RT Kernel on Jetson Nano"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html",
    "title": "Dynamics and Control",
    "section": "",
    "text": "The following is a demonstration of the derivation for the equations of motion for a single degree of freedom reaction wheel inverted pendulum. The approach used is energy methods via the Lagrangian using classical mechanics.\nAn automated derivation sequence using MATLAB is presented, which allows for parsing the equations of motion for an arbitrary system such as a 4-DOF unicycle robot. The code for the auto-derivation has been tested by hand against known solutions in the literature, as explored by (Brevik 2017), (Montoya and Gil-González 2020).",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html#mass-and-center-of-mass-measurements",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html#mass-and-center-of-mass-measurements",
    "title": "Dynamics and Control",
    "section": "Mass and Center of Mass Measurements",
    "text": "Mass and Center of Mass Measurements\nThe mass and center of mass (CM) were measured using a lab scale and a balancing method, respectively.\n\nFlywheel: The wheel and rings mass (denoted as \\(m_w\\)) was measured to be 346g. The CM of the wheel from the pendulum hinge (denoted as \\(l_w\\)) is 180mm. This was measured in CAD and also with a ruler.\nPendulum and Motor: The combined mass of the pendulum and motor with stator (denoted as \\(m_p\\)) was measured to be 531g. The CM of the pendulum with motor and stator (denoted as \\(l_p\\)) is 100mm. The pendulum CM is found by balancing the apparatus with removed flywheel overtop of a fulcrum and finding the stable resting point position.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html#inertia-calculations",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html#inertia-calculations",
    "title": "Dynamics and Control",
    "section": "Inertia Calculations",
    "text": "Inertia Calculations\nThe moment of inertia for each component was calculated using the parallel axis theorem and the physical dimensions provided by CAD models and direct measurement.\n\nWheel Inertia\nThe wheel inertia (denoted as \\(I_w\\)) was found by comparing the CAD weight to the measured weight of the flywheel to find agreement: \\[I_w = 725\\ \\text{kg}\\cdot\\text{mm}^2\\] In particular the metal rings were weighed and set to be the same weight in CAD which is the most influential part of the moment in question.\n\n\nPendulum Inertia\nThe pendulum moment of inertia (denoted as \\(I_p\\)) is a composite value derived from the inertia of individual components:\n\nBattery: The battery contributes an inertia of: \\[I_{\\text{battery}} = \\frac{1}{12} \\cdot 0.185 \\cdot (70^2 + 35^2) + 0.185 \\cdot 50^2 = 446\\ \\text{kg}\\cdot\\text{mm}^2\\]\nPendulum Arm: The corrected inertia for the pendulum arm is: \\[I_{\\text{arm}} = 346\\ \\text{kg}\\cdot\\text{mm}^2 + 0.102 \\cdot 45^2 = 552\\ \\text{kg}\\cdot\\text{mm}^2\\]\nMotor and Mount: The combined inertia for the motor and mount is: \\[I_{\\text{motor}} = 0.5 \\cdot 0.206 \\cdot 30^2 + 0.206 \\cdot 75^2 = 1251.75\\ \\text{kg}\\cdot\\text{mm}^2\\]\n\nThe total pendulum inertia is then calculated as the sum of the components: \\[I_p = I_{\\text{battery}} + I_{\\text{arm}} + I_{\\text{motor}} = 2250\\ \\text{kg}\\cdot\\text{mm}^2\\]",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html#kinetic-energy",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html#kinetic-energy",
    "title": "Dynamics and Control",
    "section": "Kinetic Energy",
    "text": "Kinetic Energy\n\\[\\begin{aligned}\nT &= T_p+T_w \\\\\nT_p &= \\frac{1}{2}(\\underbrace{I_p + m_pl_p^2}_{\\text{Parallel Axis Theorem}})\\dot{\\varphi}^2\\\\\nT_w&=\\frac{1}{2}m_w(\\underbrace{l_w\\dot{\\varphi}}_{\\text{Speed of CM}})^2 +  \\frac{1}{2}I_w(\\underbrace{\\dot{\\varphi}+\\dot{\\theta}}_{\\text{net rotation earth frame}})^2\\\\\nT_{net} &= \\frac{1}{2} \\left(I_p + m_p l_p^2 + I_w + m_w l_w^2\\right) \\dot{\\varphi}^2 + \\frac{1}{2} I_w (\\dot{\\varphi} + \\dot{\\theta})^2 \\\\\n&= \\frac{1}{2} \\left(I_p + m_p l_p^2\\right) \\dot{\\varphi}^2 + \\frac{1}{2} I_w \\left(\\dot{\\varphi}^2 + 2\\dot{\\varphi}\\dot{\\theta} + \\dot{\\theta}^2\\right)\\\\\nT_{net}&=\\frac{1}{2} [\\dot{\\varphi}, \\dot{\\theta}] \\begin{bmatrix}\n    I_p + m_pl_p^2 + I_w + m_wl_w^2 & I_w \\\\\n    I_w & I_w\n\\end{bmatrix} \\begin{bmatrix}\n    \\dot{\\varphi} \\\\\n\\dot{\\theta}\n\\end{bmatrix}\n\\end{aligned}\\]\nThis gives the form using the inertia matrix M, note the matrix is always symmetric.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html#potential-energy",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html#potential-energy",
    "title": "Dynamics and Control",
    "section": "Potential Energy",
    "text": "Potential Energy\nThe potential energy is taken by projecting the position of the center of masses onto the vertical axis using \\(\\cos(\\varphi)\\), noting that the angle \\(\\theta\\) has no impact on the potential since the wheel is radially symmetric. \\[U = (m_pl_p + m_wl_w)g \\cos (\\varphi) = m_0 \\cos (\\varphi)\\] We can simplify future equations by assigning an equivalent variable \\(m_0 = (m_p l_p + m_w l_w)g\\)\nThis gives the complete Lagrangian \\[\\mathcal{L}(\\varphi,\\theta,\\dot \\varphi,\\dot \\theta)= KE - PE = \\frac{1}{2}\\mathbf{\\dot q}^{T}\\mathbf{M}\\mathbf{\\dot q}-m_0cos(\\varphi)\\]",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html#matlab-derivation",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html#matlab-derivation",
    "title": "Dynamics and Control",
    "section": "Matlab Derivation",
    "text": "Matlab Derivation\nThe required files to run this code are included at https://github.com/Team-2411-RL-Unicycle/pid-control The automated E-L solver uses a modified version of a file made by (Veng 2023). It is incorporated into the RWIPpid_derivation.m file. The derivation technique is validated against the equations derived by (Brevik 2017).\nThe first step is to define symbolic variables for all of the parameters, states, and inputs\nsyms mp lp Ip mw lw Iw real\nparams = [mp, lp, Ip, mw, lw, Iw];\n% Define numerical values for the parameters\nvalues = [.531, 0.100, 0.002250, .346, 0.180, 0.000725];\ng=9.81;\n% State variables\nsyms phi theta dphi dtheta real\nq = [phi, theta];\ndq = [dphi, dtheta];\n% Input\nsyms tau real\n\n% Potential energy mass\nm0 = (mp*lp + mw*lw)*g; % Effective U=mgh for combined parts\n% Mass matrix\nM = [(Ip + mp*lp^2 + Iw +mw*lw^2), Iw;\n    Iw, Iw];\nlagrangian = (1/2)*([dphi, dtheta])*M*([dphi, dtheta]') - m0 * cos(phi);\n% Non-conservative forces in each coordinate q\nQ = [0, tau];\nThe Lagrangian and its non-conservative forces are fully defined now. The equations are solved using the modified imported library and the solution equations for each second time derivative is solved giving \\(\\frac{d}{dt}\\dot{q}\\), these solutions can be packed into a single array to form a matrix.\n[eqs, ddq] = EulerLagrange(q,dq,lagrangian,Q);\n% Explicit equations:\nexp_eqs = ddq == eqs;\n% Solve equations to isolate ddphi and ddtheta\nddqSolutions = solve(ddq == eqs, ddq);\n% Convert solutions to cell array\nddqSolutionEquations = struct2cell(ddqSolutions) ;\nddqArray = [ddqSolutionEquations{:}].';",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html#derived-equations-of-motion",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html#derived-equations-of-motion",
    "title": "Dynamics and Control",
    "section": "Derived Equations of Motion",
    "text": "Derived Equations of Motion\nOnce we have \\(n\\) 2nd order ODEs for \\(n\\) general coordinates and their \\(n\\) general time derivatives we have enough to make a first order system of ODEs that characterize the system. The time-domain non-linearized result from the derivation is given below.\n\\[\\frac{d}{dt}\\vec{x} = \\vec{G}(\\vec{x}, t) = \\begin{bmatrix}\nd\\varphi \\\\\nd\\theta \\\\\n\\frac{g_0 l_p m_p \\sin(\\phi) - \\tau + g_0 l_w m_w \\sin(\\phi)}{m_p l_p^2 + m_w l_w^2 + I_p} \\\\\n\\frac{m_p \\tau l_p^2 - I_w g_0 m_p \\sin(\\phi) l_p + m_w \\tau l_w^2 - I_w g_0 m_w \\sin(\\phi) l_w + I_p \\tau + I_w \\tau}{I_w (m_p l_p^2 + m_w l_w^2 + I_p)}\n\\end{bmatrix}, \\quad x = \\begin{bmatrix}\n\\varphi \\\\ \\theta \\\\ \\dot{\\varphi} \\\\ \\dot{\\theta}\n\\end{bmatrix}\\]\nNote that there is no explicit time dependence in the function \\(G\\) the inverted pendulum dynamics and rigid body characteristics are constant over time. From inspection of the solutions we see that \\(\\theta\\), the angle of the wheel does not play a role in the function \\(G\\) and can be removed entirely if desired.\nThese system dynamics can be used to create a time-domain non-linear simulation using Euler’s method to get numerical solutions. Friction can be added as a damping coefficient \\(\\beta\\) such that we superimpose \\(\\ddot{\\varphi} = - \\beta \\dot{\\varphi}\\) onto the solution for example.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html#linearization",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html#linearization",
    "title": "Dynamics and Control",
    "section": "Linearization",
    "text": "Linearization\nWe wish to convert \\[\\vec{G}(\\vec{x}, t) \\approx Ax + Bu\\] via linearization about the operating point. We choose the upright position as the target and note that \\(\\varphi\\) is the only variable present in \\(G\\). \\(\\hat{\\vec{x}}=0\\) is the chosen linearization point:\n\\[\\frac{d}{dt}\\vec{x} \\approx \\hat{\\vec{x}} + \\left. \\text{Jacobian}\\{\\vec{G}(\\vec{x}, t)\\} \\right|_{\\vec{x}=\\hat{\\vec{x}}} (\\vec{x}-\\hat{\\vec{x}}) = \\left. \\Big(A\\Big) \\right|_{\\vec{x}=\\hat{\\vec{x}}} \\vec{x}\\]\nWe perform a similar linearization to get the effect of the system inputs by taking the Jacobian with respect to \\(\\tau\\). The two combined give the cannonical \\(\\frac{d}{dt}x = Ax + Bu\\) of controls engineering. The final step is to take the Laplace transform of the entire equation and then solve for the transfer function between the system inputs \\(u\\) or in this case \\(\\tau\\) and the observables we want (mainly the system state \\(x\\)) but this generalizes to any observable that is a function of \\(x\\) and \\(u\\)\nState Vector \\[{\\mathbf{x}} =\n\\begin{bmatrix}\n    x_1 \\\\\n    x_2 \\\\\n    \\vdots\n\\end{bmatrix}\\] Input Vector \\[{\\mathbf{u}} =\n\\begin{bmatrix}\n    u_1 \\\\\n    u_2 \\\\\n    \\vdots\n\\end{bmatrix}\\] Output Vector \\[{\\mathbf{y}} =\n\\begin{bmatrix}\n    y_1 \\\\\n    y_2 \\\\\n    \\vdots\n\\end{bmatrix}\\]\nState Equation \\[\\dot{\\mathbf{x}} =\n\\begin{bmatrix}\n    \\dot{x}_1 \\\\\n    \\dot{x}_2 \\\\\n    \\vdots\n\\end{bmatrix}\n= \\mathbf{A}{\\mathbf{x}} + \\mathbf{B}{\\mathbf{u}}\\] Output Equation \\[{\\mathbf{y}} = \\mathbf{C}{\\mathbf{x}} + \\mathbf{D}{\\mathbf{u}}\\]\nState Transition Matrix \\[\\mathbf{\\Phi} = (s\\mathbf{I} - \\mathbf{A})^{-1}\\] Transfer Functions \\[\\frac{{\\mathbf{y}}}{{\\mathbf{u}}} = \\mathbf{C}\\mathbf{\\Phi}\\mathbf{B} + \\mathbf{D}\\]\nWe solve for the transfer matrix \\(y = Gu\\) at \\(x=0\\), noting that in our case \\(y=x\\)",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html#matlab-derivation-1",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html#matlab-derivation-1",
    "title": "Dynamics and Control",
    "section": "MATLAB Derivation",
    "text": "MATLAB Derivation\n% Phi, dPhi, dTheta\nX = [q(1)' ; dq']\n% The inputs are non-zero entries of Q (non-conservative forces)\nU = Q(Q ~= 0);\n% Vector functionn for the derivative of the state vector\ndX = [dphi; ddqArray]\n\n% Compute the Jacobian matrices to get nonlinear state matrices dX = Ax + Bu\nA = jacobian(dX, X);\nB = jacobian(dX, U);\n\n% Substitute or linearize about an equilibrium point\n% Define equilibrium point (for example, all zeros)\nx0 = [0; 0; 0];\n% Substitute equilibrium values x0 into A and B\nAeq = subs(A, X, x0)\nBeq = subs(B, X, x0)\n\n% U to X transfer function\n% dX = Ax + Bu implies  sX = Ax + Bu, solve for x = Gtf*u\nsyms s\nGtf = (s*eye(length(X)) - Aeq)^(-1)*Beq",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html#system-transfer-function",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html#system-transfer-function",
    "title": "Dynamics and Control",
    "section": "System Transfer Function",
    "text": "System Transfer Function\n\\[\\begin{pmatrix}\n\\varphi(s)\\\\\n\\dot{\\varphi}(s)\\\\\n\\dot\\theta(s)\n\\end{pmatrix}=\n\\begin{pmatrix}\n-\\frac{1}{{m_p l_p^2 s^2 - g_0 m_p l_p + m_w l_w^2 s^2 - g_0 m_w l_w + I_p s^2}} \\\\\n-\\frac{s}{{m_p l_p^2 s^2 - g_0 m_p l_p + m_w l_w^2 s^2 - g_0 m_w l_w + I_p s^2}} \\\\\n\\frac{{m_p l_p^2 + m_w l_w^2 + I_p + I_w}}{{I_w s (m_p l_p^2 + m_w l_w^2 + I_p)}} + \\frac{{g_0 l_p m_p + g_0 l_w m_w}}{{s (m_p l_p^2 + m_w l_w^2 + I_p)(m_p l_p^2 s^2 - g_0 m_p l_p + m_w l_w^2 s^2 - g_0 m_w l_w + I_p s^2)}}\n\\end{pmatrix}\n\\tau(s)\\]\nWe note that for our control problem we are trying to control the angle \\(\\varphi\\) using torque, so the function of interest is the upper row equation:\n\\[\\varphi(s) = \\Big(-\\frac{1}{{m_p l_p^2 s^2 - g_0 m_p l_p + m_w l_w^2 s^2 - g_0 m_w l_w + I_p s^2}}\\Big) \\tau(s)\\]\nOr rearranging we see that we have function of the form \\(\\frac{1}{s^2+a^2}\\): \\[\\varphi(s) = \\left(-\\frac{1}{s^2(m_p l_p^2 + m_w l_w^2 + I_p) - m_0}\\right) \\tau(s)\\]\nThis is a function with one pole in the RH plane making it unstable.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html#pd-controller-for-pendulum-angle",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html#pd-controller-for-pendulum-angle",
    "title": "Dynamics and Control",
    "section": "PD Controller for Pendulum Angle",
    "text": "PD Controller for Pendulum Angle\nThe PD Controller for \\(\\varphi\\) is tuned using the assumption that the torque requests have little delay before reaching the intended value. This is because the motor controller is running at 100 times faster than the main control loop frequency of 100Hz. Thus we model the feedback loop of Controller -&gt; \\(G(s)\\) -&gt; \\(H(s)\\) Sensor Fusion. The sensor fusion and torque request mechanism are modeled as a delay of one \\(100Hz\\) control cycle.\nA PD controller is selected because of the dynamic setpoint that is being controlled by the cascade arrangement. If we were to include an I term then the controller would not be memoryless and would have undesirable response characteristics to the dynamic \\(\\varphi\\) setpoint being requested by the higher level controller. The PD control model is a robust choice for a controller for this robot state parameter, (Brevik 2017).\nThe MATLAB pid tuner is used to get feasible starting values based on this loop. The experimental parameters applied to the robot were found to closely match the predicted values.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html#pi-controller-for-wheel-velocity",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html#pi-controller-for-wheel-velocity",
    "title": "Dynamics and Control",
    "section": "PI Controller for Wheel Velocity",
    "text": "PI Controller for Wheel Velocity\nThe PI controller is tuned heuristically once a good underlying PD controller for the angle is found. A starting value of around \\(K_p = 0.1\\) was found to be helpful. Blending of integral term with a corresponding reduction of \\(P\\) is one approach to further tuning.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/eosc555/lectures/lecture5/index.html",
    "href": "content/eosc555/lectures/lecture5/index.html",
    "title": "Lecture 5: Autodiff and Gauss Newton Optimization",
    "section": "",
    "text": "$$ \n$$"
  },
  {
    "objectID": "content/eosc555/lectures/lecture5/index.html#a-non-linear-dynamics-problem",
    "href": "content/eosc555/lectures/lecture5/index.html#a-non-linear-dynamics-problem",
    "title": "Lecture 5: Autodiff and Gauss Newton Optimization",
    "section": "A Non-Linear Dynamics Problem",
    "text": "A Non-Linear Dynamics Problem\nA well studied problem in non-linear dynamics involves the predator-prey model that is described by the Lotka-Volterra equations. The equations are given by:\n\\[\n\\begin{aligned}\n\\frac{dx}{dt} &= \\alpha x - \\beta xy \\\\\n\\frac{dy}{dt} &= \\delta xy - \\gamma y\n\\end{aligned}\n\\]\nwhere \\(x\\) and \\(y\\) are the populations of the prey and predator respectively. The parameters \\(\\alpha, \\beta, \\gamma, \\delta\\) are positive constants. The goal is to find the values of these parameters that best fit the data.\nThere is no closed form analytic solution that is known to this remarkably simple system of equations, which is why we must resort to numerical solutions to compute the model.\nMore information about the model can be found at the Wikipedia page.\n\nThe Forward Problem\nWe start with an initial time \\(t_0\\) and initial conditions \\(x_0, y_0\\), with parameters \\(\\alpha, \\beta, \\gamma, \\delta\\) to run a forward version of the problem using a variant of the forward Euler method, the RK4.\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\n\nclass LotkaVolterraModel(nn.Module):\n    def __init__(self, alpha, beta, gamma, delta):\n        super(LotkaVolterraModel, self).__init__()\n        # Define parameters as torch tensors that require gradients\n        self.alpha = nn.Parameter(torch.tensor(alpha, dtype=torch.float32))\n        self.beta = nn.Parameter(torch.tensor(beta, dtype=torch.float32))\n        self.gamma = nn.Parameter(torch.tensor(gamma, dtype=torch.float32))\n        self.delta = nn.Parameter(torch.tensor(delta, dtype=torch.float32))\n\n    def forward(self, x, y):\n        # Ensure x and y are tensors\n        if not isinstance(x, torch.Tensor):\n            x = torch.tensor(x, dtype=torch.float32)\n        if not isinstance(y, torch.Tensor):\n            y = torch.tensor(y, dtype=torch.float32)\n\n        # Compute dx and dy based on the current parameters\n        dx = self.alpha * x - self.beta * x * y\n        dy = self.delta * x * y - self.gamma * y\n\n        return dx, dy\n\nclass RK4Solver:\n    def __init__(self, model):\n        self.model = model\n\n    def step(self, x, y, dt):\n        \"\"\"\n        Perform a single RK4 step.\n        \"\"\"\n        # Convert x and y to tensors if they are not already\n        if not isinstance(x, torch.Tensor):\n            x = torch.tensor(x, dtype=torch.float32)\n        if not isinstance(y, torch.Tensor):\n            y = torch.tensor(y, dtype=torch.float32)\n\n        # RK4 Step calculations\n        k1_x, k1_y = self.model.forward(x, y)\n        k2_x, k2_y = self.model.forward(x + 0.5 * dt * k1_x, y + 0.5 * dt * k1_y)\n        k3_x, k3_y = self.model.forward(x + 0.5 * dt * k2_x, y + 0.5 * dt * k2_y)\n        k4_x, k4_y = self.model.forward(x + dt * k3_x, y + dt * k3_y)\n\n        # Update x and y using weighted averages of the slopes\n        x_new = x + (dt / 6) * (k1_x + 2 * k2_x + 2 * k3_x + k4_x)\n        y_new = y + (dt / 6) * (k1_y + 2 * k2_y + 2 * k3_y + k4_y)\n\n        return x_new, y_new\n\n    def solve(self, x0, y0, time_steps):\n        \"\"\"\n        Solve the system over a serie of time steps.\n        Parameters:\n            x0: Initial value of prey population\n            y0: Initial value of predator population\n            time_steps: List or numpy array of time steps to solve over\n        \"\"\"\n        x, y = x0, y0\n        DT = time_steps[1:] - time_steps[:-1]\n        trajectory = torch.zeros(len(time_steps), 2)\n        trajectory[0] = torch.tensor([x, y])\n\n        for i, dt in enumerate(DT):\n            x, y = self.step(x, y, dt)\n            trajectory[i+1] = torch.tensor([x, y])            \n\n        return trajectory\n\n# Define the model parameters\nalpha = 1.0\nbeta = .1\ngamma = 1.5\ndelta = 0.1\n\n# Create the model and solver\nmodel = LotkaVolterraModel(alpha, beta, gamma, delta)\nsolver = RK4Solver(model)\n\n# Define the initial conditions and time steps\nx0 = 5\ny0 = 1\ntime_steps = np.linspace(0, 20, 1000)\n\n# Solve the system\ntrajectory = solver.solve(x0, y0, time_steps)\n\nx_values = trajectory[:, 0].detach().numpy()\ny_values = trajectory[:, 1].detach().numpy()\n\nplt.plot(time_steps, x_values, label='Prey')\nplt.plot(time_steps, y_values, label='Predator')\nplt.xlabel('Time')\nplt.ylabel('Population')\nplt.legend()\nplt.savefig('imgs/lotka_volterra.png')\nplt.show()\n\n\n\n\n\nThe time evolution of the prey and predator populations.\n\n\n\n\nWe can additionally look at the phase space of the system for various initial conditions to see how the different solutions are periodic.\n\n\nShow the code\n# Define the initial conditions\nx0 = 5\ny0 = [.2,.5,1, 2, 3, 4, 5]\n\n# Create the model and solver\nmodel = LotkaVolterraModel(alpha, beta, gamma, delta)\nsolver = RK4Solver(model)\n\n# Define the time steps\ntime_steps = np.linspace(0, 10, 1000)\n\n# Plot the phase space\nplt.figure(figsize=(8, 6))\nfor y in y0:\n    trajectory = solver.solve(x0, y, time_steps)\n    x_values = trajectory[:, 0].detach().numpy()\n    y_values = trajectory[:, 1].detach().numpy()\n    plt.plot(x_values, y_values, label=f'y0={y}')\n\nplt.xlabel('Prey Population')\nplt.ylabel('Predator Population')\nplt.legend()\nplt.title('Lotka-Volterra Phase Space')\nplt.savefig('imgs/lotka_volterra_phase_space.png')\nplt.show()\n\n\n\n\n\nThe phase space of the predator-prey model."
  },
  {
    "objectID": "content/eosc555/lectures/lecture5/index.html#the-inverse-problem",
    "href": "content/eosc555/lectures/lecture5/index.html#the-inverse-problem",
    "title": "Lecture 5: Autodiff and Gauss Newton Optimization",
    "section": "The Inverse Problem",
    "text": "The Inverse Problem\nThe inverse problem in this case is to find the parameters \\(\\alpha, \\beta, \\gamma, \\delta\\) that best fit the data. We suppose that we have a model with parameters that takes in the initial conditions and time steps and returns the trajectory of the system.\n\\[\\frac{d \\vec{x}}{dt} = f(\\vec{x}; \\vec{p}), \\quad \\vec{x}_0 = \\vec{x}(0)\\]\nwhere \\(\\vec{x}\\) is the state of the system and \\(\\vec{p}\\) are the parameters. The goal is to form an esimate of \\(\\vec{p}\\), while the data that we have collected may be sparse, noisy, or incomplete. We represent the incompleteness in the data using the \\(Q\\) sampling operator which is applied to the true underlying data to give \\(Qx\\). If \\(x\\) is fully given then \\(Q=I\\).\n\\[f(\\vec{x}; \\vec{p}) \\cong \\frac{x_{n+1} - x_n}{\\Delta t}\\]\nand we can apply non-linear least squares to try and produce a fit. \\(F(\\vec{p}, x_0) = x(t, \\vec{p})\\) and the observed data is \\(Qx(t)\\). We also make an assumption here that \\(F\\) does not depend on the particular solver that we are using for the forward ODE and that all of the \\(p\\) are physical parameters, we assume that the parameters are faithful enough.\n\\[ \\min_{\\vec{p}} \\frac{1}{2}\\|QF(p)-d\\|^2 = \\min_{p}\\|\\phi(\\vec{p})\\|^2\\]\nSo what we have is a non-linear least squares problem, where we are trying to minimize some mean squared error of a function of the parameters \\(p\\) and the data. The data is fixed for a given problem, so it is only the optimal \\(p\\) that we are trying to find."
  },
  {
    "objectID": "content/eosc555/lectures/lecture5/index.html#minimization-of-the-objective-function",
    "href": "content/eosc555/lectures/lecture5/index.html#minimization-of-the-objective-function",
    "title": "Lecture 5: Autodiff and Gauss Newton Optimization",
    "section": "Minimization of the Objective Function",
    "text": "Minimization of the Objective Function\n\\[ \\min_{p \\in \\mathbb{R}^m} \\biggl\\{ \\sum_{i=1}^n (QF_i(\\mathbf{p}) - d_i) ^2\\biggr\\}\\]\nwhere \\(d_i\\) is the observed data. This is the same as\n\\[ \\min_{p \\in \\mathbb{R}^m} \\|G(\\mathbf{p})\\|^2\\]\nwhere \\(G(\\mathbf{p}) = QF(\\mathbf{p}) - d\\) and \\(d \\in \\mathbb{R}^n\\). We are minimizing the norm of a non-linear function of the parameters. Supposing that we want to find the minimizer, one approach would be by gradient descent.\n\nThe Jacobian: A quick review\n\nThe Jacobian is a multivariate extension of the derivative that extends to functions \\(f : \\mathbb{R}^m \\to \\mathbb{R}^n\\). Because there are \\(n\\) function outputs and \\(m\\) input variables, the Jacobian is an \\(n \\times m\\) matrix that contains the information of how each of the \\(n\\) functions changes with respect to each of the \\(m\\) variables. In an abuse of notation, it can be seen as \\(\\frac{\\partial \\vec{f}}{\\partial \\vec{x}}\\).\n\\[\n\\mathbf{J_f} =\n\\left[\n    \\frac{\\partial f}{\\partial x_1} \\cdots \\frac{\\partial f}{\\partial x_n}\n\\right]\n=\n\\begin{bmatrix}\n    \\nabla^\\top f_1\n    \\\\\n    \\vdots\n    \\\\\n    \\nabla^\\top f_m\n\\end{bmatrix}\n=\n\\left[\n    \\begin{array}{ccc}\n    \\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    \\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n    \\end{array}\n\\right]\n\\]\nNote that like the derivative, the Jacobian is a function of the input variables. The Jacobian is a linear approximation of the function \\(f\\) at a point \\(x_0\\) and can be used to approximate the function at a point \\(x_0 + \\Delta x\\).\n\\[ f(x_0 + \\Delta x) \\approx f(x_0) + J_f(x_0) \\Delta x\\]\nNoting that we are applying matrix multiplication using \\(J_f\\) evaluated at \\(x_0\\) and the vector \\(\\Delta x = \\vec{x} - \\vec{x_0}\\).\n\nWe can compute the gradient of \\(\\|G(\\mathbf{p})\\|^2\\)\n\\[\\begin{align}\n\\nabla_p \\|G(p)\\|^2 &= \\nabla_p G(p)^T G(p)\\\\\n&= \\sum_{i=1}^n \\nabla_p G_i(p)^2\\\\\n&= \\sum_{i=1}^n 2 G_i(p) \\nabla_p G_i(p)\\\\\n&= 2 J_G(p)^T G(p)\n\\end{align}\n\\]\nFrom this stage we could apply gradient descent to find the minimum of the function. However, the function \\(G(p)\\) is non-linear and so the gradient descent method may not converge quickly or the problem may have poor conditioning. The celebrated (Newton’s Method)[https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization#Higher_dimensions] addresses some of these issues, but requires computing the Hessian \\(\\nabla^2 \\|G(p)\\|^2\\) of the function, which can be expensive.\nThe true Hessian of the function is: \\[\n\\nabla^2 \\|G(p)\\|^2 = 2 J_G(p)^T J_G(p) + 2 \\sum_{i=1}^n G_i(p) \\nabla^2 G_i(p)\n\\]\nSo we’d have to compute the Hessian of each of the \\(G_i(p)\\) functions, of which there are \\(n\\), not good in practice. If we did have this Hessian, then the steps with Newton’s method would be:\n\\[ p_{k+1} = p_k - (\\nabla^2 \\|G(p_k)\\|)^{-1} \\nabla (\\|G(p_k)\\|^2)\\]"
  },
  {
    "objectID": "content/eosc555/lectures/lecture5/index.html#gauss-newton-optimization",
    "href": "content/eosc555/lectures/lecture5/index.html#gauss-newton-optimization",
    "title": "Lecture 5: Autodiff and Gauss Newton Optimization",
    "section": "Gauss-Newton Optimization",
    "text": "Gauss-Newton Optimization\nRather than solve this problem directly we can linearize inside of the norm and solve the linearized problem using the normal equations. We approximate the function \\(G(p) = QF(p) - d\\) by a linear function \\(G(p) \\approx (QF(p_k) - d) + QJ_k(p-p_k)\\) where \\(J_k\\) is the Jacobian of \\(F(p)\\) at \\(p_k\\).\n\\[ \\min_{p \\in \\mathbb{R}^m} \\|QF(p_k) - d + QJ_k(p-p_k)\\|^2\\]\nThen rearranging this we get a form that is a linear least squares problem:\n\\[ \\begin{align}\n& \\min_{p \\in \\mathbb{R}^m} \\| QJ_kp - (d + QJ_k p_k- QF(p_k) )\\|^2\\\\\n=& \\min_{p \\in \\mathbb{R}^m} \\| Ap - r_k\\|^2\\\\\nA^T A p =& A^T r_k\n\\end{align}\n\\]\nwhere \\(A = QJ_k\\) and \\(r_k = d + QJ_k p_k - QF(p_k)\\). This is the normal equations for the linear least squares problem. This gives us\n\\[\n\\begin{align}\np_{k+1} &= (A^T A)^{-1} A^T (r_k)\\\\\n&= (J_k^T Q^T Q J_k)^{-1} J_k^T Q^T (d + QJ_k p_k - QF(p_k))\\\\\n&= p_k + (J_k^T Q^T Q J_k)^{-1} J_k^T Q^T (d - QF(p_k))\\\\\np_{k+1} &= p_k - (J_k^T Q^T Q J_k)^{-1} J_k^T Q^T (QF(p_k) - d)\\\\\n\\end{align}\n\\]\nThis could be written in a more tidy and general way, reacalling that \\(G(p) = QF(p) - d\\) and let \\(J_G(p) = QJ(p)\\), then we have:\n\\[ p_{k+1} = p_k - (J_G{p_k}^TJ_G{p_k})^{-1} J_G{p_k} G(p_k)\\]\n\nComparison with Newton’s Method\nSo this resembles a scaled gradient descent. In Newton’s method we have the Hessian, in Gauss-Newton we have the Jacobian of the function. As a comparison:\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nNewton’s Method\nStep Update\n\n\n\n\\(p_{k+1} = p_k - (\\nabla^2 \\|G(p_k)\\|^2)^{-1} \\nabla \\|G(p_k)\\|^2\\)\n\n\n\nScaling Matrix\n\n\n\n\\(\\nabla^2 \\|G(p_k)\\|^2 = 2 J_G(p_k)^T J(p_k) + 2 \\sum_{i=1}^n G_i(p_k) \\nabla^2 G_i(p_k)\\)\n\n\nGauss-Newton Method\nStep Update\n\n\n\n\\(p_{k+1} = p_k - (J_G(p_k)^T J_G(p_k))^{-1} J_G(p_k)^T G(p_k)\\)\n\n\n\nScaling Matrix\n\n\n\n\\(J_G(p_k)^T J_G(p_k)\\)\n\n\n\nThe direction in Newton’s method can be rewritten as \\[d_k = \\left(J_G(p_k)^T J(p_k) + \\sum_{i=1}^n G_i(p_k) \\nabla^2 G_i(p_k)\\right)^{-1} J_G(p_k)^T G(p_k)\\]\nSo we can see that the difference between the two is the omission of the computationally expensive \\(\\sum_{i=1}^n G_i(p) \\nabla^2 G_i(p)\\) terms. The Gauss-Newton method is approximating the second-order approach of Newton’s method by only considering the first-order terms inside of the norm.\n\\[J_G(p_k)^T J(p_k) \\sum_{i=1}^n G_i(p_k) \\nabla^2 G_i(p_k) \\approx J_G(p_k)^T J(p_k)\\]\nRecall that \\(G(p) = QF(p) - d\\) which is the difference between the observed data and the model. If the difference is small then \\(G_i\\) is also small and the approximation is good.\n\n\nAlgorithm for Gauss-Newton\nWe have derived the algorithm for the Gauss-Newton method for solving the non-linear least squares problem. The algorithm is as follows:\n\n\n\\begin{algorithm} \\caption{Gauss-Newton Algorithm for Non-linear Least Squares}\\begin{algorithmic} \\State \\textbf{Input:} Initial guess $p_0$, maximum iterations $K$, tolerance $\\epsilon$ \\State \\textbf{Initialize} $p_0$ \\For{$k = 0, 1, 2, \\ldots$} \\State Compute the Jacobian $J_k$ of $F(p_k)$ \\State Compute the transpose $J_k^T$ of the Jacobian \\State Compute the residual $r_k = d - QF(p_k)$ (forward model) \\State Compute the step $s_k = (J_k^T Q^T Q J_k)^{-1} J_k^T Q^T r_k$ \\State Update the parameters $p_{k+1} = p_k + \\mu_k s_k$ \\If{$\\|s_k\\| &lt; \\epsilon$} \\State \\textbf{Stop} \\EndIf \\EndFor \\State \\textbf{Output:} $p_{k+1}$ as the optimal solution \\end{algorithmic} \\end{algorithm}\n\n\n\n\nMatrix Inversions\nIn practice it may be computationally expensive to invert the matrix \\(J_k^T Q^T Q J_k\\). We can use a conjugate gradient method to solve the normal equations instead. \\[J_k^T Q^T Q J_k s_k = J_k^T Q^T r_k\\]\nWe developed a conjugate gradient method in the last lecture, so we can use that along with the computed values for \\(J_k^T, J_k, r_k\\) to solve the normal equations and get the step \\(s_k\\).\n\n\nSummary\n\n\n\n\n\n\n\n\nComponent\nDescription\nDimensions\n\n\n\n\n\\(d\\)\nObserved data\n\\(\\mathbb{R}^{n}\\)\n\n\n\\(p_k\\)\nParameters\n\\(\\mathbb{R}^{m}\\)\n\n\n\\(Q\\)\nWeight matrix\n\\(\\mathbb{R}^{n \\times n}\\)\n\n\n\\(J_k\\)\nJacobian of \\(F(p_k)\\)\n\\(\\mathbb{R}^{n \\times m}\\)\n\n\n\\(r_k\\)\nResidual \\(d - QF(p_k)\\)\n\\(\\mathbb{R}^{n}\\)\n\n\n\\(F(p_k)\\)\nForward model output\n\\(\\mathbb{R}^{n}\\)\n\n\n\\(s_k\\)\nStep direction\n\\(\\mathbb{R}^{m}\\)\n\n\n\\(J_k^T\\)\nTranspose of the Jacobian\n\\(\\mathbb{R}^{m \\times n}\\)\n\n\n\\(J_k^T Q^T Q J_k\\)\nNormal equations matrix\n\\(\\mathbb{R}^{m \\times m}\\)"
  },
  {
    "objectID": "content/eosc555/lectures/lecture5/index.html#automatic-differentiation",
    "href": "content/eosc555/lectures/lecture5/index.html#automatic-differentiation",
    "title": "Lecture 5: Autodiff and Gauss Newton Optimization",
    "section": "Automatic Differentiation",
    "text": "Automatic Differentiation\nIn practice the Jacobian and its transpose can be computed using automatic differentiation.\nTake the forward model \\(F(p)\\) for which we want a Jacobian matrix at \\(p_k\\). We can write the Taylor expansion of the forward model as:\n\\[ F(p_k + \\epsilon v) = F(p_k) + J_k \\epsilon v + \\mathcal{O}(\\epsilon^2)\\]\nwhere \\(J_k\\) is the Jacobian of \\(F(p_k)\\). If we take the derivative of both sides in this expansion with respect to \\(\\epsilon\\) we get:\n\\[ \\frac{d}{d \\epsilon} F(p_k + \\epsilon v) = J_k v + \\mathcal{O}(\\epsilon)\\]\nIf we make \\(\\epsilon\\) very small then the Jacobian of the forward problem can be numerically approximated and bounded by a small \\(\\mathcal{O}(\\epsilon)\\). The next step to fully recover the Jacobian is to take the gradient with respect to \\(v\\) of the left-hand side of the equation.\n\\[ \\nabla_v \\frac{d}{d \\epsilon} F(p_k + \\epsilon v) = J_k\\]\nThe gradient with respect to \\(v\\) can be traced through with automatic differentiation. So we apply a chain of operations, the pytorch Jacobian vector product, followed by backpropagation on a surrogate \\(v\\) that was passed to the function to get the Jacobian of the forward model. The same principles can be used to recover \\(J_k^T\\).\nThere is also the direct method that is avaible for computing the Jacobian matrix using the torch library. Both cases are shown below. Note that the tensors have a requires_grad=True flag set to allow for the gradients to be computed, it indicates that the tensor is part of the computational graph for backpropagation and tracing by how much each element of \\(v\\) contributed to the jvp result.\n\n\nShow the code\nimport torch\nfrom torch.autograd.functional import jvp\nfrom torch.autograd.functional import jacobian\n\n# Define a simple forward function\ndef F(p):\n    return torch.stack([p[0]**2 + p[1], p[1]**3 + p[0]])\n\n# Input point p_k\np_k = torch.tensor([1.0, 1.0])\n\n# Arbitrary vector v, same size as p_k\nv = torch.tensor([1.0,1.0], requires_grad=True)\n\n# Compute the Jacobian-vector product (J(p) * v)\nF_output, jvp_result = jvp(F, (p_k,), v, create_graph=True)\nprint(\"Function output:\")\nprint(F_output)\nprint(\"Jacobian-vector product:\")\nprint(jvp_result)\n\n# Initialize a list to store each row of the Jacobian\njacobian_rows = []\n# Compute the gradient of each component of the JVP result separately, retaining the graph to avoid re-computation\nfor i in range(F_output.shape[0]):\n    v.grad = None  # Clear the gradient\n    jvp_result.backward(torch.tensor([1.0 if i == j else 0.0 for j in range(F_output.shape[0])]), retain_graph=True)\n    jacobian_rows.append(v.grad.clone())  # Append the gradient (row of the Jacobian)\n\n# Stack the rows to get the full Jacobian matrix\njacobian_matrix = torch.stack(jacobian_rows, dim=0)\n\n# Print the Jacobian matrix\nprint(\"Jacobian matrix at p_k:\")\nprint(jacobian_matrix)\n\n# Compute the full Jacobian matrix directly\njacobian_matrix = jacobian(F, p_k)\n\n# Print the Jacobian matrix\nprint(\"Jacobian matrix at p_k:\")\nprint(jacobian_matrix)\n\n\nFunction output:\ntensor([2., 2.], grad_fn=&lt;StackBackward0&gt;)\nJacobian-vector product:\ntensor([3., 4.], grad_fn=&lt;AddBackward0&gt;)\nJacobian matrix at p_k:\ntensor([[2., 1.],\n        [1., 3.]])\nJacobian matrix at p_k:\ntensor([[2., 1.],\n        [1., 3.]])"
  },
  {
    "objectID": "content/eosc555/lectures/lecture3/index.html",
    "href": "content/eosc555/lectures/lecture3/index.html",
    "title": "Lecture 3: Image Denoising with Gradient Descent and Early Stopping",
    "section": "",
    "text": "Often times we wish to find the gradient of a multi-variable function that is formulated as a linear algebra operation. In this case there are some useful “vector” derivatives and rules that can simplify the process of calculating more complex expressions. The gradient with respect to vector \\(\\mathbf{x}\\) is generally denoted as \\(\\nabla_{\\mathbf{x}}\\) or alternatively \\(\\partial_{\\mathbf{x}}\\), somewhat of an abuse of notation.\n\n\n\\[\\phi(x) = a^\\top x = \\sum_i a_i x_i\\]\nThis is a vector dotproduct and the gradient is simply the vector \\(a\\). There is a subtlety here in that the vector is usually transposed to be a column vector, but this is not always the case. Some people in the field of statistics prefer to use row vector, this can cause some confusion. The general convention is a column vector.\n\\[\\nabla_{\\mathbf{x}} \\phi = a\\]\n\n\n\n\\[\\phi(x) = Ax\\]\nBased on the previous process we are expecting to potentially get \\(A^\\top\\) as the gradient, however the transpose does not occur in this case because we are not returning a vector that needs to be reshaped into a column form.\n\\[\\nabla_{\\mathbf{x}} \\phi = A\\]\n\n\n\nOften we may encounter quadratic linear functions that are of the form: \\[ \\phi(x) = x^\\top A x\\]\nOne way to determine the gradient is to expand the expression and evaluate for a single \\(\\frac{\\partial}{\\partial x_i}\\) term. This method can be found at Mark Schmidt Notes Instead we can apply a chain rule for matrix differentiation that is based on the product rule for differentiation. The chain rule for matrix differentiation is as follows:\n\\[\\frac{d f(g,h)}{d x} = \\frac{d (g(x)^\\top)}{d x} \\frac{\\partial f(g,h)}{\\partial g} + \\frac{d (h(x)^\\top)}{d x} \\frac{\\partial f(g,h)}{\\partial h}\\]\n\\[ \\begin {align*}\n\\phi(x) &= x^\\top A x \\\\\n\\nabla_{\\mathbf{x}} \\phi &= \\nabla_{\\mathbf{x}} (x^\\top A x) \\\\\n&= \\nabla_{\\mathbf{x}} x^\\top (A x) =  \\nabla_{\\mathbf{x}} x^\\top y\\\\\n&= (\\nabla_{\\mathbf{x}} x) \\nabla_{\\mathbf{x}} x^\\top y + \\nabla_{\\mathbf{x}} y^\\top \\nabla_{\\mathbf{y}} x^\\top y\\\\\n&= I y + \\nabla_{\\mathbf{x}} (x^\\top A^\\top) x\\\\\n&= (A x) + A^\\top x\\\\\n&= (A + A^\\top) x\n\\end {align*}\n\\]\nThis fits with the generalization for a scalar quadratic form where we end up with \\((cx^2)' = (c + c^\\top)x = 2cx\\) where \\(c\\) is a scalar.\n\n\n\nAnother form of interest is the hadamard product of two vectors. \\[\\phi(x) = (Ax)^2 = Ax \\odot Ax\\]\nFor this one let \\(y=Ax\\) and we can index each element of the vector \\(y\\) as \\(y_i = \\sum_j A_{ij} x_j\\). The hadamard product is a vector \\(z\\) where \\(z_i = y_i^2\\), we can compute the jacobian since now we are taking the gradient with respect to a vector.\nThe Jacobian will contain the partial derivatives:\n\\[\\frac{d\\vec{z}}{d\\vec{x}} = \\begin{bmatrix} \\frac{\\partial z_1}{\\partial x_1} & \\frac{\\partial z_1}{\\partial x_2} & \\cdots & \\frac{\\partial z_1}{\\partial x_n} \\\\\n\\frac{\\partial z_2}{\\partial x_1} & \\frac{\\partial z_2}{\\partial x_2} & \\cdots & \\frac{\\partial z_2}{\\partial x_n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial z_n}{\\partial x_1} & \\frac{\\partial z_n}{\\partial x_2} & \\cdots & \\frac{\\partial z_n}{\\partial x_n} \\end{bmatrix}\n\\]\nIf we can recover this then we have the gradient of the hadamard product.\n\\[\n\\begin{align*}\nz_i &= y_i^2 = \\left( \\sum_j A_{ij} x_j \\right)^2\\\\\n\\frac{\\partial}{\\partial x_j} y_i^2 &= 2 y_i \\frac{\\partial y_i}{\\partial x_j} = 2 y_i A_{ij}\\\\\n\\frac{d\\vec{z}}{d\\vec{x}} &= 2 \\begin{bmatrix} y_1 A_{1j} & y_1 A_{2j} & \\cdots & y_1 A_{nj} \\\\\ny_2 A_{1j} & y_2 A_{2j} & \\cdots & y_2 A_{nj} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ny_n A_{1j} & y_n A_{2j} & \\cdots & y_n A_{nj} \\end{bmatrix}\\\\\n&= 2 \\cdot \\text{diag}(\\vec{y})A\\\\\n&= 2 \\cdot \\text{diag}(Ax)A\n\\end{align*}\n\\]\n\n\n\nWe look at taking the gradient of the expansion of least squares to find the gradient for this optimization objective.\n\\[\\phi(x) = \\frac{1}{2} ||Ax - b||^2 = \\frac{1}{2} (x^\\top A^\\top A x - 2 b^\\top A x + b^\\top b)\\]\n\\[ \\begin{align*}\n\\nabla_{\\mathbf{x}} \\phi &= \\nabla_{\\mathbf{x}} \\left( \\frac{1}{2} (x^\\top A^\\top A x - 2 b^\\top A x + b^\\top b) \\right)\\\\\n&= \\nabla_{\\mathbf{x}} \\left( \\frac{1}{2} x^\\top A^\\top A x \\right) - \\nabla_{\\mathbf{x}} \\left( b^\\top A x \\right)\\\\\n&= \\frac{1}{2} (A^\\top A + A^\\top A) x - A^\\top b\\\\\n&= A^\\top A x - A^\\top b\\\\\n\\end{align*}\n\\]\nReturning to the first-order optimality condition we have: \\[A^\\top A x = A^\\top b\\]\nAt which point it is in question if \\(A^\\top A\\) is invertible. The invertibility of \\(A^\\top A\\) is determined by the rank of \\(A\\). The rank of A for a non-square matrix is the number of independent columns. If we examine \\(A^\\top Ax = 0\\) then we see that this is only true where the range of \\(A\\) is in the nullspace of \\(A^\\top\\). But \\(N(A^\\top) = R(A)^\\perp\\) so they are orthogonal subspaces and will never coincide unless \\(Ax=0\\). So then \\(A^\\top A x = 0\\) implies that \\(Ax = 0\\) which means that if the null space of \\(A=\\{0\\}\\) then the null space of \\(A^\\top A = \\{0\\}\\) and \\(A^\\top A\\) is invertible. Since \\(A^\\top A\\) is symmetric and positive definite, it is invertible.\n\\(A^\\top A\\) is invertible \\(\\iff\\) \\(A\\) is full rank, that is all the columns are independent. For non-square matrices, an \\(m&gt;n\\) matrix that is wide will trivially not satisfy this condition. A tall matrix \\(m&lt;n\\) will satisfy the condition if the columns are independent."
  },
  {
    "objectID": "content/eosc555/lectures/lecture3/index.html#derivations-of-linear-algebra-gradients",
    "href": "content/eosc555/lectures/lecture3/index.html#derivations-of-linear-algebra-gradients",
    "title": "Lecture 3: Image Denoising with Gradient Descent and Early Stopping",
    "section": "",
    "text": "Often times we wish to find the gradient of a multi-variable function that is formulated as a linear algebra operation. In this case there are some useful “vector” derivatives and rules that can simplify the process of calculating more complex expressions. The gradient with respect to vector \\(\\mathbf{x}\\) is generally denoted as \\(\\nabla_{\\mathbf{x}}\\) or alternatively \\(\\partial_{\\mathbf{x}}\\), somewhat of an abuse of notation.\n\n\n\\[\\phi(x) = a^\\top x = \\sum_i a_i x_i\\]\nThis is a vector dotproduct and the gradient is simply the vector \\(a\\). There is a subtlety here in that the vector is usually transposed to be a column vector, but this is not always the case. Some people in the field of statistics prefer to use row vector, this can cause some confusion. The general convention is a column vector.\n\\[\\nabla_{\\mathbf{x}} \\phi = a\\]\n\n\n\n\\[\\phi(x) = Ax\\]\nBased on the previous process we are expecting to potentially get \\(A^\\top\\) as the gradient, however the transpose does not occur in this case because we are not returning a vector that needs to be reshaped into a column form.\n\\[\\nabla_{\\mathbf{x}} \\phi = A\\]\n\n\n\nOften we may encounter quadratic linear functions that are of the form: \\[ \\phi(x) = x^\\top A x\\]\nOne way to determine the gradient is to expand the expression and evaluate for a single \\(\\frac{\\partial}{\\partial x_i}\\) term. This method can be found at Mark Schmidt Notes Instead we can apply a chain rule for matrix differentiation that is based on the product rule for differentiation. The chain rule for matrix differentiation is as follows:\n\\[\\frac{d f(g,h)}{d x} = \\frac{d (g(x)^\\top)}{d x} \\frac{\\partial f(g,h)}{\\partial g} + \\frac{d (h(x)^\\top)}{d x} \\frac{\\partial f(g,h)}{\\partial h}\\]\n\\[ \\begin {align*}\n\\phi(x) &= x^\\top A x \\\\\n\\nabla_{\\mathbf{x}} \\phi &= \\nabla_{\\mathbf{x}} (x^\\top A x) \\\\\n&= \\nabla_{\\mathbf{x}} x^\\top (A x) =  \\nabla_{\\mathbf{x}} x^\\top y\\\\\n&= (\\nabla_{\\mathbf{x}} x) \\nabla_{\\mathbf{x}} x^\\top y + \\nabla_{\\mathbf{x}} y^\\top \\nabla_{\\mathbf{y}} x^\\top y\\\\\n&= I y + \\nabla_{\\mathbf{x}} (x^\\top A^\\top) x\\\\\n&= (A x) + A^\\top x\\\\\n&= (A + A^\\top) x\n\\end {align*}\n\\]\nThis fits with the generalization for a scalar quadratic form where we end up with \\((cx^2)' = (c + c^\\top)x = 2cx\\) where \\(c\\) is a scalar.\n\n\n\nAnother form of interest is the hadamard product of two vectors. \\[\\phi(x) = (Ax)^2 = Ax \\odot Ax\\]\nFor this one let \\(y=Ax\\) and we can index each element of the vector \\(y\\) as \\(y_i = \\sum_j A_{ij} x_j\\). The hadamard product is a vector \\(z\\) where \\(z_i = y_i^2\\), we can compute the jacobian since now we are taking the gradient with respect to a vector.\nThe Jacobian will contain the partial derivatives:\n\\[\\frac{d\\vec{z}}{d\\vec{x}} = \\begin{bmatrix} \\frac{\\partial z_1}{\\partial x_1} & \\frac{\\partial z_1}{\\partial x_2} & \\cdots & \\frac{\\partial z_1}{\\partial x_n} \\\\\n\\frac{\\partial z_2}{\\partial x_1} & \\frac{\\partial z_2}{\\partial x_2} & \\cdots & \\frac{\\partial z_2}{\\partial x_n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial z_n}{\\partial x_1} & \\frac{\\partial z_n}{\\partial x_2} & \\cdots & \\frac{\\partial z_n}{\\partial x_n} \\end{bmatrix}\n\\]\nIf we can recover this then we have the gradient of the hadamard product.\n\\[\n\\begin{align*}\nz_i &= y_i^2 = \\left( \\sum_j A_{ij} x_j \\right)^2\\\\\n\\frac{\\partial}{\\partial x_j} y_i^2 &= 2 y_i \\frac{\\partial y_i}{\\partial x_j} = 2 y_i A_{ij}\\\\\n\\frac{d\\vec{z}}{d\\vec{x}} &= 2 \\begin{bmatrix} y_1 A_{1j} & y_1 A_{2j} & \\cdots & y_1 A_{nj} \\\\\ny_2 A_{1j} & y_2 A_{2j} & \\cdots & y_2 A_{nj} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ny_n A_{1j} & y_n A_{2j} & \\cdots & y_n A_{nj} \\end{bmatrix}\\\\\n&= 2 \\cdot \\text{diag}(\\vec{y})A\\\\\n&= 2 \\cdot \\text{diag}(Ax)A\n\\end{align*}\n\\]\n\n\n\nWe look at taking the gradient of the expansion of least squares to find the gradient for this optimization objective.\n\\[\\phi(x) = \\frac{1}{2} ||Ax - b||^2 = \\frac{1}{2} (x^\\top A^\\top A x - 2 b^\\top A x + b^\\top b)\\]\n\\[ \\begin{align*}\n\\nabla_{\\mathbf{x}} \\phi &= \\nabla_{\\mathbf{x}} \\left( \\frac{1}{2} (x^\\top A^\\top A x - 2 b^\\top A x + b^\\top b) \\right)\\\\\n&= \\nabla_{\\mathbf{x}} \\left( \\frac{1}{2} x^\\top A^\\top A x \\right) - \\nabla_{\\mathbf{x}} \\left( b^\\top A x \\right)\\\\\n&= \\frac{1}{2} (A^\\top A + A^\\top A) x - A^\\top b\\\\\n&= A^\\top A x - A^\\top b\\\\\n\\end{align*}\n\\]\nReturning to the first-order optimality condition we have: \\[A^\\top A x = A^\\top b\\]\nAt which point it is in question if \\(A^\\top A\\) is invertible. The invertibility of \\(A^\\top A\\) is determined by the rank of \\(A\\). The rank of A for a non-square matrix is the number of independent columns. If we examine \\(A^\\top Ax = 0\\) then we see that this is only true where the range of \\(A\\) is in the nullspace of \\(A^\\top\\). But \\(N(A^\\top) = R(A)^\\perp\\) so they are orthogonal subspaces and will never coincide unless \\(Ax=0\\). So then \\(A^\\top A x = 0\\) implies that \\(Ax = 0\\) which means that if the null space of \\(A=\\{0\\}\\) then the null space of \\(A^\\top A = \\{0\\}\\) and \\(A^\\top A\\) is invertible. Since \\(A^\\top A\\) is symmetric and positive definite, it is invertible.\n\\(A^\\top A\\) is invertible \\(\\iff\\) \\(A\\) is full rank, that is all the columns are independent. For non-square matrices, an \\(m&gt;n\\) matrix that is wide will trivially not satisfy this condition. A tall matrix \\(m&lt;n\\) will satisfy the condition if the columns are independent."
  },
  {
    "objectID": "content/eosc555/lectures/lecture3/index.html#gradient-descent-analysis",
    "href": "content/eosc555/lectures/lecture3/index.html#gradient-descent-analysis",
    "title": "Lecture 3: Image Denoising with Gradient Descent and Early Stopping",
    "section": "Gradient Descent Analysis",
    "text": "Gradient Descent Analysis\nThe standard form of the gradient descent algorithm comes from the field of optimization and can be written as:\n\\[ x_{k+1} = x_k - \\alpha \\nabla_x \\phi(x_k)\\]\nWhere \\(\\alpha\\) is the learning rate, which can be dependent on the problem and the gradient. Substituting the gradient of the least squares problem we have:\n\\[ \\begin{align}\nx_{k+1} &= x_k - \\alpha (A^\\top A x_k - A^\\top b)\\\\\n\\frac{x_{k+1}-x_k}{\\alpha} &= A^\\top b - A^\\top A x_k\\\\\n\\lim_{\\alpha \\to 0} \\frac{x_{k+1}-x_k}{\\alpha} &= \\frac{dx}{dt} = A^\\top (b -A x), \\quad x(0) = x_0\n\\end{align}\n\\]\nThis ODE is the continuous version of the gradient descent algorithm, also known as the gradient flow. Since this a linear first-order ODE we can solve it analytically. The general method for a linear system ODE would be to find the homogeneous solution and the particular solution:\n\\[ \\begin{align}\nx' + A^\\top A x &= A^\\top b\\\\\n\\text{Guess:} x &= v e^{\\lambda t}\\\\\n\\lambda v e^{\\lambda t} + A^\\top A v e^{\\lambda t} &= A^\\top b e^{\\lambda t}\\\\\n\\lambda v + A^\\top A v &= 0 \\qquad \\text{Homogeneous}\\\\\n(\\lambda I + A^\\top A) v &= 0\\\\\n\\lambda &= \\text{eigenvalues of } A^\\top A, \\quad v = \\text{eigenvectors of } A^\\top A\n\\end{align}\n\\]\nBefore continuing further with this line, we can see that the solutions will be closely related to the SVD because it contains the information on these eigenvalues and vectors. So we can try to solve the ODE with the SVD.\n\nSolving the ODE with SVD\n\\[\\begin{align}\nA &= U \\Sigma V^\\top\\\\\nA^TA &= V \\Sigma^2 V^\\top\\\\\n\\frac{d}{dt}x &= V \\Sigma U^\\top b - V \\Sigma^2 V^\\top x\\\\\n\\end{align}\n\\]\nNow let \\(z = V^\\top x\\) and \\(\\hat b = U ^ \\top b\\) then we have:\n\\[\\begin{align}\n\\frac{d}{dt} (V^\\top x) &= \\Sigma \\hat b - \\Sigma^2 (V^\\top x)\\\\\n\\frac{d}{dt} z &= \\Sigma \\hat b - \\Sigma^2 z\\\\\nz' + \\Sigma^2 z &= \\Sigma \\hat b\\\\\n\\end{align}\n\\]\nAt this stage since everything has been diagonalized, all of the equations are decoupled and independent so we can solve for the \\(\\lambda_i\\) cases independently. We find the homogeneous \\(z_h\\) and particular \\(z_p\\) solutions:\n\\[\n\\begin{align}\nz_h' + \\lambda^2 z_h &= 0\\\\\nz_h &= c e^{-\\lambda^2 t}\\\\\nz_p' + \\lambda^2 z_p &= \\lambda \\hat b\\\\\nz_p &= D \\hat b \\\\\n\\lambda^2 D \\hat b &= \\lambda \\hat b\\\\\nD &= \\frac{1}{\\lambda}\\\\\nz_p &= \\frac{1}{\\lambda} \\hat b\n\\end{align}\n\\]\nSo the general solution for the \\(i^{th}\\) component is:\n\\[z_i = c_i e^{-\\lambda_i^2 t} + \\frac{1}{\\lambda_i} \\hat b_i\\]\nSupposing that we start at \\(x=0\\) then we have \\(z=0\\) at all elements and can solve the coefficients \\(c_i\\):\n\\[c_i = -\\frac{1}{\\lambda_i} \\hat b_i\\]\nThen putting it all back together with all the equations we have that\n\\[Z = \\text{diag}\\left( \\lambda_i^{-1} (1 - \\exp (-\\lambda_i t)) \\right) \\hat b\\]\nSubstituting back in for \\(x\\) and \\(b\\) we get:\n\\[x = V \\text{diag}\\left( \\lambda_i^{-1} (1 - \\exp (-\\lambda_i t)) \\right) U^\\top b\\]\nIf we stare at this long enough it begins to look a lot like the pseudoinverse of \\(A\\) from earlier:\n\\(x = V \\Sigma^{-1} U^\\top b\\) except in this case there is a time dependence. At the limit as \\(t \\rightarrow \\infty\\) we have that the exponential term goes to zero and we are left with the pseudoinverse solution. This is a nice way to see that the pseudoinverse is the limit of the gradient descent algorithm. What we may be interested in is what happens at earlier stages since each decay term is dependent on the eigenvalues.\nFor a simple matrix problem we can create a matrix and plot out the time evolution of the diagonals of the matrix that are of interest. In a sense, we have singular values that are time evolving at different rates.\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Seed for reproducibility\nnp.random.seed(4)\n# Create a 5x10 matrix A with random values\nA = np.random.randn(5, 10)\n# Create a vector b of size 5 with random values\nb = np.random.randn(5)\n\n# Compute the SVD of A\nU, S, Vt = np.linalg.svd(A, full_matrices=False)\n\n# Create a time dependent vector of the singular values\ndef St(t):\n    Sdim = S[:, np.newaxis]\n    return (1 - np.exp(-Sdim**2*t)) / Sdim\n\n# Compute the time evolution of the values and plot them on a log scale y axis with a linear time x axis\nt = np.linspace(0, .6, 100)\nT = t[np.newaxis, :]\n\nsingular_vals_t = St(T)\n\n# Initialize the plot\nplt.figure(figsize=(7.5, 4))\n\n# Create a color palette\npalette = sns.color_palette(\"husl\", len(S))\n\n# Plot the singular values and their asymptotes\nfor i in range(len(S)):\n    # Plot the time evolution of each singular value\n    sns.lineplot(x=t, y=singular_vals_t[i, :], color=palette[i], linewidth=2, label=f'$1/S_{i}$ ')\n    \n    Sinv = 1/S[i]\n\n    # Add a horizontal asymptote at the original singular value\n    plt.axhline(y=Sinv, color=palette[i], linestyle='--', linewidth=1)\n    \n    # Annotate the asymptote with the singular value\n    plt.text(t[-1] + 0.02, Sinv, f'{Sinv:.2f}', color=palette[i], va='center')\n\n# Configure plot aesthetics\nplt.xlabel('Time', fontsize=14)\nplt.ylabel('Inverse Singular Vals', fontsize=14)\nplt.title('Time Evolution of Pseudo Inverse in Gradient Flow', fontsize=16)\nplt.legend(title='Inverse Singular Vals', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.xlim(t[0], t[-1] + 0.1)\nplt.tight_layout()\nplt.savefig('imgs/pseudo_inverse_time_evolution.png')\nplt.show()\n\n\n\n\n\n\n\n\n\nSo we can use early stopping to prevent the flow from reaching the optimal point, a very useful technique. When it comes to inverse theory, often we are not interested in the optimal solution, but more interested in getting somewhere close that is not too noisy. This method differs from the thresholded pseudoinverse from the previous lecture, in that it allows some blending of the the smaller singular values, but their propensity for blowing up is controlled by the time exponent and early stopping.\n\n\nExample for Image Recovery using Analytic Solution\nReferring back to the problem of estimating the original image based on a noisy point spread function. We can monitor the time evolution of the estimate using gradient flow. Some code below defines the problem again, with recovery of the SVD decomposition for the 32x32 image, which will be used to solve the ODE for the gradient flow.\n\n\nShow the code\nimport matplotlib.pyplot as plt\nimport matplotlib\n#matplotlib.use('TkAgg')\nimport numpy as np\nimport torch.optim\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nimport copy\n\nimport seaborn as sns\n\nimport math\nimport os\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch.fft\n\nclass gaussianConv(nn.Module):\n    \"\"\"\n    A PyTorch module that applies a Gaussian convolution to an input image using \n    a parameterized Gaussian Point Spread Function (PSF). The PSF is derived \n    from a covariance matrix and the derivatives of the Gaussian are computed \n    for edge detection.\n\n    Args:\n        C (torch.Tensor): Inverse of covariance matrix used to define the shape of the Gaussian.\n        t (float, optional): Scaling factor for the Gaussian, default is np.exp(5).\n        n0 (float, optional): Scaling factor for the original PSF, default is 1.\n        nx (float, optional): Scaling factor for the derivative along the x-axis, default is 1.\n        ny (float, optional): Scaling factor for the derivative along the y-axis, default is 1.\n    \"\"\"\n    def __init__(self, C, t=np.exp(5), n0=1, nx=1, ny=1):\n        super(gaussianConv, self).__init__()\n\n        self.C = C\n        self.t = t\n        self.n0 = n0\n        self.nx = nx\n        self.ny = ny\n\n    def forward(self, image):\n        P, center = self.psfGauss(image.shape[-1], image.device)\n        P_shifted = torch.roll(P, shifts=center, dims=[2, 3])\n        S = torch.fft.fft2(P_shifted)\n        I_fft = torch.fft.fft2(image)\n        B_fft = S * I_fft\n        B = torch.real(torch.fft.ifft2(B_fft))\n\n        return B\n\n    def psfGauss(self, dim, device='cpu'):\n        m = dim\n        n = dim\n\n        # Create a meshgrid of (X, Y) coordinates\n        x = torch.arange(-m // 2 + 1, m // 2 + 1, device=device)\n        y = torch.arange(-n // 2 + 1, n // 2 + 1, device=device)\n        X, Y = torch.meshgrid(x, y, indexing='ij')\n        X = X.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, m, n)\n        Y = Y.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, m, n)\n\n        cx, cy, cxy = self.C[0, 0], self.C[1, 1], self.C[0, 1]\n\n        PSF = torch.exp(-self.t * (cx * X ** 2 + cy * Y ** 2 + 2 * cxy * X * Y))\n        PSF0 = PSF / torch.sum(PSF.abs())\n\n        Kdx = torch.tensor([[-1, 0, 1],\n                            [-2, 0, 2],\n                            [-1, 0, 1]], dtype=PSF0.dtype, device=device) / 4\n        Kdy = torch.tensor([[-1, -2, -1],\n                            [0, 0, 0],\n                            [1, 2, 1]], dtype=PSF0.dtype, device=device) / 4\n\n        Kdx = Kdx.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, 3, 3)\n        Kdy = Kdy.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, 3, 3)\n\n        PSFdx = F.conv2d(PSF0, Kdx, padding=1)\n        PSFdy = F.conv2d(PSF0, Kdy, padding=1)\n\n        PSF_combined = self.n0 * PSF0 + self.nx * PSFdx + self.ny * PSFdy\n\n        center = [1 - m // 2, 1 - n // 2]\n\n        return PSF_combined, center\n\ndim = 32\nx = torch.zeros(1, 1, dim, dim)\nx[:,:, 12:14, 12:14] = 1.0\nx[:,:, 10:12, 10:12] = -1.0\n\nC = torch.tensor([[1, 0],[0, 1]])\nAmv = gaussianConv(C, t=0.1,n0=1, nx=0.1,  ny=0.1)\n\nn=(len(x.flatten()))\nAmat = torch.zeros(n,n)\n\nk=0\nfor i in range(x.shape[-2]):\n  for j in range(x.shape[-1]):\n    e_ij = torch.zeros_like(x)\n    e_ij[:,:, i, j] = 1.0\n    y = Amv(e_ij)\n    Amat[:, k] = y.flatten()\n    k = k+1\n\nU, S, V = torch.svd(Amat.to(torch.float64))\nb = Amv(x)\n\n\nNow that we have the matrix form of the forward operator Amat defined, along with the forward result b and the the decomposition U, S, V we can run the pseudo-inverse gradient flow method as before. So in this case we will be computing:\n\\[ x = V \\text{diag}\\left( \\lambda_i^{-1} (1 - \\exp (-\\lambda_i t)) \\right) U^\\top b\\]\nSince these represents an evolution over time, an animation can be created to show the time evolution of the image recovery, along with the effect of continuing into a region where noise is amplified and dominates.\nRecalling the original and distorted images with a small amount of noise \\(\\epsilon\\) are as follows:\n\n\nShow the code\nplt.figure(figsize=(6, 3))\nplt.subplot(1, 2, 1)\nplt.imshow(x[0, 0], cmap='viridis', vmin=-1, vmax=1)\nplt.title('Original Image')\nplt.axis('off')\nplt.subplot(1, 2, 2)\n\nb_noisy = b+ 0.01 * torch.randn_like(b)\nplt.imshow(b_noisy[0, 0], cmap='viridis', vmin=-1, vmax=1)\nplt.title('Distorted Image')\nplt.axis('off')\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nThe distorted image has had much of its intensity spread out diffusely, so it is only visible as a faint outline. The noise is also visible in the image as a grainy texture. The gradient flow method will attempt to recover the original image from this distorted image.\n\n\nShow the code\nfrom matplotlib import animation\n\nb_flat = b.flatten().to(torch.float64)\nx_flat = x.flatten().to(torch.float64)\nb_noisy = b_flat + 0.001 * torch.randn_like(b_flat)\n\ndef get_xhat(t):\n    Sinv_t = (1 - torch.exp(-S**2 * t)) / S\n    A_pinv = V @ torch.diag(Sinv_t) @ U.T\n    xhat = A_pinv @ b_noisy\n    return xhat\n\n# Time evolution parameters\nnum_frames = 50\nt_vals = np.logspace(0, 6, num_frames)\n\n# Prepare the plot\nfig, ax = plt.subplots(figsize=(6, 6))\nim = ax.imshow(np.zeros((dim, dim)), cmap='viridis', vmin=-1, vmax=1)\nax.set_title('Time Evolution of Pseudo-Inverse Gradient Flow')\nplt.axis('off')\n\n# Initialize the error text\nerror_text = ax.text(0.02, 0.95, '', transform=ax.transAxes, color='blue', fontsize=12,\n                     verticalalignment='top')\n\ntime_text = ax.text(0.5, 0.95, '', transform=ax.transAxes, color='blue', fontsize=12,\n                        verticalalignment='top')\n\n# Initialize containers to track min error and best time\ntracking = {'min_error': float('inf'), 'best_t': 0.0}\n\n# Animation update function\ndef update_frame(t):\n    # Compute time-dependent singular values\n    Sinv_t = (1 - torch.exp(-S ** 2 * t)) / S\n    # Construct the pseudoinverse of Amat at time t\n    A_pinv = V @ torch.diag(Sinv_t) @ U.t()\n    # Reconstruct the image estimate x(t)\n    xt = A_pinv @ b_noisy\n    # Compute the relative error\n    error = torch.norm(x_flat - xt) / torch.norm(x_flat)\n    \n    # Update min_error and best_t if current error is lower\n    if error.item() &lt; tracking['min_error']:\n        tracking['min_error'] = error.item()\n        tracking['best_t'] = t\n\n    # Reshape to image dimensions\n    x_image = xt.reshape(dim, dim).detach().numpy()\n\n    # Update the image data\n    im.set_data(x_image)\n\n    # Update the error text\n    error_text.set_text(f'Relative Error: {error.item():.4f}')\n    time_text.set_text(f'Time: {t:.2f}')\n\n    return [im, error_text, time_text]\n\n# Create the animation\nani = animation.FuncAnimation(fig, update_frame, frames=t_vals, blit=True, interval=100)\n\nani.save('imgs/gradient_flow.gif', writer='pillow', fps=5)\nplt.close(fig)\n\n\n\nAnd we saved the best time that was discovered for the recovery (with prior knowledge of the ground truth). So we can inspect that image, this was the best that we could do with the gradient flow method.\n\n\nShow the code\nbest_img = get_xhat(tracking['best_t']).reshape(dim, dim).detach().numpy()\n\nplt.figure(figsize=(6, 6))\nplt.imshow(best_img / np.max(np.abs(best_img)), cmap='viridis', vmin=-1, vmax=1)\nplt.title(f'Best Reconstruction at t={tracking[\"best_t\"]:.2f}\\nRelative Error: {tracking[\"min_error\"]:.4f}')\nplt.axis('off')\nplt.show()"
  },
  {
    "objectID": "content/eosc555/lectures/lecture3/index.html#recovery-of-the-adjoint-operator-using-autograd",
    "href": "content/eosc555/lectures/lecture3/index.html#recovery-of-the-adjoint-operator-using-autograd",
    "title": "Lecture 3: Image Denoising with Gradient Descent and Early Stopping",
    "section": "Recovery of the Adjoint Operator using Autograd",
    "text": "Recovery of the Adjoint Operator using Autograd\nIn this case we were able to compute the matrix form of \\(A\\) and use its transpose to compute the SVD, but in many cases this might be too expensive or there may not be a closed form analytic solution to the early stopping technique. In such cases we wish to recover the adjoint. The question then is how to recover the adjoint operator from the Amv operator? There are helpful tools available through the use of automatic differentiation to track the gradients of the forward operator and recover the adjoint operator. This is a very powerful tool that can be used to recover the adjoint operator in a very general way.\nBy definition the adjoint has the property that: \\[\\langle Ax, v \\rangle = \\langle x, A^\\top v \\rangle\\]\n\nExplicit Computation of the Adjoint\nWe can compute the adjoint explicitly for the Amv operator based on its computation from earlier. The discrete fourier transform matrix operator \\(F\\) has the property that \\(F^{-1} = F^\\top\\) so we can use this to compute the adjoint.\n\\[\n\\begin{align}\nA(x) &= \\mathcal{F}^-1 \\left( \\mathcal{F}(P) \\odot \\mathcal{F}(x) \\right)\\\\\n&= F^\\top \\left( \\text{diag} (F(P)) F(x) \\right)\\\\\nA^\\top(v) &= F^\\top \\text{diag} (F(P))^* F v\\\\\n\\end{align}\n\\]\nWhere the hadamard operation of the two vectors has been modified to a matrix form by diagonalizing the vector \\(F(P)\\) that is the Fourier transform of the point spread function. From this form it is posible to take the adjoint of the operator by taking the complex conjugate of the transpose of the entire operation.\n\n\nAutograd Computation of the Adjoint\nWe start with a new function \\(h = v^\\top A(x)\\) and we wish to compute the gradient of \\(h\\) with respect to \\(x\\).\n\\[ \\nabla_x h = \\nabla_x (v^\\top A(x)) = A^\\top(v)\\]\nThe gradient of \\(h\\) with respect to \\(x\\) is the adjoint operator \\(A^\\top(v)\\). We can use the torch.autograd.grad function to compute the gradient of \\(h\\) with respect to \\(x\\).\n\n\nShow the code\ndef Amv_adjoint(v):\n    x = torch.zeros(1, 1, dim, dim)\n    x.requires_grad = True\n    b = Amv(x)\n    # Compute the dot product of the forward operator with the input vector\n    h = torch.sum(b * v)\n    # Compute the gradient of the dot product with respect to the input image\n    adjoint = torch.autograd.grad(h, x, create_graph=True)[0]\n    return adjoint\n\n\nWe can use this to recover \\(A^\\top\\) for the general case if we run the operator on the set of basis vectors in the image space. This will give us the adjoint operator in the form of a matrix. We can also use it to confirm that it recovers the matrix transpose of the forward operator if we are working with a simple matrix, reusing the Amat matrix from earlier to take its transpose and compare it to the adjoint operator.\n\n\nShow the code\nAmat_adj = torch.zeros(n,n)\n\ndim = 32 # Same as earlier\nk=0\nfor i in range(dim):\n  for j in range(dim):\n    e_ij = torch.zeros_like(x)\n    e_ij[:,:, i, j] = 1.0\n    y = Amv_adjoint(e_ij)\n    Amat_adj[:, k] = y.flatten()\n    k = k+1\n\ndiff = torch.norm(Amat_adj - Amat.T)\nprint(f'Norm of difference between adjoint and transpose: {diff:.2e}')\n\n\nNorm of difference between adjoint and transpose: 4.43e-07\n\n\nSo the difference is within the bounds of numerical precison and the code appears to be working correctly."
  },
  {
    "objectID": "content/eosc555/lectures/lecture3/index.html#gradient-descent-with-adjoint",
    "href": "content/eosc555/lectures/lecture3/index.html#gradient-descent-with-adjoint",
    "title": "Lecture 3: Image Denoising with Gradient Descent and Early Stopping",
    "section": "Gradient Descent with Adjoint",
    "text": "Gradient Descent with Adjoint\nWe can now use the defined operators (functions) from earlier to setup a simple gradient descent algorithm with a step size and early stopping to produce a recovery image that bypasses the need to compute the SVD decomposition, which may be very expensive for large matrices.\n\n\nShow the code\nfrom tqdm import tqdm\n\ndef least_squares_sol(x0, b, Amv, Amv_adjoint, max_iter=1000, alpha=1e-3, tol=1e-6):\n    \"\"\"\n    Solves the least squares problem using gradient descent with progress tracking.\n\n    Parameters:\n    - x0 (torch.Tensor): Initial guess for the solution.\n    - b (torch.Tensor): Observation vector.\n    - Amv (callable): Function to compute A @ x.\n    - Amv_adjoint (callable): Function to compute A^T @ v.\n    - max_iter (int): Maximum number of iterations.\n    - alpha (float): Learning rate.\n    - tol (float): Tolerance for convergence.\n\n    Returns:\n    - x (torch.Tensor): Approximated solution vector.\n    \"\"\"\n    x = x0.clone()\n    x.requires_grad = True\n    b_noisy = b.clone() + 0.01 * torch.randn_like(b)\n\n    # Initialize the progress bar\n    with tqdm(total=max_iter, desc='Least Squares Iteration', unit='iter') as pbar:\n        for i in range(max_iter):\n            # Gradient descent update\n            residual = Amv(x) - b_noisy\n            gradient = Amv_adjoint(residual)\n            xnext = x - alpha * gradient\n\n            # Compute relative error\n            error = torch.norm(xnext - x) \n\n            # Update the progress bar with the current error\n            pbar.set_postfix({'Error': f'{error.item():.4e}'})\n            pbar.update(1);\n\n            # Check for convergence\n            if error &lt; tol:\n                pbar.write(f'Converged at iteration {i+1} with error {error.item():.4e}')\n                x = xnext\n                break\n\n            x = xnext\n\n    return x\n\nb = Amv(x)\nx0 = torch.zeros_like(x)\nxhat = least_squares_sol(x0, b, Amv, Amv_adjoint, max_iter=1000, alpha=1, tol=1e-6)\n\nplt.figure(figsize=(6,3))\nplt.subplot(1, 2, 1)\nplt.imshow(x[0, 0], cmap='viridis', vmin=-1, vmax=1)\nplt.title('Original Image')\nplt.axis('off')\nplt.subplot(1, 2, 2)\nplt.imshow(xhat.detach().numpy()[0, 0], cmap='viridis', vmin=-1, vmax=1)\nplt.title('Recovered Image')\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n\nLeast Squares Iteration:   0%|          | 0/1000 [00:00&lt;?, ?iter/s]Least Squares Iteration:   0%|          | 0/1000 [00:00&lt;?, ?iter/s, Error=2.1563e-01]Least Squares Iteration:   0%|          | 1/1000 [00:00&lt;00:04, 219.83iter/s, Error=1.2599e-01]Least Squares Iteration:   0%|          | 2/1000 [00:00&lt;00:03, 303.86iter/s, Error=9.0033e-02]Least Squares Iteration:   0%|          | 3/1000 [00:00&lt;00:02, 345.11iter/s, Error=7.0516e-02]Least Squares Iteration:   0%|          | 4/1000 [00:00&lt;00:02, 392.76iter/s, Error=5.8476e-02]Least Squares Iteration:   0%|          | 5/1000 [00:00&lt;00:02, 410.06iter/s, Error=5.0304e-02]Least Squares Iteration:   1%|          | 6/1000 [00:00&lt;00:02, 422.22iter/s, Error=4.4323e-02]Least Squares Iteration:   1%|          | 7/1000 [00:00&lt;00:02, 406.40iter/s, Error=3.9702e-02]Least Squares Iteration:   1%|          | 8/1000 [00:00&lt;00:02, 409.52iter/s, Error=3.6006e-02]Least Squares Iteration:   1%|          | 9/1000 [00:00&lt;00:02, 433.07iter/s, Error=3.2978e-02]Least Squares Iteration:   1%|          | 10/1000 [00:00&lt;00:02, 438.77iter/s, Error=3.0457e-02]Least Squares Iteration:   1%|          | 11/1000 [00:00&lt;00:02, 443.66iter/s, Error=2.8329e-02]Least Squares Iteration:   1%|          | 12/1000 [00:00&lt;00:02, 450.71iter/s, Error=2.6512e-02]Least Squares Iteration:   1%|▏         | 13/1000 [00:00&lt;00:02, 452.52iter/s, Error=2.4943e-02]Least Squares Iteration:   1%|▏         | 14/1000 [00:00&lt;00:02, 455.73iter/s, Error=2.3576e-02]Least Squares Iteration:   2%|▏         | 15/1000 [00:00&lt;00:02, 454.41iter/s, Error=2.2371e-02]Least Squares Iteration:   2%|▏         | 16/1000 [00:00&lt;00:02, 439.31iter/s, Error=2.1301e-02]Least Squares Iteration:   2%|▏         | 17/1000 [00:00&lt;00:02, 451.96iter/s, Error=2.0342e-02]Least Squares Iteration:   2%|▏         | 18/1000 [00:00&lt;00:02, 453.20iter/s, Error=1.9476e-02]Least Squares Iteration:   2%|▏         | 19/1000 [00:00&lt;00:02, 455.17iter/s, Error=1.8689e-02]Least Squares Iteration:   2%|▏         | 20/1000 [00:00&lt;00:02, 447.02iter/s, Error=1.7969e-02]Least Squares Iteration:   2%|▏         | 21/1000 [00:00&lt;00:02, 449.30iter/s, Error=1.7307e-02]Least Squares Iteration:   2%|▏         | 22/1000 [00:00&lt;00:02, 455.90iter/s, Error=1.6694e-02]Least Squares Iteration:   2%|▏         | 23/1000 [00:00&lt;00:02, 443.75iter/s, Error=1.6126e-02]Least Squares Iteration:   2%|▏         | 24/1000 [00:00&lt;00:02, 442.15iter/s, Error=1.5596e-02]Least Squares Iteration:   2%|▎         | 25/1000 [00:00&lt;00:02, 451.48iter/s, Error=1.5101e-02]Least Squares Iteration:   3%|▎         | 26/1000 [00:00&lt;00:02, 443.46iter/s, Error=1.4636e-02]Least Squares Iteration:   3%|▎         | 27/1000 [00:00&lt;00:02, 445.37iter/s, Error=1.4200e-02]Least Squares Iteration:   3%|▎         | 28/1000 [00:00&lt;00:02, 444.71iter/s, Error=1.3788e-02]Least Squares Iteration:   3%|▎         | 29/1000 [00:00&lt;00:02, 444.69iter/s, Error=1.3399e-02]Least Squares Iteration:   3%|▎         | 30/1000 [00:00&lt;00:02, 435.33iter/s, Error=1.3031e-02]Least Squares Iteration:   3%|▎         | 31/1000 [00:00&lt;00:02, 437.11iter/s, Error=1.2682e-02]Least Squares Iteration:   3%|▎         | 32/1000 [00:00&lt;00:02, 438.84iter/s, Error=1.2351e-02]Least Squares Iteration:   3%|▎         | 33/1000 [00:00&lt;00:02, 440.48iter/s, Error=1.2036e-02]Least Squares Iteration:   3%|▎         | 34/1000 [00:00&lt;00:02, 442.02iter/s, Error=1.1737e-02]Least Squares Iteration:   4%|▎         | 35/1000 [00:00&lt;00:02, 443.34iter/s, Error=1.1451e-02]Least Squares Iteration:   4%|▎         | 36/1000 [00:00&lt;00:02, 449.26iter/s, Error=1.1179e-02]Least Squares Iteration:   4%|▎         | 37/1000 [00:00&lt;00:02, 445.03iter/s, Error=1.0919e-02]Least Squares Iteration:   4%|▍         | 38/1000 [00:00&lt;00:02, 443.92iter/s, Error=1.0670e-02]Least Squares Iteration:   4%|▍         | 39/1000 [00:00&lt;00:02, 443.44iter/s, Error=1.0433e-02]Least Squares Iteration:   4%|▍         | 40/1000 [00:00&lt;00:02, 444.69iter/s, Error=1.0205e-02]Least Squares Iteration:   4%|▍         | 41/1000 [00:00&lt;00:02, 445.82iter/s, Error=9.9872e-03]Least Squares Iteration:   4%|▍         | 42/1000 [00:00&lt;00:02, 446.98iter/s, Error=9.7783e-03]Least Squares Iteration:   4%|▍         | 43/1000 [00:00&lt;00:02, 446.45iter/s, Error=9.5779e-03]Least Squares Iteration:   4%|▍         | 44/1000 [00:00&lt;00:02, 447.52iter/s, Error=9.3856e-03]Least Squares Iteration:   4%|▍         | 45/1000 [00:00&lt;00:02, 444.15iter/s, Error=9.2009e-03]Least Squares Iteration:   5%|▍         | 46/1000 [00:00&lt;00:02, 454.02iter/s, Error=9.2009e-03]Least Squares Iteration:   5%|▍         | 46/1000 [00:00&lt;00:02, 454.02iter/s, Error=9.0235e-03]Least Squares Iteration:   5%|▍         | 47/1000 [00:00&lt;00:02, 454.02iter/s, Error=8.8529e-03]Least Squares Iteration:   5%|▍         | 48/1000 [00:00&lt;00:02, 454.02iter/s, Error=8.6888e-03]Least Squares Iteration:   5%|▍         | 49/1000 [00:00&lt;00:02, 454.02iter/s, Error=8.5310e-03]Least Squares Iteration:   5%|▌         | 50/1000 [00:00&lt;00:02, 454.02iter/s, Error=8.3790e-03]Least Squares Iteration:   5%|▌         | 51/1000 [00:00&lt;00:02, 454.02iter/s, Error=8.2326e-03]Least Squares Iteration:   5%|▌         | 52/1000 [00:00&lt;00:02, 454.02iter/s, Error=8.0915e-03]Least Squares Iteration:   5%|▌         | 53/1000 [00:00&lt;00:02, 454.02iter/s, Error=7.9555e-03]Least Squares Iteration:   5%|▌         | 54/1000 [00:00&lt;00:02, 454.02iter/s, Error=7.8244e-03]Least Squares Iteration:   6%|▌         | 55/1000 [00:00&lt;00:02, 454.02iter/s, Error=7.6978e-03]Least Squares Iteration:   6%|▌         | 56/1000 [00:00&lt;00:02, 454.02iter/s, Error=7.5757e-03]Least Squares Iteration:   6%|▌         | 57/1000 [00:00&lt;00:02, 454.02iter/s, Error=7.4578e-03]Least Squares Iteration:   6%|▌         | 58/1000 [00:00&lt;00:02, 454.02iter/s, Error=7.3438e-03]Least Squares Iteration:   6%|▌         | 59/1000 [00:00&lt;00:02, 454.02iter/s, Error=7.2337e-03]Least Squares Iteration:   6%|▌         | 60/1000 [00:00&lt;00:02, 454.02iter/s, Error=7.1272e-03]Least Squares Iteration:   6%|▌         | 61/1000 [00:00&lt;00:02, 454.02iter/s, Error=7.0243e-03]Least Squares Iteration:   6%|▌         | 62/1000 [00:00&lt;00:02, 454.02iter/s, Error=6.9246e-03]Least Squares Iteration:   6%|▋         | 63/1000 [00:00&lt;00:02, 454.02iter/s, Error=6.8282e-03]Least Squares Iteration:   6%|▋         | 64/1000 [00:00&lt;00:02, 454.02iter/s, Error=6.7348e-03]Least Squares Iteration:   6%|▋         | 65/1000 [00:00&lt;00:02, 454.02iter/s, Error=6.6443e-03]Least Squares Iteration:   7%|▋         | 66/1000 [00:00&lt;00:02, 454.02iter/s, Error=6.5567e-03]Least Squares Iteration:   7%|▋         | 67/1000 [00:00&lt;00:02, 454.02iter/s, Error=6.4717e-03]Least Squares Iteration:   7%|▋         | 68/1000 [00:00&lt;00:02, 454.02iter/s, Error=6.3893e-03]Least Squares Iteration:   7%|▋         | 69/1000 [00:00&lt;00:02, 454.02iter/s, Error=6.3093e-03]Least Squares Iteration:   7%|▋         | 70/1000 [00:00&lt;00:02, 454.02iter/s, Error=6.2316e-03]Least Squares Iteration:   7%|▋         | 71/1000 [00:00&lt;00:02, 454.02iter/s, Error=6.1563e-03]Least Squares Iteration:   7%|▋         | 72/1000 [00:00&lt;00:02, 454.02iter/s, Error=6.0830e-03]Least Squares Iteration:   7%|▋         | 73/1000 [00:00&lt;00:02, 454.02iter/s, Error=6.0119e-03]Least Squares Iteration:   7%|▋         | 74/1000 [00:00&lt;00:02, 454.02iter/s, Error=5.9427e-03]Least Squares Iteration:   8%|▊         | 75/1000 [00:00&lt;00:02, 454.02iter/s, Error=5.8755e-03]Least Squares Iteration:   8%|▊         | 76/1000 [00:00&lt;00:02, 454.02iter/s, Error=5.8100e-03]Least Squares Iteration:   8%|▊         | 77/1000 [00:00&lt;00:02, 454.02iter/s, Error=5.7463e-03]Least Squares Iteration:   8%|▊         | 78/1000 [00:00&lt;00:02, 454.02iter/s, Error=5.6843e-03]Least Squares Iteration:   8%|▊         | 79/1000 [00:00&lt;00:02, 454.02iter/s, Error=5.6240e-03]Least Squares Iteration:   8%|▊         | 80/1000 [00:00&lt;00:02, 454.02iter/s, Error=5.5651e-03]Least Squares Iteration:   8%|▊         | 81/1000 [00:00&lt;00:02, 454.02iter/s, Error=5.5078e-03]Least Squares Iteration:   8%|▊         | 82/1000 [00:00&lt;00:02, 454.02iter/s, Error=5.4519e-03]Least Squares Iteration:   8%|▊         | 83/1000 [00:00&lt;00:02, 454.02iter/s, Error=5.3974e-03]Least Squares Iteration:   8%|▊         | 84/1000 [00:00&lt;00:02, 454.02iter/s, Error=5.3442e-03]Least Squares Iteration:   8%|▊         | 85/1000 [00:00&lt;00:02, 454.02iter/s, Error=5.2923e-03]Least Squares Iteration:   9%|▊         | 86/1000 [00:00&lt;00:02, 454.02iter/s, Error=5.2416e-03]Least Squares Iteration:   9%|▊         | 87/1000 [00:00&lt;00:02, 454.02iter/s, Error=5.1921e-03]Least Squares Iteration:   9%|▉         | 88/1000 [00:00&lt;00:02, 454.02iter/s, Error=5.1437e-03]Least Squares Iteration:   9%|▉         | 89/1000 [00:00&lt;00:02, 454.02iter/s, Error=5.0964e-03]Least Squares Iteration:   9%|▉         | 90/1000 [00:00&lt;00:02, 454.02iter/s, Error=5.0502e-03]Least Squares Iteration:   9%|▉         | 91/1000 [00:00&lt;00:02, 454.02iter/s, Error=5.0050e-03]Least Squares Iteration:   9%|▉         | 92/1000 [00:00&lt;00:02, 424.05iter/s, Error=5.0050e-03]Least Squares Iteration:   9%|▉         | 92/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.9608e-03]Least Squares Iteration:   9%|▉         | 93/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.9175e-03]Least Squares Iteration:   9%|▉         | 94/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.8751e-03]Least Squares Iteration:  10%|▉         | 95/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.8337e-03]Least Squares Iteration:  10%|▉         | 96/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.7930e-03]Least Squares Iteration:  10%|▉         | 97/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.7532e-03]Least Squares Iteration:  10%|▉         | 98/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.7142e-03]Least Squares Iteration:  10%|▉         | 99/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.6760e-03]Least Squares Iteration:  10%|█         | 100/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.6385e-03]Least Squares Iteration:  10%|█         | 101/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.6017e-03]Least Squares Iteration:  10%|█         | 102/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.5656e-03]Least Squares Iteration:  10%|█         | 103/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.5302e-03]Least Squares Iteration:  10%|█         | 104/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.4955e-03]Least Squares Iteration:  10%|█         | 105/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.4614e-03]Least Squares Iteration:  11%|█         | 106/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.4279e-03]Least Squares Iteration:  11%|█         | 107/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.3949e-03]Least Squares Iteration:  11%|█         | 108/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.3626e-03]Least Squares Iteration:  11%|█         | 109/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.3308e-03]Least Squares Iteration:  11%|█         | 110/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.2996e-03]Least Squares Iteration:  11%|█         | 111/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.2688e-03]Least Squares Iteration:  11%|█         | 112/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.2386e-03]Least Squares Iteration:  11%|█▏        | 113/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.2089e-03]Least Squares Iteration:  11%|█▏        | 114/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.1797e-03]Least Squares Iteration:  12%|█▏        | 115/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.1509e-03]Least Squares Iteration:  12%|█▏        | 116/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.1226e-03]Least Squares Iteration:  12%|█▏        | 117/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.0948e-03]Least Squares Iteration:  12%|█▏        | 118/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.0673e-03]Least Squares Iteration:  12%|█▏        | 119/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.0403e-03]Least Squares Iteration:  12%|█▏        | 120/1000 [00:00&lt;00:02, 424.05iter/s, Error=4.0137e-03]Least Squares Iteration:  12%|█▏        | 121/1000 [00:00&lt;00:02, 424.05iter/s, Error=3.9875e-03]Least Squares Iteration:  12%|█▏        | 122/1000 [00:00&lt;00:02, 424.05iter/s, Error=3.9617e-03]Least Squares Iteration:  12%|█▏        | 123/1000 [00:00&lt;00:02, 424.05iter/s, Error=3.9362e-03]Least Squares Iteration:  12%|█▏        | 124/1000 [00:00&lt;00:02, 424.05iter/s, Error=3.9112e-03]Least Squares Iteration:  12%|█▎        | 125/1000 [00:00&lt;00:02, 424.05iter/s, Error=3.8865e-03]Least Squares Iteration:  13%|█▎        | 126/1000 [00:00&lt;00:02, 424.05iter/s, Error=3.8621e-03]Least Squares Iteration:  13%|█▎        | 127/1000 [00:00&lt;00:02, 424.05iter/s, Error=3.8381e-03]Least Squares Iteration:  13%|█▎        | 128/1000 [00:00&lt;00:02, 424.05iter/s, Error=3.8144e-03]Least Squares Iteration:  13%|█▎        | 129/1000 [00:00&lt;00:02, 424.05iter/s, Error=3.7910e-03]Least Squares Iteration:  13%|█▎        | 130/1000 [00:00&lt;00:02, 424.05iter/s, Error=3.7680e-03]Least Squares Iteration:  13%|█▎        | 131/1000 [00:00&lt;00:02, 424.05iter/s, Error=3.7453e-03]Least Squares Iteration:  13%|█▎        | 132/1000 [00:00&lt;00:02, 424.05iter/s, Error=3.7228e-03]Least Squares Iteration:  13%|█▎        | 133/1000 [00:00&lt;00:02, 424.05iter/s, Error=3.7007e-03]Least Squares Iteration:  13%|█▎        | 134/1000 [00:00&lt;00:02, 424.05iter/s, Error=3.6789e-03]Least Squares Iteration:  14%|█▎        | 135/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.6789e-03]Least Squares Iteration:  14%|█▎        | 135/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.6573e-03]Least Squares Iteration:  14%|█▎        | 136/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.6361e-03]Least Squares Iteration:  14%|█▎        | 137/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.6151e-03]Least Squares Iteration:  14%|█▍        | 138/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.5943e-03]Least Squares Iteration:  14%|█▍        | 139/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.5739e-03]Least Squares Iteration:  14%|█▍        | 140/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.5537e-03]Least Squares Iteration:  14%|█▍        | 141/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.5337e-03]Least Squares Iteration:  14%|█▍        | 142/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.5140e-03]Least Squares Iteration:  14%|█▍        | 143/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.4945e-03]Least Squares Iteration:  14%|█▍        | 144/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.4753e-03]Least Squares Iteration:  14%|█▍        | 145/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.4563e-03]Least Squares Iteration:  15%|█▍        | 146/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.4375e-03]Least Squares Iteration:  15%|█▍        | 147/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.4190e-03]Least Squares Iteration:  15%|█▍        | 148/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.4006e-03]Least Squares Iteration:  15%|█▍        | 149/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.3825e-03]Least Squares Iteration:  15%|█▌        | 150/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.3646e-03]Least Squares Iteration:  15%|█▌        | 151/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.3469e-03]Least Squares Iteration:  15%|█▌        | 152/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.3295e-03]Least Squares Iteration:  15%|█▌        | 153/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.3122e-03]Least Squares Iteration:  15%|█▌        | 154/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.2951e-03]Least Squares Iteration:  16%|█▌        | 155/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.2782e-03]Least Squares Iteration:  16%|█▌        | 156/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.2615e-03]Least Squares Iteration:  16%|█▌        | 157/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.2450e-03]Least Squares Iteration:  16%|█▌        | 158/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.2286e-03]Least Squares Iteration:  16%|█▌        | 159/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.2125e-03]Least Squares Iteration:  16%|█▌        | 160/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.1965e-03]Least Squares Iteration:  16%|█▌        | 161/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.1807e-03]Least Squares Iteration:  16%|█▌        | 162/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.1651e-03]Least Squares Iteration:  16%|█▋        | 163/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.1496e-03]Least Squares Iteration:  16%|█▋        | 164/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.1344e-03]Least Squares Iteration:  16%|█▋        | 165/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.1192e-03]Least Squares Iteration:  17%|█▋        | 166/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.1043e-03]Least Squares Iteration:  17%|█▋        | 167/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.0895e-03]Least Squares Iteration:  17%|█▋        | 168/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.0748e-03]Least Squares Iteration:  17%|█▋        | 169/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.0604e-03]Least Squares Iteration:  17%|█▋        | 170/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.0460e-03]Least Squares Iteration:  17%|█▋        | 171/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.0319e-03]Least Squares Iteration:  17%|█▋        | 172/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.0178e-03]Least Squares Iteration:  17%|█▋        | 173/1000 [00:00&lt;00:02, 393.64iter/s, Error=3.0039e-03]Least Squares Iteration:  17%|█▋        | 174/1000 [00:00&lt;00:02, 393.64iter/s, Error=2.9902e-03]Least Squares Iteration:  18%|█▊        | 175/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.9902e-03]Least Squares Iteration:  18%|█▊        | 175/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.9766e-03]Least Squares Iteration:  18%|█▊        | 176/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.9632e-03]Least Squares Iteration:  18%|█▊        | 177/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.9499e-03]Least Squares Iteration:  18%|█▊        | 178/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.9367e-03]Least Squares Iteration:  18%|█▊        | 179/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.9236e-03]Least Squares Iteration:  18%|█▊        | 180/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.9107e-03]Least Squares Iteration:  18%|█▊        | 181/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.8979e-03]Least Squares Iteration:  18%|█▊        | 182/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.8853e-03]Least Squares Iteration:  18%|█▊        | 183/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.8728e-03]Least Squares Iteration:  18%|█▊        | 184/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.8604e-03]Least Squares Iteration:  18%|█▊        | 185/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.8481e-03]Least Squares Iteration:  19%|█▊        | 186/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.8360e-03]Least Squares Iteration:  19%|█▊        | 187/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.8240e-03]Least Squares Iteration:  19%|█▉        | 188/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.8121e-03]Least Squares Iteration:  19%|█▉        | 189/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.8003e-03]Least Squares Iteration:  19%|█▉        | 190/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.7886e-03]Least Squares Iteration:  19%|█▉        | 191/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.7771e-03]Least Squares Iteration:  19%|█▉        | 192/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.7656e-03]Least Squares Iteration:  19%|█▉        | 193/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.7543e-03]Least Squares Iteration:  19%|█▉        | 194/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.7431e-03]Least Squares Iteration:  20%|█▉        | 195/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.7320e-03]Least Squares Iteration:  20%|█▉        | 196/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.7210e-03]Least Squares Iteration:  20%|█▉        | 197/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.7101e-03]Least Squares Iteration:  20%|█▉        | 198/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.6994e-03]Least Squares Iteration:  20%|█▉        | 199/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.6887e-03]Least Squares Iteration:  20%|██        | 200/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.6781e-03]Least Squares Iteration:  20%|██        | 201/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.6676e-03]Least Squares Iteration:  20%|██        | 202/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.6573e-03]Least Squares Iteration:  20%|██        | 203/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.6470e-03]Least Squares Iteration:  20%|██        | 204/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.6369e-03]Least Squares Iteration:  20%|██        | 205/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.6268e-03]Least Squares Iteration:  21%|██        | 206/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.6168e-03]Least Squares Iteration:  21%|██        | 207/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.6069e-03]Least Squares Iteration:  21%|██        | 208/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.5972e-03]Least Squares Iteration:  21%|██        | 209/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.5875e-03]Least Squares Iteration:  21%|██        | 210/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.5779e-03]Least Squares Iteration:  21%|██        | 211/1000 [00:00&lt;00:02, 359.51iter/s, Error=2.5684e-03]Least Squares Iteration:  21%|██        | 212/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.5684e-03]Least Squares Iteration:  21%|██        | 212/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.5590e-03]Least Squares Iteration:  21%|██▏       | 213/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.5496e-03]Least Squares Iteration:  21%|██▏       | 214/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.5404e-03]Least Squares Iteration:  22%|██▏       | 215/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.5313e-03]Least Squares Iteration:  22%|██▏       | 216/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.5222e-03]Least Squares Iteration:  22%|██▏       | 217/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.5132e-03]Least Squares Iteration:  22%|██▏       | 218/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.5043e-03]Least Squares Iteration:  22%|██▏       | 219/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.4955e-03]Least Squares Iteration:  22%|██▏       | 220/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.4868e-03]Least Squares Iteration:  22%|██▏       | 221/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.4782e-03]Least Squares Iteration:  22%|██▏       | 222/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.4696e-03]Least Squares Iteration:  22%|██▏       | 223/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.4611e-03]Least Squares Iteration:  22%|██▏       | 224/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.4527e-03]Least Squares Iteration:  22%|██▎       | 225/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.4444e-03]Least Squares Iteration:  23%|██▎       | 226/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.4361e-03]Least Squares Iteration:  23%|██▎       | 227/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.4280e-03]Least Squares Iteration:  23%|██▎       | 228/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.4199e-03]Least Squares Iteration:  23%|██▎       | 229/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.4118e-03]Least Squares Iteration:  23%|██▎       | 230/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.4039e-03]Least Squares Iteration:  23%|██▎       | 231/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.3960e-03]Least Squares Iteration:  23%|██▎       | 232/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.3882e-03]Least Squares Iteration:  23%|██▎       | 233/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.3805e-03]Least Squares Iteration:  23%|██▎       | 234/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.3728e-03]Least Squares Iteration:  24%|██▎       | 235/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.3652e-03]Least Squares Iteration:  24%|██▎       | 236/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.3577e-03]Least Squares Iteration:  24%|██▎       | 237/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.3503e-03]Least Squares Iteration:  24%|██▍       | 238/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.3429e-03]Least Squares Iteration:  24%|██▍       | 239/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.3355e-03]Least Squares Iteration:  24%|██▍       | 240/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.3283e-03]Least Squares Iteration:  24%|██▍       | 241/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.3211e-03]Least Squares Iteration:  24%|██▍       | 242/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.3140e-03]Least Squares Iteration:  24%|██▍       | 243/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.3069e-03]Least Squares Iteration:  24%|██▍       | 244/1000 [00:00&lt;00:02, 315.15iter/s, Error=2.2999e-03]Least Squares Iteration:  24%|██▍       | 245/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.2999e-03]Least Squares Iteration:  24%|██▍       | 245/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.2930e-03]Least Squares Iteration:  25%|██▍       | 246/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.2861e-03]Least Squares Iteration:  25%|██▍       | 247/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.2793e-03]Least Squares Iteration:  25%|██▍       | 248/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.2726e-03]Least Squares Iteration:  25%|██▍       | 249/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.2659e-03]Least Squares Iteration:  25%|██▌       | 250/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.2592e-03]Least Squares Iteration:  25%|██▌       | 251/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.2527e-03]Least Squares Iteration:  25%|██▌       | 252/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.2462e-03]Least Squares Iteration:  25%|██▌       | 253/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.2397e-03]Least Squares Iteration:  25%|██▌       | 254/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.2333e-03]Least Squares Iteration:  26%|██▌       | 255/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.2270e-03]Least Squares Iteration:  26%|██▌       | 256/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.2207e-03]Least Squares Iteration:  26%|██▌       | 257/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.2145e-03]Least Squares Iteration:  26%|██▌       | 258/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.2083e-03]Least Squares Iteration:  26%|██▌       | 259/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.2022e-03]Least Squares Iteration:  26%|██▌       | 260/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.1961e-03]Least Squares Iteration:  26%|██▌       | 261/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.1901e-03]Least Squares Iteration:  26%|██▌       | 262/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.1841e-03]Least Squares Iteration:  26%|██▋       | 263/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.1782e-03]Least Squares Iteration:  26%|██▋       | 264/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.1723e-03]Least Squares Iteration:  26%|██▋       | 265/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.1665e-03]Least Squares Iteration:  27%|██▋       | 266/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.1608e-03]Least Squares Iteration:  27%|██▋       | 267/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.1551e-03]Least Squares Iteration:  27%|██▋       | 268/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.1494e-03]Least Squares Iteration:  27%|██▋       | 269/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.1438e-03]Least Squares Iteration:  27%|██▋       | 270/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.1382e-03]Least Squares Iteration:  27%|██▋       | 271/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.1327e-03]Least Squares Iteration:  27%|██▋       | 272/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.1272e-03]Least Squares Iteration:  27%|██▋       | 273/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.1218e-03]Least Squares Iteration:  27%|██▋       | 274/1000 [00:00&lt;00:02, 290.22iter/s, Error=2.1164e-03]Least Squares Iteration:  28%|██▊       | 275/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.1164e-03]Least Squares Iteration:  28%|██▊       | 275/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.1111e-03]Least Squares Iteration:  28%|██▊       | 276/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.1058e-03]Least Squares Iteration:  28%|██▊       | 277/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.1006e-03]Least Squares Iteration:  28%|██▊       | 278/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.0954e-03]Least Squares Iteration:  28%|██▊       | 279/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.0902e-03]Least Squares Iteration:  28%|██▊       | 280/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.0851e-03]Least Squares Iteration:  28%|██▊       | 281/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.0800e-03]Least Squares Iteration:  28%|██▊       | 282/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.0750e-03]Least Squares Iteration:  28%|██▊       | 283/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.0700e-03]Least Squares Iteration:  28%|██▊       | 284/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.0651e-03]Least Squares Iteration:  28%|██▊       | 285/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.0602e-03]Least Squares Iteration:  29%|██▊       | 286/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.0553e-03]Least Squares Iteration:  29%|██▊       | 287/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.0505e-03]Least Squares Iteration:  29%|██▉       | 288/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.0457e-03]Least Squares Iteration:  29%|██▉       | 289/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.0409e-03]Least Squares Iteration:  29%|██▉       | 290/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.0362e-03]Least Squares Iteration:  29%|██▉       | 291/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.0315e-03]Least Squares Iteration:  29%|██▉       | 292/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.0269e-03]Least Squares Iteration:  29%|██▉       | 293/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.0223e-03]Least Squares Iteration:  29%|██▉       | 294/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.0178e-03]Least Squares Iteration:  30%|██▉       | 295/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.0132e-03]Least Squares Iteration:  30%|██▉       | 296/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.0087e-03]Least Squares Iteration:  30%|██▉       | 297/1000 [00:00&lt;00:02, 272.42iter/s, Error=2.0043e-03]Least Squares Iteration:  30%|██▉       | 298/1000 [00:00&lt;00:02, 272.42iter/s, Error=1.9999e-03]Least Squares Iteration:  30%|██▉       | 299/1000 [00:00&lt;00:02, 272.42iter/s, Error=1.9955e-03]Least Squares Iteration:  30%|███       | 300/1000 [00:00&lt;00:02, 272.42iter/s, Error=1.9912e-03]Least Squares Iteration:  30%|███       | 301/1000 [00:00&lt;00:02, 272.42iter/s, Error=1.9868e-03]Least Squares Iteration:  30%|███       | 302/1000 [00:00&lt;00:02, 272.42iter/s, Error=1.9826e-03]Least Squares Iteration:  30%|███       | 303/1000 [00:00&lt;00:02, 258.60iter/s, Error=1.9826e-03]Least Squares Iteration:  30%|███       | 303/1000 [00:00&lt;00:02, 258.60iter/s, Error=1.9783e-03]Least Squares Iteration:  30%|███       | 304/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.9741e-03]Least Squares Iteration:  30%|███       | 305/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.9699e-03]Least Squares Iteration:  31%|███       | 306/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.9658e-03]Least Squares Iteration:  31%|███       | 307/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.9617e-03]Least Squares Iteration:  31%|███       | 308/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.9576e-03]Least Squares Iteration:  31%|███       | 309/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.9535e-03]Least Squares Iteration:  31%|███       | 310/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.9495e-03]Least Squares Iteration:  31%|███       | 311/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.9455e-03]Least Squares Iteration:  31%|███       | 312/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.9416e-03]Least Squares Iteration:  31%|███▏      | 313/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.9377e-03]Least Squares Iteration:  31%|███▏      | 314/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.9338e-03]Least Squares Iteration:  32%|███▏      | 315/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.9299e-03]Least Squares Iteration:  32%|███▏      | 316/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.9260e-03]Least Squares Iteration:  32%|███▏      | 317/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.9222e-03]Least Squares Iteration:  32%|███▏      | 318/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.9185e-03]Least Squares Iteration:  32%|███▏      | 319/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.9147e-03]Least Squares Iteration:  32%|███▏      | 320/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.9110e-03]Least Squares Iteration:  32%|███▏      | 321/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.9073e-03]Least Squares Iteration:  32%|███▏      | 322/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.9036e-03]Least Squares Iteration:  32%|███▏      | 323/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.9000e-03]Least Squares Iteration:  32%|███▏      | 324/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.8963e-03]Least Squares Iteration:  32%|███▎      | 325/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.8928e-03]Least Squares Iteration:  33%|███▎      | 326/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.8892e-03]Least Squares Iteration:  33%|███▎      | 327/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.8856e-03]Least Squares Iteration:  33%|███▎      | 328/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.8821e-03]Least Squares Iteration:  33%|███▎      | 329/1000 [00:01&lt;00:02, 258.60iter/s, Error=1.8786e-03]Least Squares Iteration:  33%|███▎      | 330/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8786e-03]Least Squares Iteration:  33%|███▎      | 330/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8752e-03]Least Squares Iteration:  33%|███▎      | 331/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8717e-03]Least Squares Iteration:  33%|███▎      | 332/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8683e-03]Least Squares Iteration:  33%|███▎      | 333/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8649e-03]Least Squares Iteration:  33%|███▎      | 334/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8616e-03]Least Squares Iteration:  34%|███▎      | 335/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8582e-03]Least Squares Iteration:  34%|███▎      | 336/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8549e-03]Least Squares Iteration:  34%|███▎      | 337/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8516e-03]Least Squares Iteration:  34%|███▍      | 338/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8483e-03]Least Squares Iteration:  34%|███▍      | 339/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8451e-03]Least Squares Iteration:  34%|███▍      | 340/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8419e-03]Least Squares Iteration:  34%|███▍      | 341/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8387e-03]Least Squares Iteration:  34%|███▍      | 342/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8355e-03]Least Squares Iteration:  34%|███▍      | 343/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8323e-03]Least Squares Iteration:  34%|███▍      | 344/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8292e-03]Least Squares Iteration:  34%|███▍      | 345/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8261e-03]Least Squares Iteration:  35%|███▍      | 346/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8230e-03]Least Squares Iteration:  35%|███▍      | 347/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8199e-03]Least Squares Iteration:  35%|███▍      | 348/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8168e-03]Least Squares Iteration:  35%|███▍      | 349/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8138e-03]Least Squares Iteration:  35%|███▌      | 350/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8108e-03]Least Squares Iteration:  35%|███▌      | 351/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8078e-03]Least Squares Iteration:  35%|███▌      | 352/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8048e-03]Least Squares Iteration:  35%|███▌      | 353/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.8018e-03]Least Squares Iteration:  35%|███▌      | 354/1000 [00:01&lt;00:02, 239.94iter/s, Error=1.7989e-03]Least Squares Iteration:  36%|███▌      | 355/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7989e-03]Least Squares Iteration:  36%|███▌      | 355/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7960e-03]Least Squares Iteration:  36%|███▌      | 356/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7931e-03]Least Squares Iteration:  36%|███▌      | 357/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7902e-03]Least Squares Iteration:  36%|███▌      | 358/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7873e-03]Least Squares Iteration:  36%|███▌      | 359/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7845e-03]Least Squares Iteration:  36%|███▌      | 360/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7817e-03]Least Squares Iteration:  36%|███▌      | 361/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7789e-03]Least Squares Iteration:  36%|███▌      | 362/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7761e-03]Least Squares Iteration:  36%|███▋      | 363/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7733e-03]Least Squares Iteration:  36%|███▋      | 364/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7706e-03]Least Squares Iteration:  36%|███▋      | 365/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7678e-03]Least Squares Iteration:  37%|███▋      | 366/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7651e-03]Least Squares Iteration:  37%|███▋      | 367/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7624e-03]Least Squares Iteration:  37%|███▋      | 368/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7597e-03]Least Squares Iteration:  37%|███▋      | 369/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7570e-03]Least Squares Iteration:  37%|███▋      | 370/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7544e-03]Least Squares Iteration:  37%|███▋      | 371/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7517e-03]Least Squares Iteration:  37%|███▋      | 372/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7491e-03]Least Squares Iteration:  37%|███▋      | 373/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7465e-03]Least Squares Iteration:  37%|███▋      | 374/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7439e-03]Least Squares Iteration:  38%|███▊      | 375/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7414e-03]Least Squares Iteration:  38%|███▊      | 376/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7388e-03]Least Squares Iteration:  38%|███▊      | 377/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7362e-03]Least Squares Iteration:  38%|███▊      | 378/1000 [00:01&lt;00:02, 230.78iter/s, Error=1.7337e-03]Least Squares Iteration:  38%|███▊      | 379/1000 [00:01&lt;00:02, 218.88iter/s, Error=1.7337e-03]Least Squares Iteration:  38%|███▊      | 379/1000 [00:01&lt;00:02, 218.88iter/s, Error=1.7312e-03]Least Squares Iteration:  38%|███▊      | 380/1000 [00:01&lt;00:02, 218.88iter/s, Error=1.7287e-03]Least Squares Iteration:  38%|███▊      | 381/1000 [00:01&lt;00:02, 218.88iter/s, Error=1.7262e-03]Least Squares Iteration:  38%|███▊      | 382/1000 [00:01&lt;00:02, 218.88iter/s, Error=1.7237e-03]Least Squares Iteration:  38%|███▊      | 383/1000 [00:01&lt;00:02, 218.88iter/s, Error=1.7213e-03]Least Squares Iteration:  38%|███▊      | 384/1000 [00:01&lt;00:02, 218.88iter/s, Error=1.7189e-03]Least Squares Iteration:  38%|███▊      | 385/1000 [00:01&lt;00:02, 218.88iter/s, Error=1.7164e-03]Least Squares Iteration:  39%|███▊      | 386/1000 [00:01&lt;00:02, 218.88iter/s, Error=1.7140e-03]Least Squares Iteration:  39%|███▊      | 387/1000 [00:01&lt;00:02, 218.88iter/s, Error=1.7116e-03]Least Squares Iteration:  39%|███▉      | 388/1000 [00:01&lt;00:02, 218.88iter/s, Error=1.7092e-03]Least Squares Iteration:  39%|███▉      | 389/1000 [00:01&lt;00:02, 218.88iter/s, Error=1.7069e-03]Least Squares Iteration:  39%|███▉      | 390/1000 [00:01&lt;00:02, 218.88iter/s, Error=1.7045e-03]Least Squares Iteration:  39%|███▉      | 391/1000 [00:01&lt;00:02, 218.88iter/s, Error=1.7021e-03]Least Squares Iteration:  39%|███▉      | 392/1000 [00:01&lt;00:02, 218.88iter/s, Error=1.6998e-03]Least Squares Iteration:  39%|███▉      | 393/1000 [00:01&lt;00:02, 218.88iter/s, Error=1.6975e-03]Least Squares Iteration:  39%|███▉      | 394/1000 [00:01&lt;00:02, 218.88iter/s, Error=1.6952e-03]Least Squares Iteration:  40%|███▉      | 395/1000 [00:01&lt;00:02, 218.88iter/s, Error=1.6929e-03]Least Squares Iteration:  40%|███▉      | 396/1000 [00:01&lt;00:02, 218.88iter/s, Error=1.6906e-03]Least Squares Iteration:  40%|███▉      | 397/1000 [00:01&lt;00:02, 218.88iter/s, Error=1.6883e-03]Least Squares Iteration:  40%|███▉      | 398/1000 [00:01&lt;00:02, 218.88iter/s, Error=1.6861e-03]Least Squares Iteration:  40%|███▉      | 399/1000 [00:01&lt;00:02, 218.88iter/s, Error=1.6838e-03]Least Squares Iteration:  40%|████      | 400/1000 [00:01&lt;00:02, 218.88iter/s, Error=1.6816e-03]Least Squares Iteration:  40%|████      | 401/1000 [00:01&lt;00:02, 209.68iter/s, Error=1.6816e-03]Least Squares Iteration:  40%|████      | 401/1000 [00:01&lt;00:02, 209.68iter/s, Error=1.6794e-03]Least Squares Iteration:  40%|████      | 402/1000 [00:01&lt;00:02, 209.68iter/s, Error=1.6771e-03]Least Squares Iteration:  40%|████      | 403/1000 [00:01&lt;00:02, 209.68iter/s, Error=1.6750e-03]Least Squares Iteration:  40%|████      | 404/1000 [00:01&lt;00:02, 209.68iter/s, Error=1.6728e-03]Least Squares Iteration:  40%|████      | 405/1000 [00:01&lt;00:02, 209.68iter/s, Error=1.6706e-03]Least Squares Iteration:  41%|████      | 406/1000 [00:01&lt;00:02, 209.68iter/s, Error=1.6684e-03]Least Squares Iteration:  41%|████      | 407/1000 [00:01&lt;00:02, 209.68iter/s, Error=1.6663e-03]Least Squares Iteration:  41%|████      | 408/1000 [00:01&lt;00:02, 209.68iter/s, Error=1.6641e-03]Least Squares Iteration:  41%|████      | 409/1000 [00:01&lt;00:02, 209.68iter/s, Error=1.6620e-03]Least Squares Iteration:  41%|████      | 410/1000 [00:01&lt;00:02, 209.68iter/s, Error=1.6599e-03]Least Squares Iteration:  41%|████      | 411/1000 [00:01&lt;00:02, 209.68iter/s, Error=1.6578e-03]Least Squares Iteration:  41%|████      | 412/1000 [00:01&lt;00:02, 209.68iter/s, Error=1.6557e-03]Least Squares Iteration:  41%|████▏     | 413/1000 [00:01&lt;00:02, 209.68iter/s, Error=1.6536e-03]Least Squares Iteration:  41%|████▏     | 414/1000 [00:01&lt;00:02, 209.68iter/s, Error=1.6515e-03]Least Squares Iteration:  42%|████▏     | 415/1000 [00:01&lt;00:02, 209.68iter/s, Error=1.6494e-03]Least Squares Iteration:  42%|████▏     | 416/1000 [00:01&lt;00:02, 209.68iter/s, Error=1.6474e-03]Least Squares Iteration:  42%|████▏     | 417/1000 [00:01&lt;00:02, 209.68iter/s, Error=1.6453e-03]Least Squares Iteration:  42%|████▏     | 418/1000 [00:01&lt;00:02, 209.68iter/s, Error=1.6433e-03]Least Squares Iteration:  42%|████▏     | 419/1000 [00:01&lt;00:02, 209.68iter/s, Error=1.6413e-03]Least Squares Iteration:  42%|████▏     | 420/1000 [00:01&lt;00:02, 209.68iter/s, Error=1.6392e-03]Least Squares Iteration:  42%|████▏     | 421/1000 [00:01&lt;00:02, 209.68iter/s, Error=1.6372e-03]Least Squares Iteration:  42%|████▏     | 422/1000 [00:01&lt;00:02, 195.58iter/s, Error=1.6372e-03]Least Squares Iteration:  42%|████▏     | 422/1000 [00:01&lt;00:02, 195.58iter/s, Error=1.6352e-03]Least Squares Iteration:  42%|████▏     | 423/1000 [00:01&lt;00:02, 195.58iter/s, Error=1.6332e-03]Least Squares Iteration:  42%|████▏     | 424/1000 [00:01&lt;00:02, 195.58iter/s, Error=1.6313e-03]Least Squares Iteration:  42%|████▎     | 425/1000 [00:01&lt;00:02, 195.58iter/s, Error=1.6293e-03]Least Squares Iteration:  43%|████▎     | 426/1000 [00:01&lt;00:02, 195.58iter/s, Error=1.6273e-03]Least Squares Iteration:  43%|████▎     | 427/1000 [00:01&lt;00:02, 195.58iter/s, Error=1.6254e-03]Least Squares Iteration:  43%|████▎     | 428/1000 [00:01&lt;00:02, 195.58iter/s, Error=1.6234e-03]Least Squares Iteration:  43%|████▎     | 429/1000 [00:01&lt;00:02, 195.58iter/s, Error=1.6215e-03]Least Squares Iteration:  43%|████▎     | 430/1000 [00:01&lt;00:02, 195.58iter/s, Error=1.6196e-03]Least Squares Iteration:  43%|████▎     | 431/1000 [00:01&lt;00:02, 195.58iter/s, Error=1.6177e-03]Least Squares Iteration:  43%|████▎     | 432/1000 [00:01&lt;00:02, 195.58iter/s, Error=1.6158e-03]Least Squares Iteration:  43%|████▎     | 433/1000 [00:01&lt;00:02, 195.58iter/s, Error=1.6139e-03]Least Squares Iteration:  43%|████▎     | 434/1000 [00:01&lt;00:02, 195.58iter/s, Error=1.6120e-03]Least Squares Iteration:  44%|████▎     | 435/1000 [00:01&lt;00:02, 195.58iter/s, Error=1.6101e-03]Least Squares Iteration:  44%|████▎     | 436/1000 [00:01&lt;00:02, 195.58iter/s, Error=1.6082e-03]Least Squares Iteration:  44%|████▎     | 437/1000 [00:01&lt;00:02, 195.58iter/s, Error=1.6064e-03]Least Squares Iteration:  44%|████▍     | 438/1000 [00:01&lt;00:02, 195.58iter/s, Error=1.6045e-03]Least Squares Iteration:  44%|████▍     | 439/1000 [00:01&lt;00:02, 195.58iter/s, Error=1.6026e-03]Least Squares Iteration:  44%|████▍     | 440/1000 [00:01&lt;00:02, 195.58iter/s, Error=1.6008e-03]Least Squares Iteration:  44%|████▍     | 441/1000 [00:01&lt;00:02, 195.58iter/s, Error=1.5990e-03]Least Squares Iteration:  44%|████▍     | 442/1000 [00:01&lt;00:02, 190.77iter/s, Error=1.5990e-03]Least Squares Iteration:  44%|████▍     | 442/1000 [00:01&lt;00:02, 190.77iter/s, Error=1.5971e-03]Least Squares Iteration:  44%|████▍     | 443/1000 [00:01&lt;00:02, 190.77iter/s, Error=1.5953e-03]Least Squares Iteration:  44%|████▍     | 444/1000 [00:01&lt;00:02, 190.77iter/s, Error=1.5935e-03]Least Squares Iteration:  44%|████▍     | 445/1000 [00:01&lt;00:02, 190.77iter/s, Error=1.5917e-03]Least Squares Iteration:  45%|████▍     | 446/1000 [00:01&lt;00:02, 190.77iter/s, Error=1.5899e-03]Least Squares Iteration:  45%|████▍     | 447/1000 [00:01&lt;00:02, 190.77iter/s, Error=1.5882e-03]Least Squares Iteration:  45%|████▍     | 448/1000 [00:01&lt;00:02, 190.77iter/s, Error=1.5864e-03]Least Squares Iteration:  45%|████▍     | 449/1000 [00:01&lt;00:02, 190.77iter/s, Error=1.5846e-03]Least Squares Iteration:  45%|████▌     | 450/1000 [00:01&lt;00:02, 190.77iter/s, Error=1.5828e-03]Least Squares Iteration:  45%|████▌     | 451/1000 [00:01&lt;00:02, 190.77iter/s, Error=1.5811e-03]Least Squares Iteration:  45%|████▌     | 452/1000 [00:01&lt;00:02, 190.77iter/s, Error=1.5793e-03]Least Squares Iteration:  45%|████▌     | 453/1000 [00:01&lt;00:02, 190.77iter/s, Error=1.5776e-03]Least Squares Iteration:  45%|████▌     | 454/1000 [00:01&lt;00:02, 190.77iter/s, Error=1.5759e-03]Least Squares Iteration:  46%|████▌     | 455/1000 [00:01&lt;00:02, 190.77iter/s, Error=1.5741e-03]Least Squares Iteration:  46%|████▌     | 456/1000 [00:01&lt;00:02, 190.77iter/s, Error=1.5724e-03]Least Squares Iteration:  46%|████▌     | 457/1000 [00:01&lt;00:02, 190.77iter/s, Error=1.5707e-03]Least Squares Iteration:  46%|████▌     | 458/1000 [00:01&lt;00:02, 190.77iter/s, Error=1.5690e-03]Least Squares Iteration:  46%|████▌     | 459/1000 [00:01&lt;00:02, 190.77iter/s, Error=1.5673e-03]Least Squares Iteration:  46%|████▌     | 460/1000 [00:01&lt;00:02, 190.77iter/s, Error=1.5656e-03]Least Squares Iteration:  46%|████▌     | 461/1000 [00:01&lt;00:02, 190.77iter/s, Error=1.5639e-03]Least Squares Iteration:  46%|████▌     | 462/1000 [00:01&lt;00:02, 186.80iter/s, Error=1.5639e-03]Least Squares Iteration:  46%|████▌     | 462/1000 [00:01&lt;00:02, 186.80iter/s, Error=1.5623e-03]Least Squares Iteration:  46%|████▋     | 463/1000 [00:01&lt;00:02, 186.80iter/s, Error=1.5606e-03]Least Squares Iteration:  46%|████▋     | 464/1000 [00:01&lt;00:02, 186.80iter/s, Error=1.5589e-03]Least Squares Iteration:  46%|████▋     | 465/1000 [00:01&lt;00:02, 186.80iter/s, Error=1.5573e-03]Least Squares Iteration:  47%|████▋     | 466/1000 [00:01&lt;00:02, 186.80iter/s, Error=1.5556e-03]Least Squares Iteration:  47%|████▋     | 467/1000 [00:01&lt;00:02, 186.80iter/s, Error=1.5540e-03]Least Squares Iteration:  47%|████▋     | 468/1000 [00:01&lt;00:02, 186.80iter/s, Error=1.5523e-03]Least Squares Iteration:  47%|████▋     | 469/1000 [00:01&lt;00:02, 186.80iter/s, Error=1.5507e-03]Least Squares Iteration:  47%|████▋     | 470/1000 [00:01&lt;00:02, 186.80iter/s, Error=1.5491e-03]Least Squares Iteration:  47%|████▋     | 471/1000 [00:01&lt;00:02, 186.80iter/s, Error=1.5474e-03]Least Squares Iteration:  47%|████▋     | 472/1000 [00:01&lt;00:02, 186.80iter/s, Error=1.5458e-03]Least Squares Iteration:  47%|████▋     | 473/1000 [00:01&lt;00:02, 186.80iter/s, Error=1.5442e-03]Least Squares Iteration:  47%|████▋     | 474/1000 [00:01&lt;00:02, 186.80iter/s, Error=1.5426e-03]Least Squares Iteration:  48%|████▊     | 475/1000 [00:01&lt;00:02, 186.80iter/s, Error=1.5410e-03]Least Squares Iteration:  48%|████▊     | 476/1000 [00:01&lt;00:02, 186.80iter/s, Error=1.5394e-03]Least Squares Iteration:  48%|████▊     | 477/1000 [00:01&lt;00:02, 186.80iter/s, Error=1.5378e-03]Least Squares Iteration:  48%|████▊     | 478/1000 [00:01&lt;00:02, 186.80iter/s, Error=1.5363e-03]Least Squares Iteration:  48%|████▊     | 479/1000 [00:01&lt;00:02, 186.80iter/s, Error=1.5347e-03]Least Squares Iteration:  48%|████▊     | 480/1000 [00:01&lt;00:02, 186.80iter/s, Error=1.5331e-03]Least Squares Iteration:  48%|████▊     | 481/1000 [00:01&lt;00:02, 173.50iter/s, Error=1.5331e-03]Least Squares Iteration:  48%|████▊     | 481/1000 [00:01&lt;00:02, 173.50iter/s, Error=1.5316e-03]Least Squares Iteration:  48%|████▊     | 482/1000 [00:02&lt;00:02, 173.50iter/s, Error=1.5300e-03]Least Squares Iteration:  48%|████▊     | 483/1000 [00:02&lt;00:02, 173.50iter/s, Error=1.5285e-03]Least Squares Iteration:  48%|████▊     | 484/1000 [00:02&lt;00:02, 173.50iter/s, Error=1.5269e-03]Least Squares Iteration:  48%|████▊     | 485/1000 [00:02&lt;00:02, 173.50iter/s, Error=1.5254e-03]Least Squares Iteration:  49%|████▊     | 486/1000 [00:02&lt;00:02, 173.50iter/s, Error=1.5238e-03]Least Squares Iteration:  49%|████▊     | 487/1000 [00:02&lt;00:02, 173.50iter/s, Error=1.5223e-03]Least Squares Iteration:  49%|████▉     | 488/1000 [00:02&lt;00:02, 173.50iter/s, Error=1.5208e-03]Least Squares Iteration:  49%|████▉     | 489/1000 [00:02&lt;00:02, 173.50iter/s, Error=1.5193e-03]Least Squares Iteration:  49%|████▉     | 490/1000 [00:02&lt;00:02, 173.50iter/s, Error=1.5178e-03]Least Squares Iteration:  49%|████▉     | 491/1000 [00:02&lt;00:02, 173.50iter/s, Error=1.5162e-03]Least Squares Iteration:  49%|████▉     | 492/1000 [00:02&lt;00:02, 173.50iter/s, Error=1.5148e-03]Least Squares Iteration:  49%|████▉     | 493/1000 [00:02&lt;00:02, 173.50iter/s, Error=1.5133e-03]Least Squares Iteration:  49%|████▉     | 494/1000 [00:02&lt;00:02, 173.50iter/s, Error=1.5118e-03]Least Squares Iteration:  50%|████▉     | 495/1000 [00:02&lt;00:02, 173.50iter/s, Error=1.5103e-03]Least Squares Iteration:  50%|████▉     | 496/1000 [00:02&lt;00:02, 173.50iter/s, Error=1.5088e-03]Least Squares Iteration:  50%|████▉     | 497/1000 [00:02&lt;00:02, 173.50iter/s, Error=1.5073e-03]Least Squares Iteration:  50%|████▉     | 498/1000 [00:02&lt;00:02, 173.50iter/s, Error=1.5058e-03]Least Squares Iteration:  50%|████▉     | 499/1000 [00:02&lt;00:03, 140.89iter/s, Error=1.5058e-03]Least Squares Iteration:  50%|████▉     | 499/1000 [00:02&lt;00:03, 140.89iter/s, Error=1.5044e-03]Least Squares Iteration:  50%|█████     | 500/1000 [00:02&lt;00:03, 140.89iter/s, Error=1.5029e-03]Least Squares Iteration:  50%|█████     | 501/1000 [00:02&lt;00:03, 140.89iter/s, Error=1.5015e-03]Least Squares Iteration:  50%|█████     | 502/1000 [00:02&lt;00:03, 140.89iter/s, Error=1.5000e-03]Least Squares Iteration:  50%|█████     | 503/1000 [00:02&lt;00:03, 140.89iter/s, Error=1.4986e-03]Least Squares Iteration:  50%|█████     | 504/1000 [00:02&lt;00:03, 140.89iter/s, Error=1.4971e-03]Least Squares Iteration:  50%|█████     | 505/1000 [00:02&lt;00:03, 140.89iter/s, Error=1.4957e-03]Least Squares Iteration:  51%|█████     | 506/1000 [00:02&lt;00:03, 140.89iter/s, Error=1.4943e-03]Least Squares Iteration:  51%|█████     | 507/1000 [00:02&lt;00:03, 140.89iter/s, Error=1.4928e-03]Least Squares Iteration:  51%|█████     | 508/1000 [00:02&lt;00:03, 140.89iter/s, Error=1.4914e-03]Least Squares Iteration:  51%|█████     | 509/1000 [00:02&lt;00:03, 140.89iter/s, Error=1.4900e-03]Least Squares Iteration:  51%|█████     | 510/1000 [00:02&lt;00:03, 140.89iter/s, Error=1.4886e-03]Least Squares Iteration:  51%|█████     | 511/1000 [00:02&lt;00:03, 140.89iter/s, Error=1.4872e-03]Least Squares Iteration:  51%|█████     | 512/1000 [00:02&lt;00:03, 140.89iter/s, Error=1.4857e-03]Least Squares Iteration:  51%|█████▏    | 513/1000 [00:02&lt;00:03, 140.89iter/s, Error=1.4843e-03]Least Squares Iteration:  51%|█████▏    | 514/1000 [00:02&lt;00:03, 139.40iter/s, Error=1.4843e-03]Least Squares Iteration:  51%|█████▏    | 514/1000 [00:02&lt;00:03, 139.40iter/s, Error=1.4829e-03]Least Squares Iteration:  52%|█████▏    | 515/1000 [00:02&lt;00:03, 139.40iter/s, Error=1.4815e-03]Least Squares Iteration:  52%|█████▏    | 516/1000 [00:02&lt;00:03, 139.40iter/s, Error=1.4802e-03]Least Squares Iteration:  52%|█████▏    | 517/1000 [00:02&lt;00:03, 139.40iter/s, Error=1.4788e-03]Least Squares Iteration:  52%|█████▏    | 518/1000 [00:02&lt;00:03, 139.40iter/s, Error=1.4774e-03]Least Squares Iteration:  52%|█████▏    | 519/1000 [00:02&lt;00:03, 139.40iter/s, Error=1.4760e-03]Least Squares Iteration:  52%|█████▏    | 520/1000 [00:02&lt;00:03, 139.40iter/s, Error=1.4747e-03]Least Squares Iteration:  52%|█████▏    | 521/1000 [00:02&lt;00:03, 139.40iter/s, Error=1.4733e-03]Least Squares Iteration:  52%|█████▏    | 522/1000 [00:02&lt;00:03, 139.40iter/s, Error=1.4719e-03]Least Squares Iteration:  52%|█████▏    | 523/1000 [00:02&lt;00:03, 139.40iter/s, Error=1.4706e-03]Least Squares Iteration:  52%|█████▏    | 524/1000 [00:02&lt;00:03, 139.40iter/s, Error=1.4692e-03]Least Squares Iteration:  52%|█████▎    | 525/1000 [00:02&lt;00:03, 139.40iter/s, Error=1.4679e-03]Least Squares Iteration:  53%|█████▎    | 526/1000 [00:02&lt;00:03, 139.40iter/s, Error=1.4665e-03]Least Squares Iteration:  53%|█████▎    | 527/1000 [00:02&lt;00:03, 139.40iter/s, Error=1.4652e-03]Least Squares Iteration:  53%|█████▎    | 528/1000 [00:02&lt;00:03, 139.40iter/s, Error=1.4638e-03]Least Squares Iteration:  53%|█████▎    | 529/1000 [00:02&lt;00:03, 135.16iter/s, Error=1.4638e-03]Least Squares Iteration:  53%|█████▎    | 529/1000 [00:02&lt;00:03, 135.16iter/s, Error=1.4625e-03]Least Squares Iteration:  53%|█████▎    | 530/1000 [00:02&lt;00:03, 135.16iter/s, Error=1.4612e-03]Least Squares Iteration:  53%|█████▎    | 531/1000 [00:02&lt;00:03, 135.16iter/s, Error=1.4598e-03]Least Squares Iteration:  53%|█████▎    | 532/1000 [00:02&lt;00:03, 135.16iter/s, Error=1.4585e-03]Least Squares Iteration:  53%|█████▎    | 533/1000 [00:02&lt;00:03, 135.16iter/s, Error=1.4572e-03]Least Squares Iteration:  53%|█████▎    | 534/1000 [00:02&lt;00:03, 135.16iter/s, Error=1.4559e-03]Least Squares Iteration:  54%|█████▎    | 535/1000 [00:02&lt;00:03, 135.16iter/s, Error=1.4546e-03]Least Squares Iteration:  54%|█████▎    | 536/1000 [00:02&lt;00:03, 135.16iter/s, Error=1.4533e-03]Least Squares Iteration:  54%|█████▎    | 537/1000 [00:02&lt;00:03, 135.16iter/s, Error=1.4519e-03]Least Squares Iteration:  54%|█████▍    | 538/1000 [00:02&lt;00:03, 135.16iter/s, Error=1.4506e-03]Least Squares Iteration:  54%|█████▍    | 539/1000 [00:02&lt;00:03, 135.16iter/s, Error=1.4493e-03]Least Squares Iteration:  54%|█████▍    | 540/1000 [00:02&lt;00:03, 135.16iter/s, Error=1.4480e-03]Least Squares Iteration:  54%|█████▍    | 541/1000 [00:02&lt;00:03, 135.16iter/s, Error=1.4468e-03]Least Squares Iteration:  54%|█████▍    | 542/1000 [00:02&lt;00:03, 135.16iter/s, Error=1.4455e-03]Least Squares Iteration:  54%|█████▍    | 543/1000 [00:02&lt;00:03, 135.18iter/s, Error=1.4455e-03]Least Squares Iteration:  54%|█████▍    | 543/1000 [00:02&lt;00:03, 135.18iter/s, Error=1.4442e-03]Least Squares Iteration:  54%|█████▍    | 544/1000 [00:02&lt;00:03, 135.18iter/s, Error=1.4429e-03]Least Squares Iteration:  55%|█████▍    | 545/1000 [00:02&lt;00:03, 135.18iter/s, Error=1.4416e-03]Least Squares Iteration:  55%|█████▍    | 546/1000 [00:02&lt;00:03, 135.18iter/s, Error=1.4404e-03]Least Squares Iteration:  55%|█████▍    | 547/1000 [00:02&lt;00:03, 135.18iter/s, Error=1.4391e-03]Least Squares Iteration:  55%|█████▍    | 548/1000 [00:02&lt;00:03, 135.18iter/s, Error=1.4378e-03]Least Squares Iteration:  55%|█████▍    | 549/1000 [00:02&lt;00:03, 135.18iter/s, Error=1.4366e-03]Least Squares Iteration:  55%|█████▌    | 550/1000 [00:02&lt;00:03, 135.18iter/s, Error=1.4353e-03]Least Squares Iteration:  55%|█████▌    | 551/1000 [00:02&lt;00:03, 135.18iter/s, Error=1.4340e-03]Least Squares Iteration:  55%|█████▌    | 552/1000 [00:02&lt;00:03, 135.18iter/s, Error=1.4328e-03]Least Squares Iteration:  55%|█████▌    | 553/1000 [00:02&lt;00:03, 135.18iter/s, Error=1.4315e-03]Least Squares Iteration:  55%|█████▌    | 554/1000 [00:02&lt;00:03, 135.18iter/s, Error=1.4303e-03]Least Squares Iteration:  56%|█████▌    | 555/1000 [00:02&lt;00:03, 135.18iter/s, Error=1.4291e-03]Least Squares Iteration:  56%|█████▌    | 556/1000 [00:02&lt;00:03, 135.18iter/s, Error=1.4278e-03]Least Squares Iteration:  56%|█████▌    | 557/1000 [00:02&lt;00:03, 133.50iter/s, Error=1.4278e-03]Least Squares Iteration:  56%|█████▌    | 557/1000 [00:02&lt;00:03, 133.50iter/s, Error=1.4266e-03]Least Squares Iteration:  56%|█████▌    | 558/1000 [00:02&lt;00:03, 133.50iter/s, Error=1.4254e-03]Least Squares Iteration:  56%|█████▌    | 559/1000 [00:02&lt;00:03, 133.50iter/s, Error=1.4241e-03]Least Squares Iteration:  56%|█████▌    | 560/1000 [00:02&lt;00:03, 133.50iter/s, Error=1.4229e-03]Least Squares Iteration:  56%|█████▌    | 561/1000 [00:02&lt;00:03, 133.50iter/s, Error=1.4217e-03]Least Squares Iteration:  56%|█████▌    | 562/1000 [00:02&lt;00:03, 133.50iter/s, Error=1.4205e-03]Least Squares Iteration:  56%|█████▋    | 563/1000 [00:02&lt;00:03, 133.50iter/s, Error=1.4192e-03]Least Squares Iteration:  56%|█████▋    | 564/1000 [00:02&lt;00:03, 133.50iter/s, Error=1.4180e-03]Least Squares Iteration:  56%|█████▋    | 565/1000 [00:02&lt;00:03, 133.50iter/s, Error=1.4168e-03]Least Squares Iteration:  57%|█████▋    | 566/1000 [00:02&lt;00:03, 133.50iter/s, Error=1.4156e-03]Least Squares Iteration:  57%|█████▋    | 567/1000 [00:02&lt;00:03, 133.50iter/s, Error=1.4144e-03]Least Squares Iteration:  57%|█████▋    | 568/1000 [00:02&lt;00:03, 133.50iter/s, Error=1.4132e-03]Least Squares Iteration:  57%|█████▋    | 569/1000 [00:02&lt;00:03, 133.50iter/s, Error=1.4120e-03]Least Squares Iteration:  57%|█████▋    | 570/1000 [00:02&lt;00:03, 133.50iter/s, Error=1.4108e-03]Least Squares Iteration:  57%|█████▋    | 571/1000 [00:02&lt;00:03, 123.71iter/s, Error=1.4108e-03]Least Squares Iteration:  57%|█████▋    | 571/1000 [00:02&lt;00:03, 123.71iter/s, Error=1.4096e-03]Least Squares Iteration:  57%|█████▋    | 572/1000 [00:02&lt;00:03, 123.71iter/s, Error=1.4084e-03]Least Squares Iteration:  57%|█████▋    | 573/1000 [00:02&lt;00:03, 123.71iter/s, Error=1.4072e-03]Least Squares Iteration:  57%|█████▋    | 574/1000 [00:02&lt;00:03, 123.71iter/s, Error=1.4061e-03]Least Squares Iteration:  57%|█████▊    | 575/1000 [00:02&lt;00:03, 123.71iter/s, Error=1.4049e-03]Least Squares Iteration:  58%|█████▊    | 576/1000 [00:02&lt;00:03, 123.71iter/s, Error=1.4037e-03]Least Squares Iteration:  58%|█████▊    | 577/1000 [00:02&lt;00:03, 123.71iter/s, Error=1.4025e-03]Least Squares Iteration:  58%|█████▊    | 578/1000 [00:02&lt;00:03, 123.71iter/s, Error=1.4014e-03]Least Squares Iteration:  58%|█████▊    | 579/1000 [00:02&lt;00:03, 123.71iter/s, Error=1.4002e-03]Least Squares Iteration:  58%|█████▊    | 580/1000 [00:02&lt;00:03, 123.71iter/s, Error=1.3990e-03]Least Squares Iteration:  58%|█████▊    | 581/1000 [00:02&lt;00:03, 123.71iter/s, Error=1.3979e-03]Least Squares Iteration:  58%|█████▊    | 582/1000 [00:02&lt;00:03, 123.71iter/s, Error=1.3967e-03]Least Squares Iteration:  58%|█████▊    | 583/1000 [00:02&lt;00:03, 123.71iter/s, Error=1.3955e-03]Least Squares Iteration:  58%|█████▊    | 584/1000 [00:02&lt;00:03, 119.24iter/s, Error=1.3955e-03]Least Squares Iteration:  58%|█████▊    | 584/1000 [00:02&lt;00:03, 119.24iter/s, Error=1.3944e-03]Least Squares Iteration:  58%|█████▊    | 585/1000 [00:02&lt;00:03, 119.24iter/s, Error=1.3932e-03]Least Squares Iteration:  59%|█████▊    | 586/1000 [00:02&lt;00:03, 119.24iter/s, Error=1.3921e-03]Least Squares Iteration:  59%|█████▊    | 587/1000 [00:02&lt;00:03, 119.24iter/s, Error=1.3909e-03]Least Squares Iteration:  59%|█████▉    | 588/1000 [00:02&lt;00:03, 119.24iter/s, Error=1.3898e-03]Least Squares Iteration:  59%|█████▉    | 589/1000 [00:02&lt;00:03, 119.24iter/s, Error=1.3887e-03]Least Squares Iteration:  59%|█████▉    | 590/1000 [00:02&lt;00:03, 119.24iter/s, Error=1.3875e-03]Least Squares Iteration:  59%|█████▉    | 591/1000 [00:02&lt;00:03, 119.24iter/s, Error=1.3864e-03]Least Squares Iteration:  59%|█████▉    | 592/1000 [00:02&lt;00:03, 119.24iter/s, Error=1.3852e-03]Least Squares Iteration:  59%|█████▉    | 593/1000 [00:02&lt;00:03, 119.24iter/s, Error=1.3841e-03]Least Squares Iteration:  59%|█████▉    | 594/1000 [00:02&lt;00:03, 119.24iter/s, Error=1.3830e-03]Least Squares Iteration:  60%|█████▉    | 595/1000 [00:02&lt;00:03, 119.24iter/s, Error=1.3819e-03]Least Squares Iteration:  60%|█████▉    | 596/1000 [00:02&lt;00:03, 119.24iter/s, Error=1.3807e-03]Least Squares Iteration:  60%|█████▉    | 597/1000 [00:02&lt;00:03, 117.81iter/s, Error=1.3807e-03]Least Squares Iteration:  60%|█████▉    | 597/1000 [00:02&lt;00:03, 117.81iter/s, Error=1.3796e-03]Least Squares Iteration:  60%|█████▉    | 598/1000 [00:02&lt;00:03, 117.81iter/s, Error=1.3785e-03]Least Squares Iteration:  60%|█████▉    | 599/1000 [00:03&lt;00:03, 117.81iter/s, Error=1.3774e-03]Least Squares Iteration:  60%|██████    | 600/1000 [00:03&lt;00:03, 117.81iter/s, Error=1.3763e-03]Least Squares Iteration:  60%|██████    | 601/1000 [00:03&lt;00:03, 117.81iter/s, Error=1.3752e-03]Least Squares Iteration:  60%|██████    | 602/1000 [00:03&lt;00:03, 117.81iter/s, Error=1.3741e-03]Least Squares Iteration:  60%|██████    | 603/1000 [00:03&lt;00:03, 117.81iter/s, Error=1.3729e-03]Least Squares Iteration:  60%|██████    | 604/1000 [00:03&lt;00:03, 117.81iter/s, Error=1.3718e-03]Least Squares Iteration:  60%|██████    | 605/1000 [00:03&lt;00:03, 117.81iter/s, Error=1.3707e-03]Least Squares Iteration:  61%|██████    | 606/1000 [00:03&lt;00:03, 117.81iter/s, Error=1.3696e-03]Least Squares Iteration:  61%|██████    | 607/1000 [00:03&lt;00:03, 117.81iter/s, Error=1.3686e-03]Least Squares Iteration:  61%|██████    | 608/1000 [00:03&lt;00:03, 117.81iter/s, Error=1.3675e-03]Least Squares Iteration:  61%|██████    | 609/1000 [00:03&lt;00:03, 114.98iter/s, Error=1.3675e-03]Least Squares Iteration:  61%|██████    | 609/1000 [00:03&lt;00:03, 114.98iter/s, Error=1.3664e-03]Least Squares Iteration:  61%|██████    | 610/1000 [00:03&lt;00:03, 114.98iter/s, Error=1.3653e-03]Least Squares Iteration:  61%|██████    | 611/1000 [00:03&lt;00:03, 114.98iter/s, Error=1.3642e-03]Least Squares Iteration:  61%|██████    | 612/1000 [00:03&lt;00:03, 114.98iter/s, Error=1.3631e-03]Least Squares Iteration:  61%|██████▏   | 613/1000 [00:03&lt;00:03, 114.98iter/s, Error=1.3620e-03]Least Squares Iteration:  61%|██████▏   | 614/1000 [00:03&lt;00:03, 114.98iter/s, Error=1.3610e-03]Least Squares Iteration:  62%|██████▏   | 615/1000 [00:03&lt;00:03, 114.98iter/s, Error=1.3599e-03]Least Squares Iteration:  62%|██████▏   | 616/1000 [00:03&lt;00:03, 114.98iter/s, Error=1.3588e-03]Least Squares Iteration:  62%|██████▏   | 617/1000 [00:03&lt;00:03, 114.98iter/s, Error=1.3577e-03]Least Squares Iteration:  62%|██████▏   | 618/1000 [00:03&lt;00:03, 114.98iter/s, Error=1.3567e-03]Least Squares Iteration:  62%|██████▏   | 619/1000 [00:03&lt;00:03, 114.98iter/s, Error=1.3556e-03]Least Squares Iteration:  62%|██████▏   | 620/1000 [00:03&lt;00:03, 114.98iter/s, Error=1.3545e-03]Least Squares Iteration:  62%|██████▏   | 621/1000 [00:03&lt;00:03, 114.98iter/s, Error=1.3535e-03]Least Squares Iteration:  62%|██████▏   | 622/1000 [00:03&lt;00:03, 116.77iter/s, Error=1.3535e-03]Least Squares Iteration:  62%|██████▏   | 622/1000 [00:03&lt;00:03, 116.77iter/s, Error=1.3524e-03]Least Squares Iteration:  62%|██████▏   | 623/1000 [00:03&lt;00:03, 116.77iter/s, Error=1.3514e-03]Least Squares Iteration:  62%|██████▏   | 624/1000 [00:03&lt;00:03, 116.77iter/s, Error=1.3503e-03]Least Squares Iteration:  62%|██████▎   | 625/1000 [00:03&lt;00:03, 116.77iter/s, Error=1.3492e-03]Least Squares Iteration:  63%|██████▎   | 626/1000 [00:03&lt;00:03, 116.77iter/s, Error=1.3482e-03]Least Squares Iteration:  63%|██████▎   | 627/1000 [00:03&lt;00:03, 116.77iter/s, Error=1.3471e-03]Least Squares Iteration:  63%|██████▎   | 628/1000 [00:03&lt;00:03, 116.77iter/s, Error=1.3461e-03]Least Squares Iteration:  63%|██████▎   | 629/1000 [00:03&lt;00:03, 116.77iter/s, Error=1.3450e-03]Least Squares Iteration:  63%|██████▎   | 630/1000 [00:03&lt;00:03, 116.77iter/s, Error=1.3440e-03]Least Squares Iteration:  63%|██████▎   | 631/1000 [00:03&lt;00:03, 116.77iter/s, Error=1.3430e-03]Least Squares Iteration:  63%|██████▎   | 632/1000 [00:03&lt;00:03, 116.77iter/s, Error=1.3419e-03]Least Squares Iteration:  63%|██████▎   | 633/1000 [00:03&lt;00:03, 116.77iter/s, Error=1.3409e-03]Least Squares Iteration:  63%|██████▎   | 634/1000 [00:03&lt;00:03, 116.77iter/s, Error=1.3399e-03]Least Squares Iteration:  64%|██████▎   | 635/1000 [00:03&lt;00:03, 118.85iter/s, Error=1.3399e-03]Least Squares Iteration:  64%|██████▎   | 635/1000 [00:03&lt;00:03, 118.85iter/s, Error=1.3388e-03]Least Squares Iteration:  64%|██████▎   | 636/1000 [00:03&lt;00:03, 118.85iter/s, Error=1.3378e-03]Least Squares Iteration:  64%|██████▎   | 637/1000 [00:03&lt;00:03, 118.85iter/s, Error=1.3368e-03]Least Squares Iteration:  64%|██████▍   | 638/1000 [00:03&lt;00:03, 118.85iter/s, Error=1.3357e-03]Least Squares Iteration:  64%|██████▍   | 639/1000 [00:03&lt;00:03, 118.85iter/s, Error=1.3347e-03]Least Squares Iteration:  64%|██████▍   | 640/1000 [00:03&lt;00:03, 118.85iter/s, Error=1.3337e-03]Least Squares Iteration:  64%|██████▍   | 641/1000 [00:03&lt;00:03, 118.85iter/s, Error=1.3327e-03]Least Squares Iteration:  64%|██████▍   | 642/1000 [00:03&lt;00:03, 118.85iter/s, Error=1.3317e-03]Least Squares Iteration:  64%|██████▍   | 643/1000 [00:03&lt;00:03, 118.85iter/s, Error=1.3306e-03]Least Squares Iteration:  64%|██████▍   | 644/1000 [00:03&lt;00:02, 118.85iter/s, Error=1.3296e-03]Least Squares Iteration:  64%|██████▍   | 645/1000 [00:03&lt;00:02, 118.85iter/s, Error=1.3286e-03]Least Squares Iteration:  65%|██████▍   | 646/1000 [00:03&lt;00:02, 118.85iter/s, Error=1.3276e-03]Least Squares Iteration:  65%|██████▍   | 647/1000 [00:03&lt;00:03, 107.70iter/s, Error=1.3276e-03]Least Squares Iteration:  65%|██████▍   | 647/1000 [00:03&lt;00:03, 107.70iter/s, Error=1.3266e-03]Least Squares Iteration:  65%|██████▍   | 648/1000 [00:03&lt;00:03, 107.70iter/s, Error=1.3256e-03]Least Squares Iteration:  65%|██████▍   | 649/1000 [00:03&lt;00:03, 107.70iter/s, Error=1.3246e-03]Least Squares Iteration:  65%|██████▌   | 650/1000 [00:03&lt;00:03, 107.70iter/s, Error=1.3236e-03]Least Squares Iteration:  65%|██████▌   | 651/1000 [00:03&lt;00:03, 107.70iter/s, Error=1.3226e-03]Least Squares Iteration:  65%|██████▌   | 652/1000 [00:03&lt;00:03, 107.70iter/s, Error=1.3216e-03]Least Squares Iteration:  65%|██████▌   | 653/1000 [00:03&lt;00:03, 107.70iter/s, Error=1.3206e-03]Least Squares Iteration:  65%|██████▌   | 654/1000 [00:03&lt;00:03, 107.70iter/s, Error=1.3196e-03]Least Squares Iteration:  66%|██████▌   | 655/1000 [00:03&lt;00:03, 107.70iter/s, Error=1.3186e-03]Least Squares Iteration:  66%|██████▌   | 656/1000 [00:03&lt;00:03, 107.70iter/s, Error=1.3176e-03]Least Squares Iteration:  66%|██████▌   | 657/1000 [00:03&lt;00:03, 107.70iter/s, Error=1.3166e-03]Least Squares Iteration:  66%|██████▌   | 658/1000 [00:03&lt;00:03, 104.94iter/s, Error=1.3166e-03]Least Squares Iteration:  66%|██████▌   | 658/1000 [00:03&lt;00:03, 104.94iter/s, Error=1.3156e-03]Least Squares Iteration:  66%|██████▌   | 659/1000 [00:03&lt;00:03, 104.94iter/s, Error=1.3147e-03]Least Squares Iteration:  66%|██████▌   | 660/1000 [00:03&lt;00:03, 104.94iter/s, Error=1.3137e-03]Least Squares Iteration:  66%|██████▌   | 661/1000 [00:03&lt;00:03, 104.94iter/s, Error=1.3127e-03]Least Squares Iteration:  66%|██████▌   | 662/1000 [00:03&lt;00:03, 104.94iter/s, Error=1.3117e-03]Least Squares Iteration:  66%|██████▋   | 663/1000 [00:03&lt;00:03, 104.94iter/s, Error=1.3108e-03]Least Squares Iteration:  66%|██████▋   | 664/1000 [00:03&lt;00:03, 104.94iter/s, Error=1.3098e-03]Least Squares Iteration:  66%|██████▋   | 665/1000 [00:03&lt;00:03, 104.94iter/s, Error=1.3088e-03]Least Squares Iteration:  67%|██████▋   | 666/1000 [00:03&lt;00:03, 104.94iter/s, Error=1.3078e-03]Least Squares Iteration:  67%|██████▋   | 667/1000 [00:03&lt;00:03, 104.94iter/s, Error=1.3069e-03]Least Squares Iteration:  67%|██████▋   | 668/1000 [00:03&lt;00:03, 104.94iter/s, Error=1.3059e-03]Least Squares Iteration:  67%|██████▋   | 669/1000 [00:03&lt;00:03, 97.30iter/s, Error=1.3059e-03] Least Squares Iteration:  67%|██████▋   | 669/1000 [00:03&lt;00:03, 97.30iter/s, Error=1.3049e-03]Least Squares Iteration:  67%|██████▋   | 670/1000 [00:03&lt;00:03, 97.30iter/s, Error=1.3040e-03]Least Squares Iteration:  67%|██████▋   | 671/1000 [00:03&lt;00:03, 97.30iter/s, Error=1.3030e-03]Least Squares Iteration:  67%|██████▋   | 672/1000 [00:03&lt;00:03, 97.30iter/s, Error=1.3020e-03]Least Squares Iteration:  67%|██████▋   | 673/1000 [00:03&lt;00:03, 97.30iter/s, Error=1.3011e-03]Least Squares Iteration:  67%|██████▋   | 674/1000 [00:03&lt;00:03, 97.30iter/s, Error=1.3001e-03]Least Squares Iteration:  68%|██████▊   | 675/1000 [00:03&lt;00:03, 97.30iter/s, Error=1.2992e-03]Least Squares Iteration:  68%|██████▊   | 676/1000 [00:03&lt;00:03, 97.30iter/s, Error=1.2982e-03]Least Squares Iteration:  68%|██████▊   | 677/1000 [00:03&lt;00:03, 97.30iter/s, Error=1.2973e-03]Least Squares Iteration:  68%|██████▊   | 678/1000 [00:03&lt;00:03, 97.30iter/s, Error=1.2963e-03]Least Squares Iteration:  68%|██████▊   | 679/1000 [00:03&lt;00:03, 94.90iter/s, Error=1.2963e-03]Least Squares Iteration:  68%|██████▊   | 679/1000 [00:03&lt;00:03, 94.90iter/s, Error=1.2954e-03]Least Squares Iteration:  68%|██████▊   | 680/1000 [00:03&lt;00:03, 94.90iter/s, Error=1.2944e-03]Least Squares Iteration:  68%|██████▊   | 681/1000 [00:03&lt;00:03, 94.90iter/s, Error=1.2935e-03]Least Squares Iteration:  68%|██████▊   | 682/1000 [00:03&lt;00:03, 94.90iter/s, Error=1.2925e-03]Least Squares Iteration:  68%|██████▊   | 683/1000 [00:03&lt;00:03, 94.90iter/s, Error=1.2916e-03]Least Squares Iteration:  68%|██████▊   | 684/1000 [00:03&lt;00:03, 94.90iter/s, Error=1.2907e-03]Least Squares Iteration:  68%|██████▊   | 685/1000 [00:03&lt;00:03, 94.90iter/s, Error=1.2897e-03]Least Squares Iteration:  69%|██████▊   | 686/1000 [00:03&lt;00:03, 94.90iter/s, Error=1.2888e-03]Least Squares Iteration:  69%|██████▊   | 687/1000 [00:03&lt;00:03, 94.90iter/s, Error=1.2879e-03]Least Squares Iteration:  69%|██████▉   | 688/1000 [00:03&lt;00:03, 94.90iter/s, Error=1.2869e-03]Least Squares Iteration:  69%|██████▉   | 689/1000 [00:03&lt;00:03, 92.93iter/s, Error=1.2869e-03]Least Squares Iteration:  69%|██████▉   | 689/1000 [00:03&lt;00:03, 92.93iter/s, Error=1.2860e-03]Least Squares Iteration:  69%|██████▉   | 690/1000 [00:03&lt;00:03, 92.93iter/s, Error=1.2851e-03]Least Squares Iteration:  69%|██████▉   | 691/1000 [00:03&lt;00:03, 92.93iter/s, Error=1.2841e-03]Least Squares Iteration:  69%|██████▉   | 692/1000 [00:03&lt;00:03, 92.93iter/s, Error=1.2832e-03]Least Squares Iteration:  69%|██████▉   | 693/1000 [00:03&lt;00:03, 92.93iter/s, Error=1.2823e-03]Least Squares Iteration:  69%|██████▉   | 694/1000 [00:03&lt;00:03, 92.93iter/s, Error=1.2814e-03]Least Squares Iteration:  70%|██████▉   | 695/1000 [00:03&lt;00:03, 92.93iter/s, Error=1.2805e-03]Least Squares Iteration:  70%|██████▉   | 696/1000 [00:03&lt;00:03, 92.93iter/s, Error=1.2795e-03]Least Squares Iteration:  70%|██████▉   | 697/1000 [00:03&lt;00:03, 92.93iter/s, Error=1.2786e-03]Least Squares Iteration:  70%|██████▉   | 698/1000 [00:03&lt;00:03, 92.93iter/s, Error=1.2777e-03]Least Squares Iteration:  70%|██████▉   | 699/1000 [00:04&lt;00:03, 92.93iter/s, Error=1.2768e-03]Least Squares Iteration:  70%|███████   | 700/1000 [00:04&lt;00:03, 92.93iter/s, Error=1.2759e-03]Least Squares Iteration:  70%|███████   | 701/1000 [00:04&lt;00:03, 98.64iter/s, Error=1.2759e-03]Least Squares Iteration:  70%|███████   | 701/1000 [00:04&lt;00:03, 98.64iter/s, Error=1.2750e-03]Least Squares Iteration:  70%|███████   | 702/1000 [00:04&lt;00:03, 98.64iter/s, Error=1.2741e-03]Least Squares Iteration:  70%|███████   | 703/1000 [00:04&lt;00:03, 98.64iter/s, Error=1.2731e-03]Least Squares Iteration:  70%|███████   | 704/1000 [00:04&lt;00:03, 98.64iter/s, Error=1.2722e-03]Least Squares Iteration:  70%|███████   | 705/1000 [00:04&lt;00:02, 98.64iter/s, Error=1.2713e-03]Least Squares Iteration:  71%|███████   | 706/1000 [00:04&lt;00:02, 98.64iter/s, Error=1.2704e-03]Least Squares Iteration:  71%|███████   | 707/1000 [00:04&lt;00:02, 98.64iter/s, Error=1.2695e-03]Least Squares Iteration:  71%|███████   | 708/1000 [00:04&lt;00:02, 98.64iter/s, Error=1.2686e-03]Least Squares Iteration:  71%|███████   | 709/1000 [00:04&lt;00:02, 98.64iter/s, Error=1.2677e-03]Least Squares Iteration:  71%|███████   | 710/1000 [00:04&lt;00:02, 98.64iter/s, Error=1.2668e-03]Least Squares Iteration:  71%|███████   | 711/1000 [00:04&lt;00:02, 98.64iter/s, Error=1.2659e-03]Least Squares Iteration:  71%|███████   | 712/1000 [00:04&lt;00:02, 98.64iter/s, Error=1.2651e-03]Least Squares Iteration:  71%|███████▏  | 713/1000 [00:04&lt;00:02, 104.15iter/s, Error=1.2651e-03]Least Squares Iteration:  71%|███████▏  | 713/1000 [00:04&lt;00:02, 104.15iter/s, Error=1.2642e-03]Least Squares Iteration:  71%|███████▏  | 714/1000 [00:04&lt;00:02, 104.15iter/s, Error=1.2633e-03]Least Squares Iteration:  72%|███████▏  | 715/1000 [00:04&lt;00:02, 104.15iter/s, Error=1.2624e-03]Least Squares Iteration:  72%|███████▏  | 716/1000 [00:04&lt;00:02, 104.15iter/s, Error=1.2615e-03]Least Squares Iteration:  72%|███████▏  | 717/1000 [00:04&lt;00:02, 104.15iter/s, Error=1.2606e-03]Least Squares Iteration:  72%|███████▏  | 718/1000 [00:04&lt;00:02, 104.15iter/s, Error=1.2597e-03]Least Squares Iteration:  72%|███████▏  | 719/1000 [00:04&lt;00:02, 104.15iter/s, Error=1.2588e-03]Least Squares Iteration:  72%|███████▏  | 720/1000 [00:04&lt;00:02, 104.15iter/s, Error=1.2580e-03]Least Squares Iteration:  72%|███████▏  | 721/1000 [00:04&lt;00:02, 104.15iter/s, Error=1.2571e-03]Least Squares Iteration:  72%|███████▏  | 722/1000 [00:04&lt;00:02, 104.15iter/s, Error=1.2562e-03]Least Squares Iteration:  72%|███████▏  | 723/1000 [00:04&lt;00:02, 104.15iter/s, Error=1.2553e-03]Least Squares Iteration:  72%|███████▏  | 724/1000 [00:04&lt;00:02, 104.15iter/s, Error=1.2545e-03]Least Squares Iteration:  72%|███████▎  | 725/1000 [00:04&lt;00:02, 106.63iter/s, Error=1.2545e-03]Least Squares Iteration:  72%|███████▎  | 725/1000 [00:04&lt;00:02, 106.63iter/s, Error=1.2536e-03]Least Squares Iteration:  73%|███████▎  | 726/1000 [00:04&lt;00:02, 106.63iter/s, Error=1.2527e-03]Least Squares Iteration:  73%|███████▎  | 727/1000 [00:04&lt;00:02, 106.63iter/s, Error=1.2518e-03]Least Squares Iteration:  73%|███████▎  | 728/1000 [00:04&lt;00:02, 106.63iter/s, Error=1.2510e-03]Least Squares Iteration:  73%|███████▎  | 729/1000 [00:04&lt;00:02, 106.63iter/s, Error=1.2501e-03]Least Squares Iteration:  73%|███████▎  | 730/1000 [00:04&lt;00:02, 106.63iter/s, Error=1.2492e-03]Least Squares Iteration:  73%|███████▎  | 731/1000 [00:04&lt;00:02, 106.63iter/s, Error=1.2484e-03]Least Squares Iteration:  73%|███████▎  | 732/1000 [00:04&lt;00:02, 106.63iter/s, Error=1.2475e-03]Least Squares Iteration:  73%|███████▎  | 733/1000 [00:04&lt;00:02, 106.63iter/s, Error=1.2466e-03]Least Squares Iteration:  73%|███████▎  | 734/1000 [00:04&lt;00:02, 106.63iter/s, Error=1.2458e-03]Least Squares Iteration:  74%|███████▎  | 735/1000 [00:04&lt;00:02, 106.63iter/s, Error=1.2449e-03]Least Squares Iteration:  74%|███████▎  | 736/1000 [00:04&lt;00:02, 106.63iter/s, Error=1.2441e-03]Least Squares Iteration:  74%|███████▎  | 737/1000 [00:04&lt;00:02, 109.84iter/s, Error=1.2441e-03]Least Squares Iteration:  74%|███████▎  | 737/1000 [00:04&lt;00:02, 109.84iter/s, Error=1.2432e-03]Least Squares Iteration:  74%|███████▍  | 738/1000 [00:04&lt;00:02, 109.84iter/s, Error=1.2424e-03]Least Squares Iteration:  74%|███████▍  | 739/1000 [00:04&lt;00:02, 109.84iter/s, Error=1.2415e-03]Least Squares Iteration:  74%|███████▍  | 740/1000 [00:04&lt;00:02, 109.84iter/s, Error=1.2407e-03]Least Squares Iteration:  74%|███████▍  | 741/1000 [00:04&lt;00:02, 109.84iter/s, Error=1.2398e-03]Least Squares Iteration:  74%|███████▍  | 742/1000 [00:04&lt;00:02, 109.84iter/s, Error=1.2390e-03]Least Squares Iteration:  74%|███████▍  | 743/1000 [00:04&lt;00:02, 109.84iter/s, Error=1.2381e-03]Least Squares Iteration:  74%|███████▍  | 744/1000 [00:04&lt;00:02, 109.84iter/s, Error=1.2373e-03]Least Squares Iteration:  74%|███████▍  | 745/1000 [00:04&lt;00:02, 109.84iter/s, Error=1.2364e-03]Least Squares Iteration:  75%|███████▍  | 746/1000 [00:04&lt;00:02, 109.84iter/s, Error=1.2356e-03]Least Squares Iteration:  75%|███████▍  | 747/1000 [00:04&lt;00:02, 109.84iter/s, Error=1.2347e-03]Least Squares Iteration:  75%|███████▍  | 748/1000 [00:04&lt;00:02, 109.84iter/s, Error=1.2339e-03]Least Squares Iteration:  75%|███████▍  | 749/1000 [00:04&lt;00:02, 103.18iter/s, Error=1.2339e-03]Least Squares Iteration:  75%|███████▍  | 749/1000 [00:04&lt;00:02, 103.18iter/s, Error=1.2330e-03]Least Squares Iteration:  75%|███████▌  | 750/1000 [00:04&lt;00:02, 103.18iter/s, Error=1.2322e-03]Least Squares Iteration:  75%|███████▌  | 751/1000 [00:04&lt;00:02, 103.18iter/s, Error=1.2314e-03]Least Squares Iteration:  75%|███████▌  | 752/1000 [00:04&lt;00:02, 103.18iter/s, Error=1.2305e-03]Least Squares Iteration:  75%|███████▌  | 753/1000 [00:04&lt;00:02, 103.18iter/s, Error=1.2297e-03]Least Squares Iteration:  75%|███████▌  | 754/1000 [00:04&lt;00:02, 103.18iter/s, Error=1.2289e-03]Least Squares Iteration:  76%|███████▌  | 755/1000 [00:04&lt;00:02, 103.18iter/s, Error=1.2280e-03]Least Squares Iteration:  76%|███████▌  | 756/1000 [00:04&lt;00:02, 103.18iter/s, Error=1.2272e-03]Least Squares Iteration:  76%|███████▌  | 757/1000 [00:04&lt;00:02, 103.18iter/s, Error=1.2264e-03]Least Squares Iteration:  76%|███████▌  | 758/1000 [00:04&lt;00:02, 103.18iter/s, Error=1.2255e-03]Least Squares Iteration:  76%|███████▌  | 759/1000 [00:04&lt;00:02, 103.18iter/s, Error=1.2247e-03]Least Squares Iteration:  76%|███████▌  | 760/1000 [00:04&lt;00:02, 104.05iter/s, Error=1.2247e-03]Least Squares Iteration:  76%|███████▌  | 760/1000 [00:04&lt;00:02, 104.05iter/s, Error=1.2239e-03]Least Squares Iteration:  76%|███████▌  | 761/1000 [00:04&lt;00:02, 104.05iter/s, Error=1.2231e-03]Least Squares Iteration:  76%|███████▌  | 762/1000 [00:04&lt;00:02, 104.05iter/s, Error=1.2223e-03]Least Squares Iteration:  76%|███████▋  | 763/1000 [00:04&lt;00:02, 104.05iter/s, Error=1.2214e-03]Least Squares Iteration:  76%|███████▋  | 764/1000 [00:04&lt;00:02, 104.05iter/s, Error=1.2206e-03]Least Squares Iteration:  76%|███████▋  | 765/1000 [00:04&lt;00:02, 104.05iter/s, Error=1.2198e-03]Least Squares Iteration:  77%|███████▋  | 766/1000 [00:04&lt;00:02, 104.05iter/s, Error=1.2190e-03]Least Squares Iteration:  77%|███████▋  | 767/1000 [00:04&lt;00:02, 104.05iter/s, Error=1.2182e-03]Least Squares Iteration:  77%|███████▋  | 768/1000 [00:04&lt;00:02, 104.05iter/s, Error=1.2174e-03]Least Squares Iteration:  77%|███████▋  | 769/1000 [00:04&lt;00:02, 104.05iter/s, Error=1.2165e-03]Least Squares Iteration:  77%|███████▋  | 770/1000 [00:04&lt;00:02, 104.05iter/s, Error=1.2157e-03]Least Squares Iteration:  77%|███████▋  | 771/1000 [00:04&lt;00:02, 104.05iter/s, Error=1.2149e-03]Least Squares Iteration:  77%|███████▋  | 772/1000 [00:04&lt;00:02, 108.00iter/s, Error=1.2149e-03]Least Squares Iteration:  77%|███████▋  | 772/1000 [00:04&lt;00:02, 108.00iter/s, Error=1.2141e-03]Least Squares Iteration:  77%|███████▋  | 773/1000 [00:04&lt;00:02, 108.00iter/s, Error=1.2133e-03]Least Squares Iteration:  77%|███████▋  | 774/1000 [00:04&lt;00:02, 108.00iter/s, Error=1.2125e-03]Least Squares Iteration:  78%|███████▊  | 775/1000 [00:04&lt;00:02, 108.00iter/s, Error=1.2117e-03]Least Squares Iteration:  78%|███████▊  | 776/1000 [00:04&lt;00:02, 108.00iter/s, Error=1.2109e-03]Least Squares Iteration:  78%|███████▊  | 777/1000 [00:04&lt;00:02, 108.00iter/s, Error=1.2101e-03]Least Squares Iteration:  78%|███████▊  | 778/1000 [00:04&lt;00:02, 108.00iter/s, Error=1.2093e-03]Least Squares Iteration:  78%|███████▊  | 779/1000 [00:04&lt;00:02, 108.00iter/s, Error=1.2085e-03]Least Squares Iteration:  78%|███████▊  | 780/1000 [00:04&lt;00:02, 108.00iter/s, Error=1.2077e-03]Least Squares Iteration:  78%|███████▊  | 781/1000 [00:04&lt;00:02, 108.00iter/s, Error=1.2069e-03]Least Squares Iteration:  78%|███████▊  | 782/1000 [00:04&lt;00:02, 108.00iter/s, Error=1.2061e-03]Least Squares Iteration:  78%|███████▊  | 783/1000 [00:04&lt;00:02, 107.39iter/s, Error=1.2061e-03]Least Squares Iteration:  78%|███████▊  | 783/1000 [00:04&lt;00:02, 107.39iter/s, Error=1.2053e-03]Least Squares Iteration:  78%|███████▊  | 784/1000 [00:04&lt;00:02, 107.39iter/s, Error=1.2045e-03]Least Squares Iteration:  78%|███████▊  | 785/1000 [00:04&lt;00:02, 107.39iter/s, Error=1.2037e-03]Least Squares Iteration:  79%|███████▊  | 786/1000 [00:04&lt;00:01, 107.39iter/s, Error=1.2029e-03]Least Squares Iteration:  79%|███████▊  | 787/1000 [00:04&lt;00:01, 107.39iter/s, Error=1.2021e-03]Least Squares Iteration:  79%|███████▉  | 788/1000 [00:04&lt;00:01, 107.39iter/s, Error=1.2013e-03]Least Squares Iteration:  79%|███████▉  | 789/1000 [00:04&lt;00:01, 107.39iter/s, Error=1.2005e-03]Least Squares Iteration:  79%|███████▉  | 790/1000 [00:04&lt;00:01, 107.39iter/s, Error=1.1998e-03]Least Squares Iteration:  79%|███████▉  | 791/1000 [00:04&lt;00:01, 107.39iter/s, Error=1.1990e-03]Least Squares Iteration:  79%|███████▉  | 792/1000 [00:04&lt;00:01, 107.39iter/s, Error=1.1982e-03]Least Squares Iteration:  79%|███████▉  | 793/1000 [00:04&lt;00:01, 107.39iter/s, Error=1.1974e-03]Least Squares Iteration:  79%|███████▉  | 794/1000 [00:04&lt;00:01, 105.79iter/s, Error=1.1974e-03]Least Squares Iteration:  79%|███████▉  | 794/1000 [00:04&lt;00:01, 105.79iter/s, Error=1.1966e-03]Least Squares Iteration:  80%|███████▉  | 795/1000 [00:04&lt;00:01, 105.79iter/s, Error=1.1958e-03]Least Squares Iteration:  80%|███████▉  | 796/1000 [00:04&lt;00:01, 105.79iter/s, Error=1.1951e-03]Least Squares Iteration:  80%|███████▉  | 797/1000 [00:04&lt;00:01, 105.79iter/s, Error=1.1943e-03]Least Squares Iteration:  80%|███████▉  | 798/1000 [00:04&lt;00:01, 105.79iter/s, Error=1.1935e-03]Least Squares Iteration:  80%|███████▉  | 799/1000 [00:04&lt;00:01, 105.79iter/s, Error=1.1927e-03]Least Squares Iteration:  80%|████████  | 800/1000 [00:04&lt;00:01, 105.79iter/s, Error=1.1919e-03]Least Squares Iteration:  80%|████████  | 801/1000 [00:04&lt;00:01, 105.79iter/s, Error=1.1912e-03]Least Squares Iteration:  80%|████████  | 802/1000 [00:04&lt;00:01, 105.79iter/s, Error=1.1904e-03]Least Squares Iteration:  80%|████████  | 803/1000 [00:04&lt;00:01, 105.79iter/s, Error=1.1896e-03]Least Squares Iteration:  80%|████████  | 804/1000 [00:04&lt;00:01, 105.79iter/s, Error=1.1889e-03]Least Squares Iteration:  80%|████████  | 805/1000 [00:04&lt;00:01, 102.62iter/s, Error=1.1889e-03]Least Squares Iteration:  80%|████████  | 805/1000 [00:05&lt;00:01, 102.62iter/s, Error=1.1881e-03]Least Squares Iteration:  81%|████████  | 806/1000 [00:05&lt;00:01, 102.62iter/s, Error=1.1873e-03]Least Squares Iteration:  81%|████████  | 807/1000 [00:05&lt;00:01, 102.62iter/s, Error=1.1866e-03]Least Squares Iteration:  81%|████████  | 808/1000 [00:05&lt;00:01, 102.62iter/s, Error=1.1858e-03]Least Squares Iteration:  81%|████████  | 809/1000 [00:05&lt;00:01, 102.62iter/s, Error=1.1850e-03]Least Squares Iteration:  81%|████████  | 810/1000 [00:05&lt;00:01, 102.62iter/s, Error=1.1843e-03]Least Squares Iteration:  81%|████████  | 811/1000 [00:05&lt;00:01, 102.62iter/s, Error=1.1835e-03]Least Squares Iteration:  81%|████████  | 812/1000 [00:05&lt;00:01, 102.62iter/s, Error=1.1827e-03]Least Squares Iteration:  81%|████████▏ | 813/1000 [00:05&lt;00:01, 102.62iter/s, Error=1.1820e-03]Least Squares Iteration:  81%|████████▏ | 814/1000 [00:05&lt;00:01, 102.62iter/s, Error=1.1812e-03]Least Squares Iteration:  82%|████████▏ | 815/1000 [00:05&lt;00:01, 102.62iter/s, Error=1.1804e-03]Least Squares Iteration:  82%|████████▏ | 816/1000 [00:05&lt;00:01, 93.28iter/s, Error=1.1804e-03] Least Squares Iteration:  82%|████████▏ | 816/1000 [00:05&lt;00:01, 93.28iter/s, Error=1.1797e-03]Least Squares Iteration:  82%|████████▏ | 817/1000 [00:05&lt;00:01, 93.28iter/s, Error=1.1789e-03]Least Squares Iteration:  82%|████████▏ | 818/1000 [00:05&lt;00:01, 93.28iter/s, Error=1.1782e-03]Least Squares Iteration:  82%|████████▏ | 819/1000 [00:05&lt;00:01, 93.28iter/s, Error=1.1774e-03]Least Squares Iteration:  82%|████████▏ | 820/1000 [00:05&lt;00:01, 93.28iter/s, Error=1.1767e-03]Least Squares Iteration:  82%|████████▏ | 821/1000 [00:05&lt;00:01, 93.28iter/s, Error=1.1759e-03]Least Squares Iteration:  82%|████████▏ | 822/1000 [00:05&lt;00:01, 93.28iter/s, Error=1.1752e-03]Least Squares Iteration:  82%|████████▏ | 823/1000 [00:05&lt;00:01, 93.28iter/s, Error=1.1744e-03]Least Squares Iteration:  82%|████████▏ | 824/1000 [00:05&lt;00:01, 93.28iter/s, Error=1.1737e-03]Least Squares Iteration:  82%|████████▎ | 825/1000 [00:05&lt;00:01, 93.28iter/s, Error=1.1729e-03]Least Squares Iteration:  83%|████████▎ | 826/1000 [00:05&lt;00:01, 91.52iter/s, Error=1.1729e-03]Least Squares Iteration:  83%|████████▎ | 826/1000 [00:05&lt;00:01, 91.52iter/s, Error=1.1722e-03]Least Squares Iteration:  83%|████████▎ | 827/1000 [00:05&lt;00:01, 91.52iter/s, Error=1.1715e-03]Least Squares Iteration:  83%|████████▎ | 828/1000 [00:05&lt;00:01, 91.52iter/s, Error=1.1707e-03]Least Squares Iteration:  83%|████████▎ | 829/1000 [00:05&lt;00:01, 91.52iter/s, Error=1.1700e-03]Least Squares Iteration:  83%|████████▎ | 830/1000 [00:05&lt;00:01, 91.52iter/s, Error=1.1692e-03]Least Squares Iteration:  83%|████████▎ | 831/1000 [00:05&lt;00:01, 91.52iter/s, Error=1.1685e-03]Least Squares Iteration:  83%|████████▎ | 832/1000 [00:05&lt;00:01, 91.52iter/s, Error=1.1677e-03]Least Squares Iteration:  83%|████████▎ | 833/1000 [00:05&lt;00:01, 91.52iter/s, Error=1.1670e-03]Least Squares Iteration:  83%|████████▎ | 834/1000 [00:05&lt;00:01, 91.52iter/s, Error=1.1663e-03]Least Squares Iteration:  84%|████████▎ | 835/1000 [00:05&lt;00:01, 91.52iter/s, Error=1.1655e-03]Least Squares Iteration:  84%|████████▎ | 836/1000 [00:05&lt;00:01, 92.73iter/s, Error=1.1655e-03]Least Squares Iteration:  84%|████████▎ | 836/1000 [00:05&lt;00:01, 92.73iter/s, Error=1.1648e-03]Least Squares Iteration:  84%|████████▎ | 837/1000 [00:05&lt;00:01, 92.73iter/s, Error=1.1641e-03]Least Squares Iteration:  84%|████████▍ | 838/1000 [00:05&lt;00:01, 92.73iter/s, Error=1.1633e-03]Least Squares Iteration:  84%|████████▍ | 839/1000 [00:05&lt;00:01, 92.73iter/s, Error=1.1626e-03]Least Squares Iteration:  84%|████████▍ | 840/1000 [00:05&lt;00:01, 92.73iter/s, Error=1.1619e-03]Least Squares Iteration:  84%|████████▍ | 841/1000 [00:05&lt;00:01, 92.73iter/s, Error=1.1611e-03]Least Squares Iteration:  84%|████████▍ | 842/1000 [00:05&lt;00:01, 92.73iter/s, Error=1.1604e-03]Least Squares Iteration:  84%|████████▍ | 843/1000 [00:05&lt;00:01, 92.73iter/s, Error=1.1597e-03]Least Squares Iteration:  84%|████████▍ | 844/1000 [00:05&lt;00:01, 92.73iter/s, Error=1.1590e-03]Least Squares Iteration:  84%|████████▍ | 845/1000 [00:05&lt;00:01, 92.73iter/s, Error=1.1582e-03]Least Squares Iteration:  85%|████████▍ | 846/1000 [00:05&lt;00:01, 92.25iter/s, Error=1.1582e-03]Least Squares Iteration:  85%|████████▍ | 846/1000 [00:05&lt;00:01, 92.25iter/s, Error=1.1575e-03]Least Squares Iteration:  85%|████████▍ | 847/1000 [00:05&lt;00:01, 92.25iter/s, Error=1.1568e-03]Least Squares Iteration:  85%|████████▍ | 848/1000 [00:05&lt;00:01, 92.25iter/s, Error=1.1561e-03]Least Squares Iteration:  85%|████████▍ | 849/1000 [00:05&lt;00:01, 92.25iter/s, Error=1.1553e-03]Least Squares Iteration:  85%|████████▌ | 850/1000 [00:05&lt;00:01, 92.25iter/s, Error=1.1546e-03]Least Squares Iteration:  85%|████████▌ | 851/1000 [00:05&lt;00:01, 92.25iter/s, Error=1.1539e-03]Least Squares Iteration:  85%|████████▌ | 852/1000 [00:05&lt;00:01, 92.25iter/s, Error=1.1532e-03]Least Squares Iteration:  85%|████████▌ | 853/1000 [00:05&lt;00:01, 92.25iter/s, Error=1.1525e-03]Least Squares Iteration:  85%|████████▌ | 854/1000 [00:05&lt;00:01, 92.25iter/s, Error=1.1517e-03]Least Squares Iteration:  86%|████████▌ | 855/1000 [00:05&lt;00:01, 92.25iter/s, Error=1.1510e-03]Least Squares Iteration:  86%|████████▌ | 856/1000 [00:05&lt;00:01, 91.42iter/s, Error=1.1510e-03]Least Squares Iteration:  86%|████████▌ | 856/1000 [00:05&lt;00:01, 91.42iter/s, Error=1.1503e-03]Least Squares Iteration:  86%|████████▌ | 857/1000 [00:05&lt;00:01, 91.42iter/s, Error=1.1496e-03]Least Squares Iteration:  86%|████████▌ | 858/1000 [00:05&lt;00:01, 91.42iter/s, Error=1.1489e-03]Least Squares Iteration:  86%|████████▌ | 859/1000 [00:05&lt;00:01, 91.42iter/s, Error=1.1482e-03]Least Squares Iteration:  86%|████████▌ | 860/1000 [00:05&lt;00:01, 91.42iter/s, Error=1.1475e-03]Least Squares Iteration:  86%|████████▌ | 861/1000 [00:05&lt;00:01, 91.42iter/s, Error=1.1468e-03]Least Squares Iteration:  86%|████████▌ | 862/1000 [00:05&lt;00:01, 91.42iter/s, Error=1.1461e-03]Least Squares Iteration:  86%|████████▋ | 863/1000 [00:05&lt;00:01, 91.42iter/s, Error=1.1453e-03]Least Squares Iteration:  86%|████████▋ | 864/1000 [00:05&lt;00:01, 91.42iter/s, Error=1.1446e-03]Least Squares Iteration:  86%|████████▋ | 865/1000 [00:05&lt;00:01, 91.42iter/s, Error=1.1439e-03]Least Squares Iteration:  87%|████████▋ | 866/1000 [00:05&lt;00:01, 88.89iter/s, Error=1.1439e-03]Least Squares Iteration:  87%|████████▋ | 866/1000 [00:05&lt;00:01, 88.89iter/s, Error=1.1432e-03]Least Squares Iteration:  87%|████████▋ | 867/1000 [00:05&lt;00:01, 88.89iter/s, Error=1.1425e-03]Least Squares Iteration:  87%|████████▋ | 868/1000 [00:05&lt;00:01, 88.89iter/s, Error=1.1418e-03]Least Squares Iteration:  87%|████████▋ | 869/1000 [00:05&lt;00:01, 88.89iter/s, Error=1.1411e-03]Least Squares Iteration:  87%|████████▋ | 870/1000 [00:05&lt;00:01, 88.89iter/s, Error=1.1404e-03]Least Squares Iteration:  87%|████████▋ | 871/1000 [00:05&lt;00:01, 88.89iter/s, Error=1.1397e-03]Least Squares Iteration:  87%|████████▋ | 872/1000 [00:05&lt;00:01, 88.89iter/s, Error=1.1390e-03]Least Squares Iteration:  87%|████████▋ | 873/1000 [00:05&lt;00:01, 88.89iter/s, Error=1.1383e-03]Least Squares Iteration:  87%|████████▋ | 874/1000 [00:05&lt;00:01, 88.89iter/s, Error=1.1376e-03]Least Squares Iteration:  88%|████████▊ | 875/1000 [00:05&lt;00:01, 88.66iter/s, Error=1.1376e-03]Least Squares Iteration:  88%|████████▊ | 875/1000 [00:05&lt;00:01, 88.66iter/s, Error=1.1369e-03]Least Squares Iteration:  88%|████████▊ | 876/1000 [00:05&lt;00:01, 88.66iter/s, Error=1.1362e-03]Least Squares Iteration:  88%|████████▊ | 877/1000 [00:05&lt;00:01, 88.66iter/s, Error=1.1355e-03]Least Squares Iteration:  88%|████████▊ | 878/1000 [00:05&lt;00:01, 88.66iter/s, Error=1.1349e-03]Least Squares Iteration:  88%|████████▊ | 879/1000 [00:05&lt;00:01, 88.66iter/s, Error=1.1342e-03]Least Squares Iteration:  88%|████████▊ | 880/1000 [00:05&lt;00:01, 88.66iter/s, Error=1.1335e-03]Least Squares Iteration:  88%|████████▊ | 881/1000 [00:05&lt;00:01, 88.66iter/s, Error=1.1328e-03]Least Squares Iteration:  88%|████████▊ | 882/1000 [00:05&lt;00:01, 88.66iter/s, Error=1.1321e-03]Least Squares Iteration:  88%|████████▊ | 883/1000 [00:05&lt;00:01, 88.66iter/s, Error=1.1314e-03]Least Squares Iteration:  88%|████████▊ | 884/1000 [00:05&lt;00:01, 88.52iter/s, Error=1.1314e-03]Least Squares Iteration:  88%|████████▊ | 884/1000 [00:05&lt;00:01, 88.52iter/s, Error=1.1307e-03]Least Squares Iteration:  88%|████████▊ | 885/1000 [00:05&lt;00:01, 88.52iter/s, Error=1.1300e-03]Least Squares Iteration:  89%|████████▊ | 886/1000 [00:05&lt;00:01, 88.52iter/s, Error=1.1293e-03]Least Squares Iteration:  89%|████████▊ | 887/1000 [00:05&lt;00:01, 88.52iter/s, Error=1.1287e-03]Least Squares Iteration:  89%|████████▉ | 888/1000 [00:05&lt;00:01, 88.52iter/s, Error=1.1280e-03]Least Squares Iteration:  89%|████████▉ | 889/1000 [00:05&lt;00:01, 88.52iter/s, Error=1.1273e-03]Least Squares Iteration:  89%|████████▉ | 890/1000 [00:05&lt;00:01, 88.52iter/s, Error=1.1266e-03]Least Squares Iteration:  89%|████████▉ | 891/1000 [00:05&lt;00:01, 88.52iter/s, Error=1.1259e-03]Least Squares Iteration:  89%|████████▉ | 892/1000 [00:06&lt;00:01, 88.52iter/s, Error=1.1253e-03]Least Squares Iteration:  89%|████████▉ | 893/1000 [00:06&lt;00:01, 87.63iter/s, Error=1.1253e-03]Least Squares Iteration:  89%|████████▉ | 893/1000 [00:06&lt;00:01, 87.63iter/s, Error=1.1246e-03]Least Squares Iteration:  89%|████████▉ | 894/1000 [00:06&lt;00:01, 87.63iter/s, Error=1.1239e-03]Least Squares Iteration:  90%|████████▉ | 895/1000 [00:06&lt;00:01, 87.63iter/s, Error=1.1232e-03]Least Squares Iteration:  90%|████████▉ | 896/1000 [00:06&lt;00:01, 87.63iter/s, Error=1.1225e-03]Least Squares Iteration:  90%|████████▉ | 897/1000 [00:06&lt;00:01, 87.63iter/s, Error=1.1219e-03]Least Squares Iteration:  90%|████████▉ | 898/1000 [00:06&lt;00:01, 87.63iter/s, Error=1.1212e-03]Least Squares Iteration:  90%|████████▉ | 899/1000 [00:06&lt;00:01, 87.63iter/s, Error=1.1205e-03]Least Squares Iteration:  90%|█████████ | 900/1000 [00:06&lt;00:01, 87.63iter/s, Error=1.1199e-03]Least Squares Iteration:  90%|█████████ | 901/1000 [00:06&lt;00:01, 87.63iter/s, Error=1.1192e-03]Least Squares Iteration:  90%|█████████ | 902/1000 [00:06&lt;00:01, 87.63iter/s, Error=1.1185e-03]Least Squares Iteration:  90%|█████████ | 903/1000 [00:06&lt;00:01, 90.42iter/s, Error=1.1185e-03]Least Squares Iteration:  90%|█████████ | 903/1000 [00:06&lt;00:01, 90.42iter/s, Error=1.1178e-03]Least Squares Iteration:  90%|█████████ | 904/1000 [00:06&lt;00:01, 90.42iter/s, Error=1.1172e-03]Least Squares Iteration:  90%|█████████ | 905/1000 [00:06&lt;00:01, 90.42iter/s, Error=1.1165e-03]Least Squares Iteration:  91%|█████████ | 906/1000 [00:06&lt;00:01, 90.42iter/s, Error=1.1158e-03]Least Squares Iteration:  91%|█████████ | 907/1000 [00:06&lt;00:01, 90.42iter/s, Error=1.1152e-03]Least Squares Iteration:  91%|█████████ | 908/1000 [00:06&lt;00:01, 90.42iter/s, Error=1.1145e-03]Least Squares Iteration:  91%|█████████ | 909/1000 [00:06&lt;00:01, 90.42iter/s, Error=1.1138e-03]Least Squares Iteration:  91%|█████████ | 910/1000 [00:06&lt;00:00, 90.42iter/s, Error=1.1132e-03]Least Squares Iteration:  91%|█████████ | 911/1000 [00:06&lt;00:00, 90.42iter/s, Error=1.1125e-03]Least Squares Iteration:  91%|█████████ | 912/1000 [00:06&lt;00:00, 90.42iter/s, Error=1.1119e-03]Least Squares Iteration:  91%|█████████▏| 913/1000 [00:06&lt;00:00, 92.22iter/s, Error=1.1119e-03]Least Squares Iteration:  91%|█████████▏| 913/1000 [00:06&lt;00:00, 92.22iter/s, Error=1.1112e-03]Least Squares Iteration:  91%|█████████▏| 914/1000 [00:06&lt;00:00, 92.22iter/s, Error=1.1105e-03]Least Squares Iteration:  92%|█████████▏| 915/1000 [00:06&lt;00:00, 92.22iter/s, Error=1.1099e-03]Least Squares Iteration:  92%|█████████▏| 916/1000 [00:06&lt;00:00, 92.22iter/s, Error=1.1092e-03]Least Squares Iteration:  92%|█████████▏| 917/1000 [00:06&lt;00:00, 92.22iter/s, Error=1.1086e-03]Least Squares Iteration:  92%|█████████▏| 918/1000 [00:06&lt;00:00, 92.22iter/s, Error=1.1079e-03]Least Squares Iteration:  92%|█████████▏| 919/1000 [00:06&lt;00:00, 92.22iter/s, Error=1.1073e-03]Least Squares Iteration:  92%|█████████▏| 920/1000 [00:06&lt;00:00, 92.22iter/s, Error=1.1066e-03]Least Squares Iteration:  92%|█████████▏| 921/1000 [00:06&lt;00:00, 92.22iter/s, Error=1.1059e-03]Least Squares Iteration:  92%|█████████▏| 922/1000 [00:06&lt;00:00, 92.22iter/s, Error=1.1053e-03]Least Squares Iteration:  92%|█████████▏| 923/1000 [00:06&lt;00:00, 89.89iter/s, Error=1.1053e-03]Least Squares Iteration:  92%|█████████▏| 923/1000 [00:06&lt;00:00, 89.89iter/s, Error=1.1046e-03]Least Squares Iteration:  92%|█████████▏| 924/1000 [00:06&lt;00:00, 89.89iter/s, Error=1.1040e-03]Least Squares Iteration:  92%|█████████▎| 925/1000 [00:06&lt;00:00, 89.89iter/s, Error=1.1033e-03]Least Squares Iteration:  93%|█████████▎| 926/1000 [00:06&lt;00:00, 89.89iter/s, Error=1.1027e-03]Least Squares Iteration:  93%|█████████▎| 927/1000 [00:06&lt;00:00, 89.89iter/s, Error=1.1020e-03]Least Squares Iteration:  93%|█████████▎| 928/1000 [00:06&lt;00:00, 89.89iter/s, Error=1.1014e-03]Least Squares Iteration:  93%|█████████▎| 929/1000 [00:06&lt;00:00, 89.89iter/s, Error=1.1007e-03]Least Squares Iteration:  93%|█████████▎| 930/1000 [00:06&lt;00:00, 89.89iter/s, Error=1.1001e-03]Least Squares Iteration:  93%|█████████▎| 931/1000 [00:06&lt;00:00, 89.89iter/s, Error=1.0995e-03]Least Squares Iteration:  93%|█████████▎| 932/1000 [00:06&lt;00:00, 89.89iter/s, Error=1.0988e-03]Least Squares Iteration:  93%|█████████▎| 933/1000 [00:06&lt;00:00, 86.99iter/s, Error=1.0988e-03]Least Squares Iteration:  93%|█████████▎| 933/1000 [00:06&lt;00:00, 86.99iter/s, Error=1.0982e-03]Least Squares Iteration:  93%|█████████▎| 934/1000 [00:06&lt;00:00, 86.99iter/s, Error=1.0975e-03]Least Squares Iteration:  94%|█████████▎| 935/1000 [00:06&lt;00:00, 86.99iter/s, Error=1.0969e-03]Least Squares Iteration:  94%|█████████▎| 936/1000 [00:06&lt;00:00, 86.99iter/s, Error=1.0962e-03]Least Squares Iteration:  94%|█████████▎| 937/1000 [00:06&lt;00:00, 86.99iter/s, Error=1.0956e-03]Least Squares Iteration:  94%|█████████▍| 938/1000 [00:06&lt;00:00, 86.99iter/s, Error=1.0950e-03]Least Squares Iteration:  94%|█████████▍| 939/1000 [00:06&lt;00:00, 86.99iter/s, Error=1.0943e-03]Least Squares Iteration:  94%|█████████▍| 940/1000 [00:06&lt;00:00, 86.99iter/s, Error=1.0937e-03]Least Squares Iteration:  94%|█████████▍| 941/1000 [00:06&lt;00:00, 86.99iter/s, Error=1.0931e-03]Least Squares Iteration:  94%|█████████▍| 942/1000 [00:06&lt;00:00, 83.14iter/s, Error=1.0931e-03]Least Squares Iteration:  94%|█████████▍| 942/1000 [00:06&lt;00:00, 83.14iter/s, Error=1.0924e-03]Least Squares Iteration:  94%|█████████▍| 943/1000 [00:06&lt;00:00, 83.14iter/s, Error=1.0918e-03]Least Squares Iteration:  94%|█████████▍| 944/1000 [00:06&lt;00:00, 83.14iter/s, Error=1.0912e-03]Least Squares Iteration:  94%|█████████▍| 945/1000 [00:06&lt;00:00, 83.14iter/s, Error=1.0905e-03]Least Squares Iteration:  95%|█████████▍| 946/1000 [00:06&lt;00:00, 83.14iter/s, Error=1.0899e-03]Least Squares Iteration:  95%|█████████▍| 947/1000 [00:06&lt;00:00, 83.14iter/s, Error=1.0893e-03]Least Squares Iteration:  95%|█████████▍| 948/1000 [00:06&lt;00:00, 83.14iter/s, Error=1.0886e-03]Least Squares Iteration:  95%|█████████▍| 949/1000 [00:06&lt;00:00, 83.14iter/s, Error=1.0880e-03]Least Squares Iteration:  95%|█████████▌| 950/1000 [00:06&lt;00:00, 83.14iter/s, Error=1.0874e-03]Least Squares Iteration:  95%|█████████▌| 951/1000 [00:06&lt;00:00, 78.01iter/s, Error=1.0874e-03]Least Squares Iteration:  95%|█████████▌| 951/1000 [00:06&lt;00:00, 78.01iter/s, Error=1.0867e-03]Least Squares Iteration:  95%|█████████▌| 952/1000 [00:06&lt;00:00, 78.01iter/s, Error=1.0861e-03]Least Squares Iteration:  95%|█████████▌| 953/1000 [00:06&lt;00:00, 78.01iter/s, Error=1.0855e-03]Least Squares Iteration:  95%|█████████▌| 954/1000 [00:06&lt;00:00, 78.01iter/s, Error=1.0849e-03]Least Squares Iteration:  96%|█████████▌| 955/1000 [00:06&lt;00:00, 78.01iter/s, Error=1.0842e-03]Least Squares Iteration:  96%|█████████▌| 956/1000 [00:06&lt;00:00, 78.01iter/s, Error=1.0836e-03]Least Squares Iteration:  96%|█████████▌| 957/1000 [00:06&lt;00:00, 78.01iter/s, Error=1.0830e-03]Least Squares Iteration:  96%|█████████▌| 958/1000 [00:06&lt;00:00, 78.01iter/s, Error=1.0824e-03]Least Squares Iteration:  96%|█████████▌| 959/1000 [00:06&lt;00:00, 78.01iter/s, Error=1.0817e-03]Least Squares Iteration:  96%|█████████▌| 960/1000 [00:06&lt;00:00, 79.14iter/s, Error=1.0817e-03]Least Squares Iteration:  96%|█████████▌| 960/1000 [00:06&lt;00:00, 79.14iter/s, Error=1.0811e-03]Least Squares Iteration:  96%|█████████▌| 961/1000 [00:06&lt;00:00, 79.14iter/s, Error=1.0805e-03]Least Squares Iteration:  96%|█████████▌| 962/1000 [00:06&lt;00:00, 79.14iter/s, Error=1.0799e-03]Least Squares Iteration:  96%|█████████▋| 963/1000 [00:06&lt;00:00, 79.14iter/s, Error=1.0793e-03]Least Squares Iteration:  96%|█████████▋| 964/1000 [00:06&lt;00:00, 79.14iter/s, Error=1.0787e-03]Least Squares Iteration:  96%|█████████▋| 965/1000 [00:06&lt;00:00, 79.14iter/s, Error=1.0780e-03]Least Squares Iteration:  97%|█████████▋| 966/1000 [00:06&lt;00:00, 79.14iter/s, Error=1.0774e-03]Least Squares Iteration:  97%|█████████▋| 967/1000 [00:06&lt;00:00, 79.14iter/s, Error=1.0768e-03]Least Squares Iteration:  97%|█████████▋| 968/1000 [00:06&lt;00:00, 79.18iter/s, Error=1.0768e-03]Least Squares Iteration:  97%|█████████▋| 968/1000 [00:06&lt;00:00, 79.18iter/s, Error=1.0762e-03]Least Squares Iteration:  97%|█████████▋| 969/1000 [00:06&lt;00:00, 79.18iter/s, Error=1.0756e-03]Least Squares Iteration:  97%|█████████▋| 970/1000 [00:06&lt;00:00, 79.18iter/s, Error=1.0750e-03]Least Squares Iteration:  97%|█████████▋| 971/1000 [00:06&lt;00:00, 79.18iter/s, Error=1.0744e-03]Least Squares Iteration:  97%|█████████▋| 972/1000 [00:06&lt;00:00, 79.18iter/s, Error=1.0737e-03]Least Squares Iteration:  97%|█████████▋| 973/1000 [00:07&lt;00:00, 79.18iter/s, Error=1.0731e-03]Least Squares Iteration:  97%|█████████▋| 974/1000 [00:07&lt;00:00, 79.18iter/s, Error=1.0725e-03]Least Squares Iteration:  98%|█████████▊| 975/1000 [00:07&lt;00:00, 79.18iter/s, Error=1.0719e-03]Least Squares Iteration:  98%|█████████▊| 976/1000 [00:07&lt;00:00, 76.36iter/s, Error=1.0719e-03]Least Squares Iteration:  98%|█████████▊| 976/1000 [00:07&lt;00:00, 76.36iter/s, Error=1.0713e-03]Least Squares Iteration:  98%|█████████▊| 977/1000 [00:07&lt;00:00, 76.36iter/s, Error=1.0707e-03]Least Squares Iteration:  98%|█████████▊| 978/1000 [00:07&lt;00:00, 76.36iter/s, Error=1.0701e-03]Least Squares Iteration:  98%|█████████▊| 979/1000 [00:07&lt;00:00, 76.36iter/s, Error=1.0695e-03]Least Squares Iteration:  98%|█████████▊| 980/1000 [00:07&lt;00:00, 76.36iter/s, Error=1.0689e-03]Least Squares Iteration:  98%|█████████▊| 981/1000 [00:07&lt;00:00, 76.36iter/s, Error=1.0683e-03]Least Squares Iteration:  98%|█████████▊| 982/1000 [00:07&lt;00:00, 76.36iter/s, Error=1.0677e-03]Least Squares Iteration:  98%|█████████▊| 983/1000 [00:07&lt;00:00, 76.36iter/s, Error=1.0671e-03]Least Squares Iteration:  98%|█████████▊| 984/1000 [00:07&lt;00:00, 70.77iter/s, Error=1.0671e-03]Least Squares Iteration:  98%|█████████▊| 984/1000 [00:07&lt;00:00, 70.77iter/s, Error=1.0665e-03]Least Squares Iteration:  98%|█████████▊| 985/1000 [00:07&lt;00:00, 70.77iter/s, Error=1.0659e-03]Least Squares Iteration:  99%|█████████▊| 986/1000 [00:07&lt;00:00, 70.77iter/s, Error=1.0653e-03]Least Squares Iteration:  99%|█████████▊| 987/1000 [00:07&lt;00:00, 70.77iter/s, Error=1.0647e-03]Least Squares Iteration:  99%|█████████▉| 988/1000 [00:07&lt;00:00, 70.77iter/s, Error=1.0641e-03]Least Squares Iteration:  99%|█████████▉| 989/1000 [00:07&lt;00:00, 70.77iter/s, Error=1.0635e-03]Least Squares Iteration:  99%|█████████▉| 990/1000 [00:07&lt;00:00, 70.77iter/s, Error=1.0629e-03]Least Squares Iteration:  99%|█████████▉| 991/1000 [00:07&lt;00:00, 70.77iter/s, Error=1.0623e-03]Least Squares Iteration:  99%|█████████▉| 992/1000 [00:07&lt;00:00, 70.37iter/s, Error=1.0623e-03]Least Squares Iteration:  99%|█████████▉| 992/1000 [00:07&lt;00:00, 70.37iter/s, Error=1.0617e-03]Least Squares Iteration:  99%|█████████▉| 993/1000 [00:07&lt;00:00, 70.37iter/s, Error=1.0611e-03]Least Squares Iteration:  99%|█████████▉| 994/1000 [00:07&lt;00:00, 70.37iter/s, Error=1.0605e-03]Least Squares Iteration: 100%|█████████▉| 995/1000 [00:07&lt;00:00, 70.37iter/s, Error=1.0599e-03]Least Squares Iteration: 100%|█████████▉| 996/1000 [00:07&lt;00:00, 70.37iter/s, Error=1.0593e-03]Least Squares Iteration: 100%|█████████▉| 997/1000 [00:07&lt;00:00, 70.37iter/s, Error=1.0587e-03]Least Squares Iteration: 100%|█████████▉| 998/1000 [00:07&lt;00:00, 70.37iter/s, Error=1.0581e-03]Least Squares Iteration: 100%|█████████▉| 999/1000 [00:07&lt;00:00, 70.37iter/s, Error=1.0575e-03]Least Squares Iteration: 100%|██████████| 1000/1000 [00:07&lt;00:00, 70.45iter/s, Error=1.0575e-03]Least Squares Iteration: 100%|██████████| 1000/1000 [00:07&lt;00:00, 135.15iter/s, Error=1.0575e-03]\n\n\n\n\n\n\n\n\n\nNote that torch does have the framework to run autograd on the least squares objective itself, but for this general method we are using the adjoint to compute the gradient (and indirectly invoking autograd). This framework is the most general for when there might not be explicit analytic solutions to the least squares problem, but we have the forward operator and its adjoint."
  },
  {
    "objectID": "content/eosc555/lectures/lecture1-2/index.html",
    "href": "content/eosc555/lectures/lecture1-2/index.html",
    "title": "Lecture 1: Introduction to Inverse Theory",
    "section": "",
    "text": "Inverse theory is a set of mathematical techniques used to infer the properties of a physical system from observations of its output. It is a fundamental tool in many scientific disciplines, including geophysics, seismology, and medical imaging. Inverse theory is used to solve a wide range of problems, such as:\n\nParameter Estimation: Determining the values of unknown parameters in a model that best fit the observed data.\nSystem Identification: Identifying the structure and dynamics of a system from input-output data.\nImage Reconstruction: Reconstructing an image or object from noisy or incomplete measurements.\n\nWhat many of these tasks have in common is that we are working with incomplete information. There is a forward problem that has generated the data that we observe \\(\\vec{b}\\) from a set of input data \\(\\vec{x}\\), and we want to infer the inverse problem that generated the data. However the inverse problem is often ill-posed, meaning that there are multiple solutions that can fit the data equally well. Inverse theory provides a framework for finding the best solution to these problems.\nThe forward problem can be described for example as a differetial equation or operator \\(L\\) that takes in some measured parameters \\(u\\) with model parameters \\(x\\) :\n\\[ L(x)[u] = q \\iff u = L^{-1}(x)[q] \\]\nFor example making measurements of an electromagnetic field in correspondence to conductivity values that are underground we have:\n\\[ \\nabla \\sigma \\nabla u = q + \\text{BC}\\]\nWe measure the \\(u\\) at some points and use that to try and form an estimate of the conductivity \\(\\sigma\\). The forward problem is to solve for \\(u\\) given \\(\\sigma\\) and the inverse problem is to solve for \\(\\sigma\\) given \\(u\\). The forward problem is often well-posed and the inverse problem is often ill-posed.\nFor a computational framework we can discretize the the equation so that the operator is a matrix \\(A\\) and the data is a vector \\(\\vec{b}\\):\n\\[ \\underbrace{A}_{\\text{Forward Map}} \\underbrace{\\vec{x}}_{\\text{Model Parameters}} + \\epsilon = \\underbrace{\\vec{b}}_{\\text{Observed Data}} \\]\nIn this case we may have a sparse set of measurements \\(b\\) and a large set of \\(x\\) making the problem underdetermined. The goal of inverse theory is to find the best estimate of \\(x\\) given \\(b\\).\n\n\nTo illustrate the concept of inverse theory, consider the following example:\n\nSuppose that you have agreed to meet a friend to watch them during a triathlon race but you showed up late and missed the start. They are expecting for you to have been there at some point during the time at which they were changing from a running phase to a cycle phase. They expect you to know the time at which they made the transition. However you only know the overall start time and finish time of the race.\nIf the race starts at time \\(t=0\\) and then ends at time \\(t=b\\) how do you use this information to deduce the actual time \\(t_r \\in [0,b]\\) at which they crossed the transition zone of the race?\n\nThe first restriction on feasible solutions is the domain \\([0,b]\\) so that we know that \\(0&lt;t_r&lt;b\\).\nAfter this there are some other techniquest that we could use to better inform the probability of the occurence at different times. For example, we might have a good idea of their fitness level or average running speed from previous experience. Or in the abscence of this information there might be average times for the competitors that are available to further inform the problem and reduce the amount of error in the estimate.\n\n\n\nFor cases where the matrix \\(A\\) is not full rank, the singular value decomposition (SVD) provides a more general framework for solving the least squares problem. The SVD decomposes the matrix \\(A\\) into three matrices \\(U\\), \\(\\Sigma\\), and \\(V\\)\n\\[ A = U \\Sigma V^T \\]\nThe matrices have the following special properties:\n\nOrthogonal Subspaces: \\(U\\) and \\(V\\) are orthogonal matrices, meaning that \\(U^TU = I\\) and \\(V^TV = I\\), that is \\(U^T = U^{-1}\\) and $V^T = V^{-1}.\nOrdered Singular Values: \\(\\Sigma\\) is a diagonal matrix with non-negative values on the diagonal, known as the singular values of \\(A\\). The singular values are ordered such that \\(\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r\\). The number of non-zero singular values is equal to the rank of \\(A\\).\n\nSupposed that we have a \\(\\text{rank}(A) = r\\) matrix \\(A\\) which maps from \\(\\mathbb{R}^m\\rightarrow \\mathbb{R}^n\\). A fundamental way to view this mapping is as a composition of three linear transformations: a rotation \\(V\\), a scaling \\(\\Sigma\\), and another rotation \\(U\\). The orthogonal matrix \\(V\\) has the property that all of its rows and columns are orthogonal to each other, and the vectors themselves are normalized to \\(1\\). To see this property of the orthogonal matrix consider that \\(V^T V = I\\) and \\(V V^T = I\\):\n\\[ \\begin{align}\nZ = V^T V &= I \\\\\nz_{ij} = \\langle v_i, v_j \\rangle &= \\delta_{ij} \\end{align} \\]\nEach of the elements of the matrix \\(V^T\\) is the dot product of the \\(i\\)th and \\(j\\)th columns of \\(V\\). The dotproduct of all vectors against themselves is \\(1\\) and the dotproduct of any two different vectors is \\(0\\). So from this we can see that all of the columns of \\(V\\) are orthogonal to each other. The same property holds for \\(U\\).\n\\(V^T\\) by our definition of \\(A\\) must accept a vector from \\(\\mathbb{R}^m\\) and the matrix is square, indicating an \\(m \\times m\\) matrix. The matrix \\(U\\) must output a vector in \\(\\mathbb{R}^n\\) and the matrix is square, indicating an \\(n \\times n\\) matrix. The matrix \\(\\Sigma\\) must be \\(n \\times m\\) to map from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^n\\).\nIn all its glory:\n\\[\n\\begin{aligned}\nA_{n \\times m} &= U_{n \\times n} \\, \\Sigma_{n \\times m} \\, V^T_{m \\times m} \\\\\n&= \\left[ \\begin{array}{ccc|ccc}\n\\mathbf{u}_1 & \\cdots & \\mathbf{u}_r & \\mathbf{u}_{r+1} & \\cdots & \\mathbf{u}_n\n\\end{array} \\right]_{n \\times n}\n\\left[ \\begin{array}{ccc}\n\\sigma_1 &  &  \\\\\n& \\ddots &  \\\\\n&  & \\sigma_r \\\\\n0 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & 0\n\\end{array} \\right]_{n \\times m}\n\\left[ \\begin{array}{ccc|ccc}\n\\mathbf{v}^T_1 \\\\\n\\vdots \\\\\n\\mathbf{v}^T_r \\\\\n\\mathbf{v}^T_{r+1} \\\\\n\\vdots \\\\\n  \\mathbf{v}^T_m\n\\end{array} \\right]_{m \\times m}\n\\end{aligned}\n\\]\nIn this case the first \\(r\\) columns of \\(U\\) are the range of \\(A\\), the rest of \\(U\\) is filled with its orthogonal complement. The first \\(r\\) columns of \\(V\\) are the domain of \\(A\\), the rest of \\(V\\) is filled with its orthogonal complement. These are the four fundamental subspaces of the matrix \\(A\\), more information on this can be found at: Wikipedia: SVD\nThe matrices as shown above are for a rectangular \\(A\\) where \\(n&gt;m\\) but the same properties hold for all \\(n,m\\). Some of the singular values \\(\\sigma_i\\) may be zero, in which case the matrix \\(A\\) is not full rank.\nAnother way to decompose the SVD is to write it as a sum of outer products that are scaled by the diagonal matrix of singular values:\n\\[ A = \\sum_{i=1}^r \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T \\]\nIf \\(\\sigma_i&gt;0\\) then \\(v_i\\) is not in the null space of \\(A\\) because \\(A v_i = \\sigma_i u_i\\). If \\(\\sigma_i = 0\\) then \\(v_i\\) is in the null space of \\(A\\) because \\(A v_i = 0\\).\n\n\nBack to the task of inverting \\(Ax + \\epsilon = b\\) we can apply the SVD decomposition:\n\\[\\begin{align}\nU \\Sigma V^T x + \\epsilon &= b \\\\\n\\Sigma V^T x +&= U^T (b-\\epsilon) \\\\\nV \\Sigma^{-1} U^T (b-\\epsilon) &= x\\\\\nA^+ (b-\\epsilon) &= \\hat{x}\n\\end{align}\\]\nWhere \\(A^+ = V \\Sigma^{-1} U^T\\) is the pseudoinverse of \\(A\\). The pseudoinverse is a generalization of the matrix inverse for non-square matrices. We recover a square matrix by removing all of the absent or zero singular values from \\(\\Sigma\\) and inverting the rest, giving an \\(r \\times r\\) diagonal matrix whose inverse is simply the inverse of each element.\n\\[ \\left[ \\begin{array}{ccc}\n\\sigma_1 &  &  \\\\\n& \\ddots &  \\\\\n&  & \\sigma_r \\\\\n0 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & 0\n\\end{array} \\right]_{n \\times m}\n\\rightarrow \\left[ \\begin{array}{ccc}\n\\sigma_1^{-1} &  &  \\\\\n  & \\ddots &  \\\\\n  &  & \\sigma_r^{-1} \\\\\n  \\end{array} \\right]_{r \\times r}\\]\nThen \\[\\hat{x} = \\sum_i^N \\sigma_i^{-1} \\mathbf{u}_i^T (b-\\epsilon) \\mathbf{v}_i\\] is the solution to the least squares problem. This can be solved also as a truncated sum since \\(0&lt;N&lt;r\\). In actual practice with real world measurement we end up with many singular values that may be effectively \\(0\\) by nature of being very small relative to the noise in the data and the largest single value. We have that the solution \\(\\hat{x}\\) is a sum of \\(v_i\\) components that form an orthogonal basis \\(\\hat{x} = \\sum_i \\beta_i v_i\\) where \\(\\beta_i = \\frac{u_i^T (b-\\epsilon)}{\\sigma_i}\\). These small singular values blow up in size when inverted and so extra truncation is often necessary to avoid numerical instability and excessive amplification of noise \\(\\epsilon\\).\n\n\n\n\nLeast squares and matrix inversion is a classic starting point for understanding inverse theory. Suppose that we have input data \\(\\vec{x}\\) and output data \\(\\vec{b}\\) that are related by a linear system of equations: \\[Ax = b\\] where \\(A\\) is a matrix of coefficients. In many cases, the system is overdetermined, meaning that there are more equations than unknowns. In this case, there is no exact solution to the system, and we must find the best solution that minimizes the error between the observed data \\(\\vec{b}\\) and the predicted data \\(A\\vec{x}\\). In the simplest form of inversion that we can attempt, we can solve the least squares solution. In this case we reject all of the observed data that is from the null space of \\(A\\) assuming a zero value for each of those parameters.\n\n\nLet \\(A\\) be a \\(3 \\times 2\\) matrix and \\(\\vec{b}\\) be a \\(3 \\times 1\\) vector. The \\(\\vec{x}\\) that we are trying to solve for is a \\(2 \\times 1\\) vector. The system of equations is given by:\n\\[ A = \\begin{bmatrix}  \\vec{a}_1 & \\vec{a}_2 \\end{bmatrix} \\quad \\vec{x} = \\begin{bmatrix} x_1 \\\\ x_2  \\end{bmatrix}  \\quad \\vec{b} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix} \\]\nIn this case we have an overdetermined system with three equations, two unknowns, and three data samples. If the system of equations is full rank then we are trying to map from a 2D space to a 3D space: \\(A: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3\\). In this case there is no exact solution to the system for any \\(b\\) that is not in the column space of \\(A\\).\nInstead we can solve for the least squares solution \\(\\vec{x}_{LS}\\) by minimizing the error between the observed data \\(\\vec{b}\\) and the predicted data \\(A\\vec{x}\\) from the forward model.\n\\[ \\vec{x}_{LS} = \\arg \\min_{\\vec{x}} ||A\\vec{x} - \\vec{b}||_2^2 \\]\nWe want to find the argument that minimizes the function \\(f(\\vec{x}) = ||A\\vec{x} - \\vec{b}||_2^2\\). By first order optimality conditions, the gradient of the function must be zero at the minimum.\n\\[ \\begin{align}\n\\nabla f(\\vec{x}) &= 0 \\\\\n\\nabla ||A\\vec{x} - \\vec{b}||_2^2 &= 0 \\\\\n\\nabla (A\\vec{x} - \\vec{b})^T (A\\vec{x} - \\vec{b}) &= 0 \\\\\n\\nabla \\left( \\vec{x}^T A^T A \\vec{x} - 2 \\vec{b}^T A \\vec{x} + \\vec{b}^T \\vec{b} \\right) &= 0 \\\\\n2 A^T A \\vec{x} - 2 A^T \\vec{b} &= 0 \\\\\nA^T A \\vec{x} &= A^T \\vec{b} \\\\\n\\vec{x}_{LS} &= (A^T A)^{-1} A^T \\vec{b}\n\\end{align} \\]\nThis is known as the normal equations for the least squares solution. We take a note of caution here that \\(A^T A\\) must be invertible for this solution to exist. If \\(A\\) is not full rank then the matrix \\(A^T A\\) will not be invertible and other methods must be used.\nWe call the difference between the observed data and the predicted data the residual.\n\\(r = \\vec{b} - A\\vec{x}_{LS}\\)\nUsing this information, what we really want to minimize is the sum of the squares of the residuals: \\(||r||_2^2\\). This is the same as the sum of the squares of the errors in the data.\nThere is an altogether informative way to think about the minimization problem purely in terms of linear algebra and subspaces to derive the same normal equations.\n\n\n\nLeast Squares Visual\n\n\nWe have the range of \\(A\\) or image of \\(A\\) as the subspace of \\(\\mathbb{R}^3\\) that is spanned by the columns of \\(A\\). This subspace is rank \\(2\\) because there are only two columns in \\(A\\), \\(R(A) \\subset \\mathbb{R}^3\\). The inaccessible parts of \\(\\mathbb{R}^3\\) are in the orthogonal complement of \\(R(A)\\), \\(R(A)^\\perp\\). Recalling that \\(R(A)^\\perp = N(A^T)\\) we can diagram the solution to least squares as a minimization of the error vector \\(r\\) in the orthogonal complement of \\(R(A)\\).\nAs seen the \\(r\\) vector is perpendicular to the \\(x_{LS}\\) solution, the projection of \\(r\\) onto \\(R(A)\\) is zero. Since it is in a null space of \\(A^T\\) then \\(A^T r = 0\\).\n\\[ \\begin{align} A^T \\left ( Ax_{LS} - b \\right ) &= 0\\\\\nA^T A x_{LS} &= A^T b \\\\\n\\end {align} \\]\nSo we recover the normal equations without using any of the machinery of calculus.\nFor a review on the four fundamental subspaces of a matrix see the UBC Math 307 notes on the topic: Math 307"
  },
  {
    "objectID": "content/eosc555/lectures/lecture1-2/index.html#the-singular-value-decomposition",
    "href": "content/eosc555/lectures/lecture1-2/index.html#the-singular-value-decomposition",
    "title": "Lecture 1: Introduction to Inverse Theory",
    "section": "",
    "text": "For cases where the matrix \\(A\\) is not full rank, the singular value decomposition (SVD) provides a more general framework for solving the least squares problem. The SVD decomposes the matrix \\(A\\) into three matrices \\(U\\), \\(\\Sigma\\), and \\(V\\)\n\\[ A = U \\Sigma V^T \\]\nThe matrices have the following special properties:\n\nOrthogonal Subspaces: \\(U\\) and \\(V\\) are orthogonal matrices, meaning that \\(U^TU = I\\) and \\(V^TV = I\\), that is \\(U^T = U^{-1}\\) and $V^T = V^{-1}.\nOrdered Singular Values: \\(\\Sigma\\) is a diagonal matrix with non-negative values on the diagonal, known as the singular values of \\(A\\). The singular values are ordered such that \\(\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r\\). The number of non-zero singular values is equal to the rank of \\(A\\).\n\nSupposed that we have a \\(\\text{rank}(A) = r\\) matrix \\(A\\) which maps from \\(\\mathbb{R}^m\\rightarrow \\mathbb{R}^n\\). A fundamental way to view this mapping is as a composition of three linear transformations: a rotation \\(V\\), a scaling \\(\\Sigma\\), and another rotation \\(U\\). The orthogonal matrix \\(V\\) has the property that all of its rows and columns are orthogonal to each other, and the vectors themselves are normalized to \\(1\\). To see this property of the orthogonal matrix consider that \\(V^T V = I\\) and \\(V V^T = I\\):\n\\[ \\begin{align}\nZ = V^T V &= I \\\\\nz_{ij} = \\langle v_i, v_j \\rangle &= \\delta_{ij} \\end{align} \\]\nEach of the elements of the matrix \\(V^T\\) is the dot product of the \\(i\\)th and \\(j\\)th columns of \\(V\\). The dotproduct of all vectors against themselves is \\(1\\) and the dotproduct of any two different vectors is \\(0\\). So from this we can see that all of the columns of \\(V\\) are orthogonal to each other. The same property holds for \\(U\\).\n\\(V^T\\) by our definition of \\(A\\) must accept a vector from \\(\\mathbb{R}^m\\) and the matrix is square, indicating an \\(m \\times m\\) matrix. The matrix \\(U\\) must output a vector in \\(\\mathbb{R}^n\\) and the matrix is square, indicating an \\(n \\times n\\) matrix. The matrix \\(\\Sigma\\) must be \\(n \\times m\\) to map from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^n\\).\nIn all its glory:\n\\[\n\\begin{aligned}\nA_{n \\times m} &= U_{n \\times n} \\, \\Sigma_{n \\times m} \\, V^T_{m \\times m} \\\\\n&= \\left[ \\begin{array}{ccc|ccc}\n\\mathbf{u}_1 & \\cdots & \\mathbf{u}_r & \\mathbf{u}_{r+1} & \\cdots & \\mathbf{u}_n\n\\end{array} \\right]_{n \\times n}\n\\left[ \\begin{array}{ccc}\n\\sigma_1 &  &  \\\\\n& \\ddots &  \\\\\n&  & \\sigma_r \\\\\n0 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & 0\n\\end{array} \\right]_{n \\times m}\n\\left[ \\begin{array}{ccc|ccc}\n\\mathbf{v}^T_1 \\\\\n\\vdots \\\\\n\\mathbf{v}^T_r \\\\\n\\mathbf{v}^T_{r+1} \\\\\n\\vdots \\\\\n  \\mathbf{v}^T_m\n\\end{array} \\right]_{m \\times m}\n\\end{aligned}\n\\]\nIn this case the first \\(r\\) columns of \\(U\\) are the range of \\(A\\), the rest of \\(U\\) is filled with its orthogonal complement. The first \\(r\\) columns of \\(V\\) are the domain of \\(A\\), the rest of \\(V\\) is filled with its orthogonal complement. These are the four fundamental subspaces of the matrix \\(A\\), more information on this can be found at: Wikipedia: SVD\nThe matrices as shown above are for a rectangular \\(A\\) where \\(n&gt;m\\) but the same properties hold for all \\(n,m\\). Some of the singular values \\(\\sigma_i\\) may be zero, in which case the matrix \\(A\\) is not full rank.\nAnother way to decompose the SVD is to write it as a sum of outer products that are scaled by the diagonal matrix of singular values:\n\\[ A = \\sum_{i=1}^r \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T \\]\nIf \\(\\sigma_i&gt;0\\) then \\(v_i\\) is not in the null space of \\(A\\) because \\(A v_i = \\sigma_i u_i\\). If \\(\\sigma_i = 0\\) then \\(v_i\\) is in the null space of \\(A\\) because \\(A v_i = 0\\).\n\n\nBack to the task of inverting \\(Ax + \\epsilon = b\\) we can apply the SVD decomposition:\n\\[\\begin{align}\nU \\Sigma V^T x + \\epsilon &= b \\\\\n\\Sigma V^T x +&= U^T (b-\\epsilon) \\\\\nV \\Sigma^{-1} U^T (b-\\epsilon) &= x\\\\\nA^+ (b-\\epsilon) &= \\hat{x}\n\\end{align}\\]\nWhere \\(A^+ = V \\Sigma^{-1} U^T\\) is the pseudoinverse of \\(A\\). The pseudoinverse is a generalization of the matrix inverse for non-square matrices. We recover a square matrix by removing all of the absent or zero singular values from \\(\\Sigma\\) and inverting the rest, giving an \\(r \\times r\\) diagonal matrix whose inverse is simply the inverse of each element.\n\\[ \\left[ \\begin{array}{ccc}\n\\sigma_1 &  &  \\\\\n& \\ddots &  \\\\\n&  & \\sigma_r \\\\\n0 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & 0\n\\end{array} \\right]_{n \\times m}\n\\rightarrow \\left[ \\begin{array}{ccc}\n\\sigma_1^{-1} &  &  \\\\\n  & \\ddots &  \\\\\n  &  & \\sigma_r^{-1} \\\\\n  \\end{array} \\right]_{r \\times r}\\]\nThen \\[\\hat{x} = \\sum_i^N \\sigma_i^{-1} \\mathbf{u}_i^T (b-\\epsilon) \\mathbf{v}_i\\] is the solution to the least squares problem. This can be solved also as a truncated sum since \\(0&lt;N&lt;r\\). In actual practice with real world measurement we end up with many singular values that may be effectively \\(0\\) by nature of being very small relative to the noise in the data and the largest single value. We have that the solution \\(\\hat{x}\\) is a sum of \\(v_i\\) components that form an orthogonal basis \\(\\hat{x} = \\sum_i \\beta_i v_i\\) where \\(\\beta_i = \\frac{u_i^T (b-\\epsilon)}{\\sigma_i}\\). These small singular values blow up in size when inverted and so extra truncation is often necessary to avoid numerical instability and excessive amplification of noise \\(\\epsilon\\)."
  },
  {
    "objectID": "content/eosc555/lectures/lecture1-2/index.html#least-squares",
    "href": "content/eosc555/lectures/lecture1-2/index.html#least-squares",
    "title": "Lecture 1: Introduction to Inverse Theory",
    "section": "",
    "text": "Least squares and matrix inversion is a classic starting point for understanding inverse theory. Suppose that we have input data \\(\\vec{x}\\) and output data \\(\\vec{b}\\) that are related by a linear system of equations: \\[Ax = b\\] where \\(A\\) is a matrix of coefficients. In many cases, the system is overdetermined, meaning that there are more equations than unknowns. In this case, there is no exact solution to the system, and we must find the best solution that minimizes the error between the observed data \\(\\vec{b}\\) and the predicted data \\(A\\vec{x}\\). In the simplest form of inversion that we can attempt, we can solve the least squares solution. In this case we reject all of the observed data that is from the null space of \\(A\\) assuming a zero value for each of those parameters.\n\n\nLet \\(A\\) be a \\(3 \\times 2\\) matrix and \\(\\vec{b}\\) be a \\(3 \\times 1\\) vector. The \\(\\vec{x}\\) that we are trying to solve for is a \\(2 \\times 1\\) vector. The system of equations is given by:\n\\[ A = \\begin{bmatrix}  \\vec{a}_1 & \\vec{a}_2 \\end{bmatrix} \\quad \\vec{x} = \\begin{bmatrix} x_1 \\\\ x_2  \\end{bmatrix}  \\quad \\vec{b} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix} \\]\nIn this case we have an overdetermined system with three equations, two unknowns, and three data samples. If the system of equations is full rank then we are trying to map from a 2D space to a 3D space: \\(A: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3\\). In this case there is no exact solution to the system for any \\(b\\) that is not in the column space of \\(A\\).\nInstead we can solve for the least squares solution \\(\\vec{x}_{LS}\\) by minimizing the error between the observed data \\(\\vec{b}\\) and the predicted data \\(A\\vec{x}\\) from the forward model.\n\\[ \\vec{x}_{LS} = \\arg \\min_{\\vec{x}} ||A\\vec{x} - \\vec{b}||_2^2 \\]\nWe want to find the argument that minimizes the function \\(f(\\vec{x}) = ||A\\vec{x} - \\vec{b}||_2^2\\). By first order optimality conditions, the gradient of the function must be zero at the minimum.\n\\[ \\begin{align}\n\\nabla f(\\vec{x}) &= 0 \\\\\n\\nabla ||A\\vec{x} - \\vec{b}||_2^2 &= 0 \\\\\n\\nabla (A\\vec{x} - \\vec{b})^T (A\\vec{x} - \\vec{b}) &= 0 \\\\\n\\nabla \\left( \\vec{x}^T A^T A \\vec{x} - 2 \\vec{b}^T A \\vec{x} + \\vec{b}^T \\vec{b} \\right) &= 0 \\\\\n2 A^T A \\vec{x} - 2 A^T \\vec{b} &= 0 \\\\\nA^T A \\vec{x} &= A^T \\vec{b} \\\\\n\\vec{x}_{LS} &= (A^T A)^{-1} A^T \\vec{b}\n\\end{align} \\]\nThis is known as the normal equations for the least squares solution. We take a note of caution here that \\(A^T A\\) must be invertible for this solution to exist. If \\(A\\) is not full rank then the matrix \\(A^T A\\) will not be invertible and other methods must be used.\nWe call the difference between the observed data and the predicted data the residual.\n\\(r = \\vec{b} - A\\vec{x}_{LS}\\)\nUsing this information, what we really want to minimize is the sum of the squares of the residuals: \\(||r||_2^2\\). This is the same as the sum of the squares of the errors in the data.\nThere is an altogether informative way to think about the minimization problem purely in terms of linear algebra and subspaces to derive the same normal equations.\n\n\n\nLeast Squares Visual\n\n\nWe have the range of \\(A\\) or image of \\(A\\) as the subspace of \\(\\mathbb{R}^3\\) that is spanned by the columns of \\(A\\). This subspace is rank \\(2\\) because there are only two columns in \\(A\\), \\(R(A) \\subset \\mathbb{R}^3\\). The inaccessible parts of \\(\\mathbb{R}^3\\) are in the orthogonal complement of \\(R(A)\\), \\(R(A)^\\perp\\). Recalling that \\(R(A)^\\perp = N(A^T)\\) we can diagram the solution to least squares as a minimization of the error vector \\(r\\) in the orthogonal complement of \\(R(A)\\).\nAs seen the \\(r\\) vector is perpendicular to the \\(x_{LS}\\) solution, the projection of \\(r\\) onto \\(R(A)\\) is zero. Since it is in a null space of \\(A^T\\) then \\(A^T r = 0\\).\n\\[ \\begin{align} A^T \\left ( Ax_{LS} - b \\right ) &= 0\\\\\nA^T A x_{LS} &= A^T b \\\\\n\\end {align} \\]\nSo we recover the normal equations without using any of the machinery of calculus.\nFor a review on the four fundamental subspaces of a matrix see the UBC Math 307 notes on the topic: Math 307"
  },
  {
    "objectID": "content/CV.html",
    "href": "content/CV.html",
    "title": "CV",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: &lt;a href=\"www/cv/CV.pdf\"&gt;Download PDF&lt;/a&gt;."
  },
  {
    "objectID": "content/about/biography.html",
    "href": "content/about/biography.html",
    "title": "Bio",
    "section": "",
    "text": "I wasn’t always so academically focused. In fact, I had a ten year career in forestry where I planted over 2 million trees along with a variety of other projects. I also spent some years living in the Peruvian Amazon working as a travel guide and translator.\nI came back to study at UBC in 2020 to revisit my interest in science and technology, after an injury required me to change lifestyles. It has been a challenging but rewarding journey, and I am excited to see where it takes me next."
  },
  {
    "objectID": "content/about/biography.html#some-past-adventures",
    "href": "content/about/biography.html#some-past-adventures",
    "title": "Bio",
    "section": "Some Past Adventures",
    "text": "Some Past Adventures\n\n\n\n\nSta. Clautilde, Rio Napo, Peru\n\n\n\n\n\nTarapoto, San Martin, Peru\n\n\n\n\n\nRemote Helicopter Forestry Work\n\n\n\n\n\nEverest Base Camp\n\n\n\n\n\nDirtbiking in Myanmar\n\n\n\n\n\nSailing to Mexico from Victoria"
  },
  {
    "objectID": "blog/posts/scorematching/index.html",
    "href": "blog/posts/scorematching/index.html",
    "title": "Score Matching for Density Estimation",
    "section": "",
    "text": "The Problem of Density Estimation\nWhen working with a set of data, one of the tasks that we often want to do is to estimate the underlying probability density function (PDF) of the data. Knowing the probability distribution is a powerful tool that allows to make predictions, generate new samples, and understand the data better. For example, we may have a coin and want to know the probability of getting heads or tails. We can flip the coin many times and count the number of heads and tails to estimate the probability of each outcome. However, when it comes to higher dimensional spaces that are continuous in distribution, the problem of estimating the PDF in this way becomes intractable.\nFor example, with a set of small images such as the CIFAR-10 dataset, the images are 32x32 pixels with 3 color channels. The number of dimensions in the data space is 32x32x3 = 3072. With 8-bit images the number of all possible unique images is \\(255^{3072}\\), which is an incomprehensibly large number. The 60,000 images that are included in CIFAR-10 represent but a tiny fraction of samples in the space of all possible images.\n Figure 1: Sample images from the CIFAR-10 dataset (Krizhevsky 2009)\nTo demonstrate the issue with random image generation in such a sparsely populated space, we can generate a random 32x32 image with 3 color channels and display it.\nusing Plots, Images\n\n# Generate 6 random images and display them in a grid\nplots = []\nfor i in 1:6\n    # Generate a random 32x32 3-channel image\n    im = rand(3, 32, 32)\n    im = colorview(RGB, im)\n    p = plot(im, showaxis=false, showgrid=false, title=\"Random Image $i\")\n    push!(plots, p)\nend\n\n# Create a plot with a 2x3 grid layout\nplot_grid = plot(plots..., layout=(2, 3), size=(800, 400))\nYes we have successfuly generated random 32x32 color images, but they are not very useful or interesting.\nIf there were some way to learn the underlying distribution of the data, we could generate new samples that are realistic (probable) but that have never been seen before by sampling from higher probability regions of the learned distribution. So how do recent developments in machine learning manage to generate new and plausible samples from high dimensional data sets?\nOne of the techniques that has been developed is called generative modeling. Generative models are a class of machine learning models that are trained to learn the underlying distribution of the data. Once the model has learned the distribution, it can generate new samples that are similar to the training data.\nOne of the powerful techniques that allows for learning a probability distribution is score matching.\n\n\nParameter Estimation\nLet us take a moment to consider the problem of fitting a model to data in the most simple sense. Suppose that we have a set of data points and want to fit a linear model by drawing a line through it. One of the techniques that can be used is to minimize the sum of the squared errors between the data points and the line. This is known as the method of least squares.\nWe have model \\(f(x) = \\hat y = mx + b\\) with free parameters \\(\\theta = {m, b}\\) and data points \\((x_i, y_i)\\). The objective is to find the parameters \\(\\theta\\) that minimize the sum of the squared errors \\(J\\) between the predicted values \\(\\hat y_i\\) and the true values \\(y_i\\).: \\[  \\text{arg}\\min_{m,b} J(m,b) = \\text{arg}\\min_{m,b} \\sum_{i=1}^{n} (\\hat y_i - y_i)^2 \\]\nWe could use some calculus at this point to solve the minimization problem but more general matrix methods can be used to solve the problem.\n\\[\\begin{align*}\n    X &= \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix} , \\quad  \\vec{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} , \\quad \\vec{\\theta} &= \\begin{bmatrix} b \\\\ m \\end{bmatrix} \\\\\n    \\text{arg}\\min_{\\vec{\\theta}} J(\\vec{\\theta}) &= \\text{arg}\\min_{\\vec{\\theta}} ||\\vec{\\hat y} - \\vec{y}||^2\\\\\n    \\text{arg}\\min_{\\vec{\\theta}} J(\\vec{\\theta}) &= \\text{arg}\\min_{\\vec{\\theta}} ||X\\vec{\\theta} - \\vec{y}||^2\n\\end{align*}\\]\nThe solution to this problem is well known and can be found by solving the normal equations: \\[ X^T X \\vec{\\theta} = X^T \\vec{y} \\]\nAn example of this optimization problem is shown below where we generate some random data points and fit a line to them.\nusing Plots, Random\n\n# Generate some random data points with correlation along a line\nRandom.seed!(1234)\nn_points = 10\nx = rand(n_points)\nm_true = 0.8; b_true = -1\ny = .8* x .- 1 + 0.3 * randn(n_points)\n\n# Create the matrix X\nX = hcat(ones(n_points), x)\n\n# Solve the normal equations to find the parameters theta\ntheta = X \\ y\n\n# Generate x values for the complete line\nx_line = range(minimum(x) - 0.1, maximum(x) + 0.1, length=100)\nX_line = hcat(ones(length(x_line)), x_line)\n\n# Compute the y values for the line\ny_hat_line = X_line * theta\n\n# Compute the fitted values for the original data points\ny_hat = X * theta\n\n# Unpack learned parameters\nb, m = theta\n\n# Plot the data points and the fitted line\ntitle_text = \"Fitted Line: y = $(round(m, digits=2))x + $(round(b, digits=2)) vs. \\n True Line: y = $(m_true)x + $(b_true)\"\np1 = scatter(x, y, label=\"Data Points\", color=:red, ms=5, aspect_ratio=:equal, xlabel=\"x\", ylabel=\"y\", title=title_text)\nplot!(p1, x_line, y_hat_line, label=\"Fitted Line\", color=:blue)\n# Add dashed lines for residuals\nfor i in 1:n_points\n    plot!(p1, [x[i], x[i]], [y[i], y_hat[i]], color=:black, linestyle=:dash, label=\"\")\nend\n\ndisplay(p1)\n\nThis is a simple example of parameter estimation but it shows some of the important concepts that are used in more complex models. There is an underlying distribution which is a line with some error or noise added to it. We collected some random points from the distribution and then used them in an optimization problem where we minimized the squared error between the predicted values and the true values. The best solution is the parameters \\(\\theta\\) that minimize the error. In doing so we recovered a line that is close to the one that was used to generate the data.\n\n\nEstimating a Density Function\nWhen it comes to estimating denstity we are constrained by the fact that any model must sum to 1 of the entire sample space.\n\n\nDenoising Autoencoders\nDenoising Autoencoders (DAE) are a type of machine learning model that is trained to reconstruct the input data from a noisy or corrupted version of the input. The DAE is trained to take an sample such as an image with unwanted noise and restore it to the original sample.\nIn the process of learning the denoising parameters, the DAE also can learn the score function the underlying distribution of noisy samples, which is a kernel density estimate of the true distribution.\nThe score function is an operator defined as: \\[ s(f(x)) = \\nabla_x \\log f(x) \\]\nWhere \\(f(x)\\) is the density function or PDF of the distribution.\nBy learning a score function for a model, we can reverse the score operation to obtain the original density function it was derived from. This is the idea behind score matching, where we indirectly find the the pdf of a distribution by matching the score of a proposed model \\(p(x;\\theta)\\) to the score of the true distribution \\(q(x)\\).\nAnother benefit of learning the score function of a distribution is that it can be used to move from less probable regions of the distribution to more probable regions using gradient ascent. This is useful when it comes to generative models, where we want to generate new samples from the distribution that are more probable.\nHowever one of the challenges is that the score function is not always well-defined, especially in regions of low probability where there are sparse samples. This can make it difficult to learn the score function accurately in these regions.\nThis post explores some of those limitations and how increasing the bandwidth of the noise kernel in the DAE can help to stabilize the score function in regions of low probability.\n\n\nSample of Score Matching\nSuppose we have a distribution in 2D space that consists of three Gaussians as our ground truth. We can plot this pdf and its gradient field.\nusing Plots, Distributions\n\n# Define the ground truth distribution\nfunction p(x, y)\n    mu1, mu2, mu3 = [-1, -1], [1, 1], [1, -1]\n    sigma1, sigma2, sigma3 = [0.5 0.3; 0.3 0.5], [0.5 0.3; 0.3 0.5], [0.5 0; 0 0.5]\n\n    return 0.2 * pdf(MvNormal(mu1, sigma1), [x, y]) + 0.2 * pdf(MvNormal(mu2, sigma2), [x, y]) + 0.6 * pdf(MvNormal(mu3, sigma3), [x, y])\nend\n\n# Plot the distribution using a heatmap\nheatmap(\n    -3:0.01:3, -3:0.01:3, p,\n    c=cgrad(:davos, rev=true),\n    aspect_ratio=:equal,\n    xlabel=\"x\", ylabel=\"y\", title=\"Ground Truth PDF q(x)\",\n    xlims=(-3, 3), ylims=(-3, 3),\n    xticks=[-3, 3], yticks=[-3, 3]\n)\nSampling from the distribution can be done by generating 100 random points\nusing Random, Plots, Distributions\n\n# Define the ground truth distribution\nfunction p(x, y)\n    mu1, mu2, mu3 = [-1, -1], [1, 1], [1, -1]\n    sigma1, sigma2, sigma3 = [0.5 0.3; 0.3 0.5], [0.5 0.3; 0.3 0.5], [0.5 0; 0 0.5]\n\n    return 0.2 * pdf(MvNormal(mu1, sigma1), [x, y]) + 0.2 * pdf(MvNormal(mu2, sigma2), [x, y]) + 0.6 * pdf(MvNormal(mu3, sigma3), [x, y])\nend\n\n# Sample 200 points from the ground truth distribution\nn_points = 200\npoints = []\n\n# Set random seed for reproducibility\nRandom.seed!(1234)\n\nwhile length(points) &lt; n_points\n    x = rand() * 6 - 3\n    y = rand() * 6 - 3\n    if rand() &lt; p(x, y)\n        push!(points, (x, y))\n    end\nend\n\n# Plot the distribution using a heatmap\n# heatmap(\n#     -3:0.01:3, -3:0.01:3, p,\n#     c=cgrad(:davos, rev=true),\n#     aspect_ratio=:equal,\n#     xlabel=\"x\", ylabel=\"y\", title=\"Ground Truth PDF q(θ)\",\n\n# )\n\n# Scatter plot of the sampled points\nscatter([x for (x, y) in points], [y for (x, y) in points], label=\"Sampled Points\", color=:red, ms=2,\n     xlims=(-3, 3), ylims=(-3, 3),\n     xticks=[-3, 3], yticks=[-3, 3])\nFrom this sampling of points we can visualize the effect of the choice of noise bandwidth on the kernel density estimate.\nusing Plots, Distributions, ForwardDiff\n\n# Define the ground truth distribution\nfunction p(x, y)\n    mu1, mu2, mu3 = [-1, -1], [1, 1], [1, -1]\n    sigma1, sigma2, sigma3 = [0.5 0.3; 0.3 0.5], [0.5 0.3; 0.3 0.5], [0.5 0; 0 0.5]\n\n    return 0.2 * pdf(MvNormal(mu1, sigma1), [x, y]) + 0.2 * pdf(MvNormal(mu2, sigma2), [x, y]) + 0.6 * pdf(MvNormal(mu3, sigma3), [x, y])\nend\n\n# Define the log of the distribution\nfunction log_p(x, y)\n    val = p(x, y)\n    return val &gt; 0 ? log(val) : -Inf\nend\n\n# Function to compute the gradient using ForwardDiff\nfunction gradient_log_p(u, v)\n    grad = ForwardDiff.gradient(x -&gt; log_p(x[1], x[2]), [u, v])\n    return grad[1], grad[2]\nend\n\n# Generate a grid of points\nxs = -3:0.5:3\nys = -3:0.5:3\n\n# Create meshgrid manually\nxxs = [x for x in xs, y in ys]\nyys = [y for x in xs, y in ys]\n\n# Compute the gradients at each point\nU = []\nV = []\nfor x in xs\n    for y in ys\n        u, v = gradient_log_p(x, y)\n\n        push!(U, u)\n        push!(V, v)\n    end\nend\n\n# Convert U and V to arrays\nU = reshape(U, length(xs), length(ys))\nV = reshape(V, length(xs), length(ys))\n\n# Plot the distribution using a heatmap\nheatmap(\n    -3:0.01:3, -3:0.01:3, p,\n    c=cgrad(:davos, rev=true),\n    aspect_ratio=:equal,\n    xlabel=\"x\", ylabel=\"y\", title=\"Ground Truth PDF q(x) with score\",\n    xlims=(-3, 3), ylims=(-3, 3),\n    xticks=[-3, 3], yticks=[-3, 3]\n)\n\n# Flatten the gradients and positions for quiver plot\nxxs_flat = [x for x in xs for y in ys]\nyys_flat = [y for x in xs for y in ys]\n\n# Plot the vector field\nquiver!(xxs_flat, yys_flat, quiver=(vec(U)/20, vec(V)/20), color=:green, quiverkeyscale=0.5)\nNow we apply a Gaussian kernel to the sample points to create the kernel density estimate:\nusing Plots, Distributions, KernelDensity\n\n# Convert points to x and y vectors\nx_points = [x for (x, y) in points]\ny_points = [y for (x, y) in points]\n\n# Perform kernel density estimation using KernelDensity.jl\nparzen = kde((y_points, x_points); boundary=((-3,3),(-3,3)), bandwidth = (.3,.3))\n\n# Plot the ground truth PDF\np1 = heatmap(\n    -3:0.01:3, -3:0.01:3, p,\n    c=cgrad(:davos, rev=true),\n    aspect_ratio=:equal,\n    xlabel=\"x\", ylabel=\"y\", title=\"Ground Truth PDF q(x)\",\n    xlims=(-3, 3), ylims=(-3, 3),\n    xticks=[-3, 3], yticks=[-3, 3]\n)\n\n# Scatter plot of the sampled points on top of the ground truth PDF\nscatter!(p1, x_points, y_points, label=\"Sampled Points\", color=:red, ms=2)\n\n\n# Plot the kernel density estimate\np2 = heatmap(\n    parzen.x, parzen.y, parzen.density,\n    c=cgrad(:davos, rev=true),\n    aspect_ratio=:equal,\n    xlabel=\"x\", ylabel=\"y\", title=\"Kernel Density Estimate\",\n    xlims=(-3, 3), ylims=(-3, 3),\n    xticks=[-3, 3], yticks=[-3, 3]\n)\n\n# Scatter plot of the sampled points on top of the kernel density estimate\nscatter!(p2, x_points,  y_points, label=\"Sampled Points\", color=:red, ms=2)\n\n# Arrange the plots side by side\nplot(p1, p2, layout = @layout([a b]), size=(800, 400))\n\nNow looking at the density estimate across many bandwidths, we can see the effect on adding more and more noise to the original sampled points and our density estimate that we are learning. At very large bandwidths the estimate becomes a uniform distribution.\nusing Plots, Distributions, KernelDensity\n# Define the range of bandwidths for the animation\nbandwidths = [(0.01 + 0.05 * i, 0.01 + 0.05 * i) for i in 0:40]\n\n# Create the animation\nanim = @animate for bw in bandwidths\n    kde_result = kde((x_points,y_points); boundary=((-6, 6), (-6, 6)), bandwidth=bw)\n\n    p2 = heatmap(\n        kde_result.x, kde_result.y, kde_result.density',\n        c=cgrad(:davos, rev=true),\n        aspect_ratio=:equal,\n        xlabel=\"x\", ylabel=\"y\", title=\"Kernel Density Estimate,Bandwidth = $(round(bw[1],digits=2))\",\n        xlims=(-6, 6), ylims=(-6, 6),\n        xticks=[-6, 6], yticks=[-6, 6]\n    )\n\n    scatter!(p2, x_points, y_points, label=\"Sampled Points\", color=:red, ms=2)\nend\n\n# Save the animation as a GIF\ngif(anim, \"parzen_density_animation_with_gradients.gif\", fps=2,show_msg = false)\nNow we can compute the score of the kernel density estimate to see how it changes with the bandwidth. The score function of the distribution is numerically unstable at regions of sparse data. Recalling that the score is the gradient of the log-density funtion, when the density is very low the function approaches negative infinity. Within the limits of numerical precision, taking the log of the density function will result in a negative infinity in sparse and low probability regions. Higher bandwidths of KDE using the Gaussian kernel for example, spread out both the discrete sampling and the true distribution over space. This extends the region of numerical stability for a higher bandwidth.\nThe regions with poor numerical stability can be seen as noise artifacts and missing data in the partial derivatives of the log-density function. Some of these artifacts may also propogate from the fourier transform calculations that the kernel density estimate uses.\nusing Plots, Distributions, KernelDensity, ForwardDiff\n\n# Define the range of bandwidths for the animation\nbandwidths = [(0.01 + 0.05 * i, 0.01 + 0.05 * i) for i in 0:30]\n\nboundary = (-10, 10)\n# Create the animation\nanim = @animate for bw in bandwidths\n    kde_result = kde((x_points, y_points); boundary=(boundary, boundary), bandwidth=bw)\n\n        # Compute log-density\n    log_density = log.(kde_result.density)\n\n    # Compute gradients of log-density\n    grad_x = zeros(size(log_density))\n    grad_y = zeros(size(log_density))\n\n    # Compute gradients using finite difference centered difference\n    for i in 2:size(log_density, 1)-1\n        for j in 2:size(log_density, 2)-1\n            grad_x[i, j] = (log_density[i+1, j] - log_density[i-1, j]) / (kde_result.x[i+1] - kde_result.x[i-1])\n            grad_y[i, j] = (log_density[i, j+1] - log_density[i, j-1]) / (kde_result.y[j+1] - kde_result.y[j-1])\n        end\n    end\n    # Downsample the gradients and coordinates by selecting every 10th point\n    downsample_indices_x = 1:10:size(grad_x, 1)\n    downsample_indices_y = 1:10:size(grad_y, 2)\n\n    grad_x_downsampled = grad_x[downsample_indices_x, downsample_indices_y]\n    grad_y_downsampled = grad_y[downsample_indices_x, downsample_indices_y]\n\n    x_downsampled = kde_result.x[downsample_indices_x]\n    y_downsampled = kde_result.y[downsample_indices_y]\n\n    xxs_flat = repeat(x_downsampled, inner=[length(y_downsampled)])\n    yys_flat = repeat(y_downsampled, outer=[length(x_downsampled)])\n\n    grad_x_flat = grad_x_downsampled[:]\n    grad_y_flat = grad_y_downsampled[:]\n\n    # Plot heatmaps of the gradients\n    p1 = heatmap(\n        kde_result.x, kde_result.y, grad_x',\n        c=cgrad(:davos, rev=true),\n        aspect_ratio=:equal,\n        xlabel=\"x\", ylabel=\"y\", title=\"Partial Derivative of Log-Density wrt x \\n Bandwidth = $(round(bw[1],digits=2))\",\n        xlims=boundary, ylims=boundary\n    )\n\n    # Overlay the scatter plot of the sampled points\n    scatter!(p1, x_points, y_points, label=\"Sampled Points\", color=:red, ms=2)\n\n    p2 = heatmap(\n        kde_result.x, kde_result.y, grad_y',\n        c=cgrad(:davos, rev=true),\n        aspect_ratio=:equal,\n        xlabel=\"x\", ylabel=\"y\", title=\"Partial Derivative of Log-Density wrt y \\n Bandwidth = $(round(bw[1],digits=2))\",\n        xlims=boundary, ylims=boundary\n    )\n\n    # Overlay the scatter plot of the sampled points\n    scatter!(p2, x_points, y_points, label=\"Sampled Points\", color=:red, ms=2)\n\n    plot(p1, p2, layout = @layout([a b]), size=(800, 400))\nend\n# Save the animation as a GIF\ngif(anim, \"parzen_density_partials.gif\", fps=2, show_msg=false)\nAnd combining the gradient overtop of the ground truth distribution that is modeled with the kernel density estimate, starting with the larger bandwidths and moving to the smaller bandwidths, we can see that the region of numerical stability is extended with the larger bandwidths. The larger bandwidths also remove some of the precision in the model, with larger bandwidths the model approaches a single gaussian distribution.\n# Define the range of bandwidths for the animation\nbandwidths = [(0.01 + 0.2 * i, 0.01 + 0.2 * i) for i in 0:10]\nbandwidths = reverse(bandwidths)\n\nboundary = (-10, 10)\n# Create the animation\nanim = @animate for bw in bandwidths\n    kde_result = kde((x_points, y_points); boundary=(boundary, boundary), bandwidth=bw)\n\n    # Compute log-density\n    log_density = log.(kde_result.density)\n\n    # Compute gradients of log-density\n    grad_x = zeros(size(log_density))\n    grad_y = zeros(size(log_density))\n\n    # Compute gradients using finite difference centered difference\n    for i in 2:size(log_density, 1)-1\n        for j in 2:size(log_density, 2)-1\n            grad_x[i, j] = (log_density[i+1, j] - log_density[i-1, j]) / (kde_result.x[i+1] - kde_result.x[i-1])\n            grad_y[i, j] = (log_density[i, j+1] - log_density[i, j-1]) / (kde_result.y[j+1] - kde_result.y[j-1])\n        end\n    end\n    # Downsample the gradients and coordinates by selecting every 10th point\n    downsample_indices_x = 1:20:size(grad_x, 1)\n    downsample_indices_y = 1:20:size(grad_y, 2)\n\n    grad_x_downsampled = grad_x[downsample_indices_x, downsample_indices_y]\n    grad_y_downsampled = grad_y[downsample_indices_x, downsample_indices_y]\n\n    x_downsampled = kde_result.x[downsample_indices_x]\n    y_downsampled = kde_result.y[downsample_indices_y]\n\n    xxs_flat = repeat(x_downsampled, inner=[length(y_downsampled)])\n    yys_flat = repeat(y_downsampled, outer=[length(x_downsampled)])\n\n    grad_x_flat = grad_x_downsampled[:]\n    grad_y_flat = grad_y_downsampled[:]\n\n     # Plot the actual distribution\n    x_range = boundary[1]:0.01:boundary[2]\n    y_range = boundary[1]:0.01:boundary[2]\n    p1 = heatmap(\n        x_range, y_range, p,\n        c=cgrad(:davos, rev=true),\n        aspect_ratio=:equal,\n        xlabel=\"x\", ylabel=\"y\", title=\"Ground Truth PDF q(x)\\n with score of Kernel Density Estimate, \\n Bandwidth = $(round(bw[1],digits=2))\",\n        xlims=boundary, ylims=boundary,\n        size=(800, 800)\n    )\n\n    # Plot a quiver plot of the downsampled gradients\n    quiver!(yys_flat, xxs_flat, quiver=(grad_x_flat/10, grad_y_flat/10), \n    color=:green, quiverkeyscale=0.5, aspect_ratio=:equal)\nend\n# Save the animation as a GIF\ngif(anim, \"parzen_density_gradient_animation_with_gradients.gif\", fps=2, show_msg=false)\n\n\n\n\n\nReferences\n\nKrizhevsky, Alex. 2009. “Learning Multiple Layers of Features from Tiny Images.” https://www.cs.toronto.edu/~kriz/cifar.html."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Posts",
    "section": "",
    "text": "Score Matching for Density Estimation\n\n\nEstimation of the probability density function using score matching\n\n\nScore matching is a method for indirectly estimating the probability density function of a distribution. In this post, I will explain the score matching method as well as some of its limitations.\n\n\n\n\n\nJun 22, 2024\n\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome\n\n\nA first post using Quarto\n\n\nFind out more about the tools I’m using to create this blog.\n\n\n\n\n\nApr 22, 2024\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/welcome/index.html",
    "href": "blog/posts/welcome/index.html",
    "title": "Welcome",
    "section": "",
    "text": "This year, I have been expanding my knowledge of publishing and coding techniques that are designed to make sharing technical work easier, more visual, and more interactive. Michael Friedlander, who teaches CPSC 406 Computational Optimization at UBC, is an advocate for using Julia and Quarto in teaching and research. Drawing inspiration from his work along with Patrick Altmeyer’s website, I have decided to start my own blog using Quarto.\nQuarto is a scientific and technical publishing system built on Pandoc. It is designed to make it easy to write and publish technical content, such as research papers, books, and reports. One of its main features is that it allows for writing content in markdown along with code chunks in Julia, Python, R, and other languages. In addition, Quarto supports a wide range of output formats, including HTML, PDF, and Word. It has the great convenience of being able to port writing from Obsidian or in Latex to a blog post or html with minimal effort.\nI’m excited to use this new tool to share my work and ideas, especially as I continue to learn more about data science, machine learning, and optimization. I hope you find the content here useful and/or interesting."
  },
  {
    "objectID": "blog/posts/welcome/index.html#examples-of-julia-code-and-plots",
    "href": "blog/posts/welcome/index.html#examples-of-julia-code-and-plots",
    "title": "Welcome",
    "section": "Examples of Julia Code and Plots",
    "text": "Examples of Julia Code and Plots\nHere’s a parametrically defined, snail-like surface. Although it exists in 3D space, the surface is two-dimensional in that any location on it can be specified using just two coordinates—similar to how we navigate the surface of the Earth. You van see this incorporated as the two parameters \\(u\\) and \\(v\\) in the code below. These two coordinates map into 3D space that is defined by the functions \\(s1\\), \\(s2\\), and \\(s3\\) giving a vector \\[\\mathbf{s}(u,v) = \\begin{bmatrix}s1(u,v) \\\\ s2(u,v) \\\\ s3(u,v)\\end{bmatrix}\\]\nThe surface is then plotted using the surface function from the Julia Plots package.\nNote the usage of the vectorized operation of the functions \\(s1\\), \\(s2\\), and \\(s3\\) to create the vectors xs, ys, and zs. The passing of the input vectors u and v' creates the required meshgrid for the surface plot.\n#| label: fig-snail\n#| fig-cap: \"Surface Plot Example\"\n\nusing Plots\n\n# Your plotting code here\nu = range(0, stop=6π, length=100)\nv = range(0, stop=2π, length=30)\ns1(u, v) = 2 * (1 - exp(u / (6 * π))) * cos(u) * cos(v / 2)^2\ns2(u, v) = 2 * (-1 + exp(u / (6 * π))) * sin(u) * cos(v / 2)^2\ns3(u, v) = 1 - 0.71 * exp(u / (3 * π)) - sin(v) + exp(u / (6 * π)) * sin(v)\n\nxs, ys, zs = s1.(u, v'), s2.(u, v'), s3.(u, v')\nsurface(xs, ys, zs, color=cgrad(:acton), alpha=0.5, legend=false)\nThis code is an example of the animation features included in the Julia Plot library found at Julia Plots Package that can be used to create a gif. The gif below shows a parametric plot of a heart. Note just how compact the code is for creating this gif and the natural expression that the code has. This is the power of Julia.\n#| label: fig-heart\n#| fig-cap: \"Heart Animation Example\"\n\nusing Plots\n\n@userplot CirclePlot\n@recipe function f(cp::CirclePlot)\n    x, y, i = cp.args\n    n = length(x)\n    inds = circshift(1:n, 1 - i)\n    linewidth --&gt; range(0, 10, length = n)\n    seriesalpha --&gt; range(0, 1, length = n)\n    aspect_ratio --&gt; 1\n    label --&gt; false\n    x[inds], y[inds]\nend\n\nn = 400\nt = range(0, 2π, length = n)\nx = 16sin.(t).^3\ny = 13cos.(t) .- 5cos.(2t) .- 2cos.(3t) .- cos.(4t)\n\nanim = @animate for i ∈ 1:n\n    circleplot(x, y, i, line_z = 1:n, cbar = false, c = :reds, framestyle = :none)\nend every 5\ngif(anim, \"anim_fps15.gif\", fps = 15, show_msg = false)"
  },
  {
    "objectID": "content/about/overview.html",
    "href": "content/about/overview.html",
    "title": "Simon Ghyselincks",
    "section": "",
    "text": "Welcome to my personal site! I’m Simon Ghyselincks, currently a 5th-year Engineering Physics student at the University of British Columbia (UBC), with a minor in Computer Science. I am studying a cross-disciplinary blend of engineering, computer science, and applied mathematics. What I really love is coding to solve tough problems in robotics, machine learning, signal processing, and more."
  },
  {
    "objectID": "content/about/overview.html#welcome",
    "href": "content/about/overview.html#welcome",
    "title": "Simon Ghyselincks",
    "section": "",
    "text": "Welcome to my personal site! I’m Simon Ghyselincks, currently a 5th-year Engineering Physics student at the University of British Columbia (UBC), with a minor in Computer Science. I am studying a cross-disciplinary blend of engineering, computer science, and applied mathematics. What I really love is coding to solve tough problems in robotics, machine learning, signal processing, and more."
  },
  {
    "objectID": "content/about/overview.html#academics-and-projects",
    "href": "content/about/overview.html#academics-and-projects",
    "title": "Simon Ghyselincks",
    "section": "Academics and Projects",
    "text": "Academics and Projects\nI am currently working with Eldad Haber at UBC Earth and Ocean Sciences on generative AI for geophysical applications. Our work explores the application of recent advances in normalizing flows with stochastic interpolants to generate 3d models of the earth’s crust. I am also continuing to develop our Engineering Physics capstone project “Learning to Balance” which explores the application of reinforcement learning to a reaction wheel robot with complex dynamics. Read more about my projects here.\n\nFeel free to connect with me on LinkedIn or check out my GitHub."
  },
  {
    "objectID": "content/about/overview.html#my-journey",
    "href": "content/about/overview.html#my-journey",
    "title": "Simon Ghyselincks",
    "section": "My Journey",
    "text": "My Journey\nRead more about my journey and past pursuits here."
  },
  {
    "objectID": "content/eosc555/index.html",
    "href": "content/eosc555/index.html",
    "title": "EOSC 555B: Nonlinear Inverse Theory",
    "section": "",
    "text": "Lecture notes for EOSC 555B: Nonlinear Inverse Theory taken at the University of British Columbia. Portions of template code originate from the course instrcutor Prof. Eldad Haber, while I’ve added some of my own numerical experiments and examples explored as part of self-study on the topics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 5: Autodiff and Gauss Newton Optimization\n\n\nA look at some of the foundations of automatic differentiation and the Gauss-Newton optimization method.\n\n\nAutomatic differentiation is a powerful tool for solving optimization problems and for recovering the jacobian. It can be used to automate the process of Gauss-Newton optimization.\n\n\n\n\n\nSep 25, 2024\n\n\n18 min\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 4: Regularization and the Conjugate Gradient Methods\n\n\nA derivation of regularization techniques for least squares\n\n\nTikhonov regularization is a common technique used in inverse theory to stabilize ill-posed problems. In this lecture, we derive the Tikhonov regularization technique, we also have a look at a least squares solution that does not require the computation of the full SVD of the matrix \\(A\\), using the conjugate gradient method.\n\n\n\n\n\nSep 20, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 3: Image Denoising with Gradient Descent and Early Stopping\n\n\nA derivation of least squares gradient descent and ODE analysis\n\n\nIn continuation of Lecture 2, we now look at an alternative approach to image denoising using gradient descent and early stopping. We will derive the least squares gradient descent algorithm and analyze it as an ordinary differential equation.\n\n\n\n\n\nSep 17, 2024\n\n\n22 min\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 2: Image Denoising with SVD\n\n\nApplications of Least Squares and SVD\n\n\nImage denoising and deblurring are important techniques in signal processing and recovery. I this coding exercise, we will explore the application of least squares, SVD, and the pseudoinverse to denoise and deblur images.\n\n\n\n\n\nSep 15, 2024\n\n\n22 min\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 1: Introduction to Inverse Theory\n\n\nLeast Squares and the SVD\n\n\nInverse theory has broad applications across many scientific disciplines. This lecture introduces the concept of least squares and the singular value decomposition (SVD) as a foundation for understanding inverse theory. We then use these properties to analyse the stability and conditioning of linear systems for solving inverse problems using the pseudoinverse and ML techniques.\n\n\n\n\n\nSep 14, 2024\n\n\n11 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/eosc555/lectures/lecture2/index.html",
    "href": "content/eosc555/lectures/lecture2/index.html",
    "title": "Lecture 2: Image Denoising with SVD",
    "section": "",
    "text": "The motivation for the exercise comes from a real world problem. The Hubble space telescope when launched had a defect in its mirror. This defect caused the images to be blurred. The problem was initially addressed by using signal processing techniques to remove the aberrations from the images.\n\n\nFor such an image processing problem, we can consider the continuous incoming light as striking a 2D mirror that distorts the light, followed by a 2D sensor that captures the light. In this context we suppose that we have a noise kernel or a point spread function (PSF) that describes the distortion of the light at the mirror. The point spread function, being a convolution kernel, behaves as a Green’s function for the system in the continuous case:\n\\[ \\vec{b}(x,y) = \\int_{\\mathcal{X}} \\int_{\\mathcal{Y}} \\vec{G}(x - x', y - y') \\vec{u}(x',y') \\, dx' dy' \\]\nwhere \\(\\vec{b}(x,y)\\) is the blurred image data that is recovered at the sensor, \\(\\vec{u}(x',y')\\) is the true image data, and \\(\\vec{G}(x,y)\\) is the point spread function.\nIn the special case that the point spread function is \\(\\delta(x-x',y-y')\\), then the image data is not distorted and the sensor captures the true image data. However our experiment is to consider cases where there could be even severe distortions and see how this impacts the proposition of recovering the true image data, \\(\\vec{u}(x',y')\\) from our sensor data, \\(\\vec{b}(x,y)\\).\n\n\nThe discrete analog of the continuous PSF can be more conveniently treated with we essentially flatten the the 2D mesh into a 1D vector, a common operation for signal processing. The unflattened case we have:\n\\[ b_{ij} = \\sum_{k=1}^{n} \\sum_{l=1}^{m} \\Delta x \\Delta y G(x_i - x_k, y_j - y_l) u_{kl} \\]\nwhere \\(b\\) is the blurred image data at the sensor, \\(u\\) is the true image data, and \\(G\\) is the discrete point spread function. If we flatten the 2D mesh into a 1D vector we can represent this as a 1D convolution operation: \\[ \\vec{b} = \\vec{G} * \\vec{u} \\]\nSince this is a convolution operation, we can process it much more quickly by leveraging the convolution theorem.\n\\[\\begin{align}\n\\mathcal{F}(\\vec{b}) &= \\mathcal{F}(\\vec{G} * \\vec{u}) \\\\\n\\mathcal{F}(\\vec{b}) &= \\mathcal{F}(\\vec{G}) \\mathcal{F}(\\vec{u}) \\\\\n\\vec{b} &= \\mathcal{F}^{-1}(\\mathcal{F}(\\vec{G}) \\odot \\mathcal{F}(\\vec{u}))\n\\end{align}\n\\]\nThe \\(\\odot\\) hadamard product is element-wise multiplication, the discrete analog of multiplication of two functions except over an array.\n\n\n\n\nIf we flatten the data down into a 1D vector then it is possible to construct a matrix operator that performs the convolution. This is a Toeplitz matrix, a matrix where each descending diagonal from left to right is constant, so that the row vectors represent a sliding window of the convolution kernel. We can flatten out the PSF and construct the matrix using it as the first row entry and then shifting the PSF to the right to fill out the rest of the rows."
  },
  {
    "objectID": "content/eosc555/lectures/lecture2/index.html#least-squares-recovery-with-svd-and-pseudoinverse",
    "href": "content/eosc555/lectures/lecture2/index.html#least-squares-recovery-with-svd-and-pseudoinverse",
    "title": "Lecture 2: Image Denoising with SVD",
    "section": "Least Squares Recovery with SVD and Pseudoinverse",
    "text": "Least Squares Recovery with SVD and Pseudoinverse\nNow that we have a matrix operator recovered we can formulate the forward problem as \\(A\\mathbf{x} = \\mathbf{b}\\) with our known \\(A\\) and \\(\\mathbf{b}\\), and we want to recover \\(\\mathbf{x}\\). To do this we use the SVD decomposition to gather the pseudo inverse. We can decide to filter out some of the singular values that are very small to improve the conditioning on the matrix as well, using a cutoff value for example.\n\nSVD Decomposition\n\n\nShow the code\nU, S, V = torch.svd(Amat.to(torch.float64))\nb = Amv(x)\n\n\nNow we make a log plot of the singular values to see how they decay, noting that we lose numerical precision around the \\(10^{-6}\\) mark. We can also asses what the frobenius norm of the difference between the original matrix and the reconstructed matrix is to get a sense of the error in the decomposition and reconstruction.\n\n\nShow the code\nplt.semilogy(S)\nplt.xlabel('Singular Value Index')\nplt.ylabel('Singular Value')\n\nloss = F.mse_loss(Amat, U @ torch.diag(S) @ V.T)\nprint(f\"The loss is {loss}\")\n\n\nThe loss is 1.5941001880912316e-23\n\n\n\n\n\nSVD Decomposition of the Convolution Matrix.\n\n\n\n\nThe loss is quite small which is a good sign that the decomposition is working well within the numerical precision of the machine.\n\n\nInitial Attempt at Pseudoinverse\nTo recover the original image data we first naively try to invert the matrix to see what happens.\n\n\nShow the code\nxhat = torch.linalg.solve(Amat,b.reshape(dim**2))\nplt.subplot(1,2,1)\nplt.imshow(xhat.reshape(x.shape[-2:]))\nplt.title('Naive Inverse')\nplt.subplot(1,2,2)\nplt.imshow(x.reshape(x.shape[-2:]))\nplt.title('Original Image');\n\n\n\n\n\nNaive Pseudoinverse Recovery of the Original Image.\n\n\n\n\nWow, not even close! This is because the matrix is so ill conditioned that it is effectively low rank and not invertible. We can improve the situation by filtering out the singular values that are very small.\n\n\nPseudoinverse with Filtering\nWe can filter out the poor conditioning singular values and exclude those values from the inversion. To get an idea of what the values are doing, we can plot the first few singular values and the corresponding singular vector that they project onto. In the case of the SVD the most important information about the matrix is captured in the left-most vectors of the matrix \\(U\\).\n\n\nShow the code\nn= 5\nfor i in range(n):\n  plt.subplot(1,n,i+1)\n  plt.imshow(U[:,i+1].reshape(x.shape[-2:]))\n  plt.title(f'Mode {i}')\n\n\n\n\n\n\n\n\n\nFor the inverse problem, the most import singular values are conversely found in the left-most vectors of the matrix \\(V\\). We can also check what the right-most vectors are doing, as they will blow up in value when inverting small singular values. They are high frequency modes of the image, creating the reconstruction issues when they are subjected to error in numerical precision.\n\n\nShow the code\nn= 5\nfor i in range(n):\n  plt.subplot(1,n,i+1)\n  plt.imshow(V[:,i+1].reshape(x.shape[-2:]))\n  plt.title(f'Mode {i}')\nplt.show()\n\nfor i in range(n):\n  plt.subplot(1,n,i+1)\n  plt.imshow(V[:,-(i+1)].reshape(x.shape[-2:]))\n  plt.title(f'Mode {V.shape[1]-i}')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese modes are the most important ones, as they contain the big-picture detail without the high frequency noise. We can now filter out the singular values that are very small and invert the matrix to recover the original image.\n\n\nShow the code\nb_flat = b.flatten().to(torch.float64)\nx_flat = x.flatten().to(torch.float64)\nthresholds = [1e-1, 1e-3, 1e-6, 1e-7, 1e-8, 1e-10]\n\nplt.figure(figsize=(7,5))  # Adjust the figure size as needed\n\nfor idx, threshold in enumerate(thresholds):\n    # Filter the singular values\n    S_filtered = S.clone()\n    S_filtered[S_filtered &lt; threshold] = 0\n\n    # Compute the reciprocal of the filtered singular values\n    S_inv = torch.zeros_like(S_filtered)\n    non_zero_mask = S_filtered &gt; 0\n    S_inv[non_zero_mask] = 1 / S_filtered[non_zero_mask]\n\n    # Construct the pseudoinverse of Amat\n    A_pinv = V @ torch.diag(S_inv) @ U.T\n\n    # Reconstruct the original image\n    xhat = A_pinv @ b_flat\n\n    # Compute the reconstruction error\n    error = torch.norm(xhat - x_flat, p='fro').item()\n\n    # Plot the reconstructed image in the appropriate subplot\n    plt.subplot(2, 3, idx + 1)  # idx + 1 because subplot indices start at 1\n    plt.imshow(xhat.reshape(x.shape[-2:]))\n    plt.title(f'Threshold {threshold}\\nError: {error:.4f}')\n    plt.colorbar()\n    plt.axis('off')  # Optionally turn off axis ticks and labels\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nPseudoinverse Recovery of the Original Image with Filtering.\n\n\n\n\nLooking at the results, around the \\(10^{-7}\\) mark we start to a peak level of recovery, as measured by the error in the Frobenius norm of the reconstruction. But what happens when we add noise to the data signal?\n\n\nAdding Noise to the Signal\nNow we add some noise to the signal and try least squares again for the direct solution\n\n\nShow the code\nb_flat = b.flatten().to(torch.float64)\nx_flat = x.flatten().to(torch.float64)\nAmat = Amat.to(torch.float64)\n\nalpha = .01\nnoise = torch.randn_like(b_flat) * alpha\n\nH = Amat.T @ Amat + alpha**2 * torch.eye(Amat.shape[0])\nxhat = torch.linalg.solve(H, Amat.T @ (b_flat + noise))\n\nplt.subplot(1,2,1)\nplt.imshow(x[0,0])\nplt.title('Original Image')\nplt.subplot(1,2,2)\nplt.imshow(xhat.reshape(x.shape[-2:]))\nplt.title('Reconstructed Image');\n\n\n\n\n\nPseudoinverse Recovery of the Original Image with Noise.\n\n\n\n\nThe reconstruction is not very good, the noise has been amplifed all over the image. We can try the pseudoinverse method again with the noise added to the signal.\n\n\nShow the code\nAmat_noisy = Amat + alpha * torch.eye(Amat.shape[0])\nUn, Sn, Vn = torch.svd(Amat_noisy)\n\nthresholds = [.5, .1, .05, .03, .005, .001]\n\nplt.figure(figsize=(7,5))  # Adjust the figure size as needed\n\nfor idx, threshold in enumerate(thresholds):\n    # Filter the singular values\n    S_filtered = Sn.clone()\n    S_filtered[S_filtered &lt; threshold] = 0\n\n    # Compute the reciprocal of the filtered singular values\n    S_inv = torch.zeros_like(S_filtered)\n    non_zero_mask = S_filtered &gt; 0\n    S_inv[non_zero_mask] = 1 / S_filtered[non_zero_mask]\n\n    # Construct the pseudoinverse of Amat\n    A_pinv = Vn @ torch.diag(S_inv) @ Un.T\n\n    # Reconstruct the original image\n    xhat = A_pinv @ (b_flat + noise)\n\n    # Compute the reconstruction error\n    error = torch.norm(xhat - x_flat, p='fro').item()\n\n    # Plot the reconstructed image in the appropriate subplot\n    plt.subplot(2, 3, idx + 1)  # idx + 1 because subplot indices start at 1\n    plt.imshow(xhat.reshape(x.shape[-2:]))\n    plt.title(f'Threshold {threshold}\\nError: {error:.4f}')\n    plt.colorbar()\n    plt.axis('off')  # Optionally turn off axis ticks and labels\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nPseudoinverse Recovery of the Original Image with Noise.\n\n\n\n\nThe small addition of noise is quite significant in the recovery threshold for reconstruction. Using a higher threshold for the singular values becomes important when dealing with noise in the signal. Previously numerical precision was the main issue, but now the measurement noise is the main issue."
  },
  {
    "objectID": "content/eosc555/lectures/lecture4/index.html",
    "href": "content/eosc555/lectures/lecture4/index.html",
    "title": "Lecture 4: Regularization and the Conjugate Gradient Methods",
    "section": "",
    "text": "$$ \n$$"
  },
  {
    "objectID": "content/eosc555/lectures/lecture4/index.html#tikhnov-regularization",
    "href": "content/eosc555/lectures/lecture4/index.html#tikhnov-regularization",
    "title": "Lecture 4: Regularization and the Conjugate Gradient Methods",
    "section": "Tikhnov Regularization",
    "text": "Tikhnov Regularization\nWe have looked at the least squares formulation for solving inverse problems:\n\\[ \\min \\frac{1}{2} \\|A x - b\\|^2 \\]\nwhere \\(A \\in \\mathbb R^{m \\times n}\\) is a linear operator, \\(x \\in \\mathbb R^n\\) is the unknown model, and \\(b \\in \\mathbb R^m\\) is the data.\nThe least squares problem is often ill-posed, meaning that the solution is not unique or stable. If there are more unknowns than equations, such as the case when \\(n &gt; m\\), then the problem is underdetermined and there are infinitely many solutions.\nWe can return to unique solutions by adding a regularization term to the selection of the \\(x\\) that we want to minimize. The Tikhonov regularization technique adds a penalty term to the least squares problem:\n\\[ \\min \\frac{1}{2} \\|A x - b\\|^2 + \\frac{1}{2}  \\lambda \\|Lx\\|^2 \\]\nwhere \\(L \\in \\mathbb R^{n \\times n}\\) is a regularization matrix. The regularization matrix \\(L\\) is often chosen to be the identity matrix, but other choices are possible.\n\nUniqueness\nTo check the uniqueness of the solution, we can rewrite the problem as a quadratic form:\n\\[ \\min \\frac{1}{2} x^T A^T A x - b^T A x + \\frac{1}{2} \\lambda x^T L^T L x \\] \\[ = \\min \\frac{1}{2} x^T H x - b^T A x + \\frac{1}{2}\\|b\\|^2\\]\nwhere \\(H = A^T A + \\lambda L^T L\\) is the Hessian matrix which is symmetric and positive semi-definite by spectral theorem. If we choose an appropriate \\(\\lambda\\), then the Hessian matrix is positive definite and the problem is well-posed. In the case where \\(L=I\\), the Hessian becomes full rank for \\(\\lambda &gt; 0\\) and the problem is well-posed. The quality that \\(H \\succ 0\\) means that the matrix is invertible.\n\n\nSolution\nThe unique solution is given by by the first order optimatility condition:\n\\[ \\begin{align}\n(A^T A + \\lambda L^T L) \\mathbf{x}_{\\text{RLS}} - A^T b&= 0 \\\\\n\\mathbf{x}_{\\text{RLS}} &= (A^T A + \\lambda L^T L)^{-1} A^T b\n\\end{align}\n\\]\n\n\nSVD Decomposition\nThe solution can be written in terms of the singular value decomposition of \\(A\\), and with the assumption that \\(L=I\\):\n\\[ \\begin{align}\nA &= U \\Sigma V^T \\\\\nA^T A &= V \\Sigma^T \\Sigma V^T \\\\\n\\mathbf{x}_{\\text{RLS}} &= \\left( V \\Sigma^2 V^T + \\lambda I \\right)^{-1} V \\Sigma^T U^T b \\\\\n&= \\left( V \\Sigma^2 V^T + \\lambda I V V^T \\right)^{-1} V \\Sigma^T U^T b\\\\\n&= V \\left( \\Sigma^2 + \\lambda I \\right)^{-1} \\Sigma^T U^T b\\\\\n&= V \\mathbf{Diag}\\left( \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda} \\right) U^T b\\\\\n&= \\sum _i ^ n \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda} \\langle u_i, b \\rangle v_i\n\\end{align}\n\\]\nThis form is more readily comparable to some of the other methods that we have see so far, which are presented in the table below:"
  },
  {
    "objectID": "content/eosc555/lectures/lecture4/index.html#comparison-of-least-squares-methods",
    "href": "content/eosc555/lectures/lecture4/index.html#comparison-of-least-squares-methods",
    "title": "Lecture 4: Regularization and the Conjugate Gradient Methods",
    "section": "Comparison of Least Squares Methods",
    "text": "Comparison of Least Squares Methods\n\n\n\n\n\n\n\nMethod\nSolution\n\n\n\n\nTikhonov\n\\(x_{\\text{RLS}} = \\sum _i ^ n \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda} \\langle u_i, b \\rangle v_i\\)\n\n\nThresholded SVD\n\\(x_{\\text{TSVD}} = \\sum _i ^ n h(\\sigma_i) \\langle u_i, b \\rangle v_i\\)\n\n\nGradient Flow\n\\(x_{\\text{SDF}} = \\sum _i ^ n \\frac{\\exp(-\\sigma_i^2 t) - 1}{\\sigma_i} \\langle u_i, b \\rangle v_i\\)\n\n\n\nAs we can see all three methods have a similar form and offer some mechanism for controlling the noise induced by the small singular values of \\(A\\).\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef generate_ill_conditioned_matrix(m, n, condition_number):   \n    # Generate random orthogonal matrices U and V\n    U, _ = np.linalg.qr(np.random.randn(m, m))\n    V, _ = np.linalg.qr(np.random.randn(n, n))\n    \n    sigma = np.linspace(1, 1/condition_number, min(m, n))    \n    Sigma = np.diag(sigma)    \n    A = U @ Sigma @ V[:min(m, n), :]\n    \n    return A, sigma\n\n# Seed for reproducibility\nnp.random.seed(4)\nA, S = generate_ill_conditioned_matrix(8, 24, 1e3)\n\n# Create a vector b of size 5 with random values\nb = np.random.randn(8)\n\n# Compute the SVD of A\nU, S, Vt = np.linalg.svd(A, full_matrices=False)\nV = Vt.T\nU = U  # Already in proper shape\n\n# Number of singular values\nn = len(S)\n\n# Define parameters for each method\n# Gradient Flow\nt_values = np.linspace(0, 0.6, 100)\n\n# Tikhonov Regularization\nlambda_values = np.linspace(1e-4, 1, 100)\n\n# Thresholded SVD\nthreshold_values = np.linspace(0, max(S), 100)\n\n# Compute scaling factors for each method\n# Gradient Flow Scaling\ndef gradient_flow_scaling(sigma, t):\n    return (1 - np.exp(-sigma**2 * t)) / sigma\n\ngradient_scalings = np.array([gradient_flow_scaling(s, t_values) for s in S])\n\n# Tikhonov Scaling\ndef tikhonov_scaling(sigma, lambd):\n    return sigma / (sigma**2 + lambd)\n\ntikhonov_scalings = np.array([tikhonov_scaling(s, lambda_values) for s in S])\n\n# Thresholded SVD Scaling\ndef tsvd_scaling(sigma, threshold):\n    return np.where(sigma &gt;= threshold, 1/sigma, 0)\n\ntsvd_scalings = np.array([tsvd_scaling(s, threshold_values) for s in S])\n\n# Initialize the plot with 3 subplots\nfig, axes = plt.subplots(3, 1, figsize=(5, 15))\n\n# Define a color palette\npalette = sns.color_palette(\"husl\", n)\n\n# Plot Gradient Flow\nax = axes[0]\nfor i in range(n):\n    ax.plot(t_values, gradient_scalings[i], color=palette[i], linewidth=2, label=f'$1/\\sigma_{i}$' )\n    ax.axhline(y=1/S[i], color=palette[i], linestyle='--', linewidth=1)\nax.set_yscale('log')\nax.set_xlabel('Time (t)', fontsize=14)\nax.set_ylabel('Scaling Factor', fontsize=14)\nax.set_title('Gradient Flow', fontsize=16)\nax.legend()\nax.grid(True)\n\n# Plot Tikhonov Regularization\nax = axes[1]\nfor i in range(n):\n    ax.plot(lambda_values, tikhonov_scalings[i], color=palette[i], linewidth=2, label=f'$1/\\sigma_{i}$' )\n    ax.axhline(y=1/S[i], color=palette[i], linestyle='--', linewidth=1)\nax.set_yscale('log')\nax.set_xlabel('Regularization Parameter (λ)', fontsize=14)\nax.set_ylabel('Scaling Factor', fontsize=14)\nax.set_title('Tikhonov Regularization', fontsize=16)\nax.legend()\nax.grid(True)\n\n# Plot Thresholded SVD\nax = axes[2]\nfor i in range(n):\n    ax.plot(threshold_values, tsvd_scalings[i], color=palette[i], linewidth=2, label=f'$1/\\sigma_{i}$')\n    ax.axhline(y=1/S[i], color=palette[i], linestyle='--', linewidth=1)\nax.set_yscale('log')\nax.set_xlabel('Threshold (τ)', fontsize=14)\nax.set_ylabel('Scaling Factor', fontsize=14)\nax.set_title('Thresholded SVD', fontsize=16)\nax.legend()\nax.grid(True)\n\n# Adjust layout and add a legend\nplt.tight_layout()\nplt.show()\n\n\n&lt;&gt;:69: SyntaxWarning: invalid escape sequence '\\s'\n&lt;&gt;:81: SyntaxWarning: invalid escape sequence '\\s'\n&lt;&gt;:93: SyntaxWarning: invalid escape sequence '\\s'\n&lt;&gt;:69: SyntaxWarning: invalid escape sequence '\\s'\n&lt;&gt;:81: SyntaxWarning: invalid escape sequence '\\s'\n&lt;&gt;:93: SyntaxWarning: invalid escape sequence '\\s'\nC:\\Users\\sghys\\AppData\\Local\\Temp\\ipykernel_10828\\1786010090.py:69: SyntaxWarning: invalid escape sequence '\\s'\n  ax.plot(t_values, gradient_scalings[i], color=palette[i], linewidth=2, label=f'$1/\\sigma_{i}$' )\nC:\\Users\\sghys\\AppData\\Local\\Temp\\ipykernel_10828\\1786010090.py:81: SyntaxWarning: invalid escape sequence '\\s'\n  ax.plot(lambda_values, tikhonov_scalings[i], color=palette[i], linewidth=2, label=f'$1/\\sigma_{i}$' )\nC:\\Users\\sghys\\AppData\\Local\\Temp\\ipykernel_10828\\1786010090.py:93: SyntaxWarning: invalid escape sequence '\\s'\n  ax.plot(threshold_values, tsvd_scalings[i], color=palette[i], linewidth=2, label=f'$1/\\sigma_{i}$')\n\n\n\n\n\nEvolution of scaling factors for three different methods"
  },
  {
    "objectID": "content/eosc555/lectures/lecture4/index.html#solving-least-squares-using-conjugate-gradient",
    "href": "content/eosc555/lectures/lecture4/index.html#solving-least-squares-using-conjugate-gradient",
    "title": "Lecture 4: Regularization and the Conjugate Gradient Methods",
    "section": "Solving Least Squares Using Conjugate Gradient",
    "text": "Solving Least Squares Using Conjugate Gradient\nA detailed explanation of this method can be found at Wikipedia\n\nConjugate Vectors Definition\nA set of vectors \\(\\{ p_1, p_2, \\ldots, p_n \\}\\) is said to be conjugate with respect to a matrix \\(A\\) if:\n\\[\n\\langle p_i, A p_j \\rangle = 0 \\quad \\text{for all } i \\neq j\n\\]\nThis is a generalization of the concept of orthoganality to non-symmetric matrices.\nStandard Orthogonality: When $ A = I $ (the identity matrix), the definition reduces to the standard concept of orthogonality. For a symmetric \\(A\\) we also have an orthogonal decomposition of eigenvectors by spectral theorem.\n\nBack to the problem of least squares, we can express the solution $ x $ as a linear combination of conjugate vectors:\n\\[\nx = x_0 + \\sum_{i=1}^n \\alpha_i p_i\n\\]\nwhere:\n\n\\(x_0\\) is an initial guess (can be zero).\n\\(\\alpha_i\\) are scalar coefficients.\n\\(p_i\\) are conjugate vectors with respect to \\(A\\).\n\nTo recover the coefficients of \\(\\alpha_i\\) we can use a projection in the weighted space of \\(A\\):\n\\[ \\begin{align}\nA x_0 + \\sum_{i=1}^n \\alpha_i A p_i &= b\\\\\nr &= b - A x_0\\\\\n\\sum_{i=1}^n \\alpha_i A p_i &= r\\\\\n\\langle p_i, \\sum_{i=1}^n \\alpha_i A p_i \\rangle &= \\langle p_i, r \\rangle\\\\\n\\alpha_i \\langle p_i, A p_i \\rangle &= \\langle p_i, r \\rangle\\\\\n\\alpha_i &= \\frac{\\langle p_i, r \\rangle}{\\langle p_i, A p_i \\rangle}\n\\end{align}\n\\] In the case where \\(x_0\\) is zero, then this reduces to \\[ \\alpha_i = \\frac{\\langle p_i, b \\rangle}{\\langle p_i, A p_i \\rangle} \\]\n\n\nAlgorithm Steps\nInitialize:\n\n\\(x = x_0\\)\n\\(r_0 = b - A x_0\\)\n\\(p_0 = r_0\\)\n\nFor \\(i = 0,1, 2, \\ldots\\):\n\nCompute \\(\\alpha_i\\):\n\\[\n\\alpha_i = \\frac{\\langle r_i, r_i \\rangle}{\\langle p_i, A p_i \\rangle}\n\\]\nUpdate Solution \\(x\\):\n\\[\nx_{i+1} = x_{i} + \\alpha_i p_i\n\\]\nUpdate Residual \\(r\\):\n\\[\nr_{i+1} = r_{i} - \\alpha_i A p_i\n\\]\nCheck for Convergence:\n\nIf \\(\\| r_{i+1} \\|\\) is small enough, stop.\n\nCompute \\(\\beta_i\\):\n\\[\n\\beta_i = \\frac{\\langle r_{i+1}, r_{i+1}\\rangle}{\\langle r_i,r_i \\rangle}\n\\]\nUpdate Conjugate Direction \\(p_{i+1}\\):\n\\[\np_{i+1} = r_{i+1} + \\beta_i p_i\n\\]\n\n\nThe method can be seen better if we trace through the minimization problem for fixed \\(x\\) and with variable \\(\\alpha\\):\n\\[\n\\begin{align}\n& \\min \\frac{1}{2} \\|A (x+\\alpha p) - b\\|^2  \\\\\n&= \\frac{1}{2}r^T r + \\alpha \\langle r, A p \\rangle + \\frac{1}{2} \\alpha^2 \\langle p, A^T A p \\rangle \\\\\n0 &= \\langle r, A p \\rangle + \\alpha \\langle p, A^T A p \\rangle \\\\\n\\alpha &= -\\frac{\\langle r, A p \\rangle}{\\|A p\\|^2}\n\\end{align}\n\\]\nBut we can also trace this through using the expansion of lest squares and removing the \\(\\|b\\|^2\\) term:\n\\[\n\\begin{align}\n& \\min \\frac{1}{2} \\tilde x^T A x - \\tilde x^T b  \\\\\n&= \\frac{1}{2} \\left( x^T A x + 2 \\alpha x^T A p + \\alpha^2 p^T A p \\right) - x^T b - \\alpha p^T b\\\\\n0&= x^TAp + \\alpha p^T A p - p^T b \\\\\n\\alpha &= \\frac{p^T (Ax-b)}{p^T A p}\n\\end {align}\n\\]"
  },
  {
    "objectID": "content/projects/projects.html",
    "href": "content/projects/projects.html",
    "title": "Project Home",
    "section": "",
    "text": "This project focuses on the development and challenges of building and controlling a reaction wheel unicycle. Performed as part of a 2-year long capstone project in the UBC Engineering Physics program, sponsored by the Engineering Physics project lab.\nExplore the Learning to Balance Project",
    "crumbs": [
      "Home",
      "Projects",
      "Project Home"
    ]
  },
  {
    "objectID": "content/projects/projects.html#learning-to-balance-a-reaction-wheel-unicycle",
    "href": "content/projects/projects.html#learning-to-balance-a-reaction-wheel-unicycle",
    "title": "Project Home",
    "section": "",
    "text": "This project focuses on the development and challenges of building and controlling a reaction wheel unicycle. Performed as part of a 2-year long capstone project in the UBC Engineering Physics program, sponsored by the Engineering Physics project lab.\nExplore the Learning to Balance Project",
    "crumbs": [
      "Home",
      "Projects",
      "Project Home"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/introduction.html",
    "href": "content/projects/RLUnicycle/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Welcome to our self-balancing unicycle robot capstone project! We are a team of undergraduate UBC Engineering Physics students working on our final academic checkpoint as engineering students before being released into the wild. This project is directly sponsored by the UBC Engineering Physics Project Lab.\n\n\n\nTristan Lee, Julian Lapenna, Kyle Mackenzie, Jackson Fraser, and Simon Ghyselincks\n\n\n\n\nOur goal is to design and develop a self-balancing reaction wheel robot that can navigate autonomously and be used as a platform to compare traditional control methods with reinforcement learning. The spirit of the project is to explore some of the challenges in implementing advanced control strategies on a real-world system. This includes bridging the gap between simulated models and real applications, coordinating peripherals with low latency, and designing hardware for controllability. It also presents a great opportunity to apply some fundamental physics and engineering concepts in a hands-on challenge.\n\n\n\nOur work draws on previous advances made in robotics. Notably, the Max Planck Institute’s Wheelbot project has served as a significant source of inspiration, many of our design choices and control strategies are influenced by their work. We aim to build on their development with a more advanced control and motor system that can navigate autonomously and adapt to dynamic disturbances using reinforcement learning.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Introduction"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/introduction.html#project-overview",
    "href": "content/projects/RLUnicycle/introduction.html#project-overview",
    "title": "Introduction",
    "section": "",
    "text": "Welcome to our self-balancing unicycle robot capstone project! We are a team of undergraduate UBC Engineering Physics students working on our final academic checkpoint as engineering students before being released into the wild. This project is directly sponsored by the UBC Engineering Physics Project Lab.\n\n\n\nTristan Lee, Julian Lapenna, Kyle Mackenzie, Jackson Fraser, and Simon Ghyselincks\n\n\n\n\nOur goal is to design and develop a self-balancing reaction wheel robot that can navigate autonomously and be used as a platform to compare traditional control methods with reinforcement learning. The spirit of the project is to explore some of the challenges in implementing advanced control strategies on a real-world system. This includes bridging the gap between simulated models and real applications, coordinating peripherals with low latency, and designing hardware for controllability. It also presents a great opportunity to apply some fundamental physics and engineering concepts in a hands-on challenge.\n\n\n\nOur work draws on previous advances made in robotics. Notably, the Max Planck Institute’s Wheelbot project has served as a significant source of inspiration, many of our design choices and control strategies are influenced by their work. We aim to build on their development with a more advanced control and motor system that can navigate autonomously and adapt to dynamic disturbances using reinforcement learning.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Introduction"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/introduction.html#the-robot",
    "href": "content/projects/RLUnicycle/introduction.html#the-robot",
    "title": "Introduction",
    "section": "The Robot",
    "text": "The Robot\n\n\nThe robot is composed of two reaction wheels, a single drive wheel, a controller, and a battery, all mounted on a 3D printed PLA frame. It has a total height of 30cm and a weight of 1.25kg, incorporating a compact and efficient design intended to allow self-erection from a position resting on its resetting legs. The Jetson Nano acts as an autonomous controller that reads the sensors and reacts to the environment using the motors.\n\nMuch like a unicycle, it balances on one wheel, with side-to-side stability provided by the roll wheel and direction controlled by a yaw wheel. The mechanism of balancing and steering relies on a reaction torque produced by spinning the reaction wheels. When a motor applies torque to one of the flywheels, an equal and opposite torque acts on the robot’s body, with the net effect altering the angular motion of both the wheel and the robot. The unstable axes to control are roll and pitch where the robot will fall to the ground without any intervention.\n\n\n\n\nSide View Pitch Axis\n\n\n\n\n\nSide View Roll Axis",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Introduction"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/introduction.html#the-challenge",
    "href": "content/projects/RLUnicycle/introduction.html#the-challenge",
    "title": "Introduction",
    "section": "The Challenge",
    "text": "The Challenge\nRobotics often confronts complex dynamics that are difficult to model precisely. Traditional control systems, while reliable under predictable conditions, may falter with unexpected disturbances. This project explores how Reinforcement Learning can enable our unicycle robot to adapt through trial and error, improving its decision-making capabilities in a dynamic environment.\n\nPrototyping and Progress\nWe have initiated our project with a Reaction Wheel Inverted Pendulum (RWIP) model to understand and tackle the unstable roll axis dynamics. Our efforts so far have included the application of both a traditional PID controller and an RL controller, with the latter showing promising results in handling dynamic disturbances aggressively yet effectively. With the completion of a function 2-DOF underactuated model, we are now moving towards the development of a full-scale 3-axis robot prototype.\n\n\n\n\n3-Axis Partial Build\n\n\n\n\n\nWith Prototype\n\n\n\n\n\n\n\nRWIP Model",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Introduction"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/introduction.html#looking-ahead",
    "href": "content/projects/RLUnicycle/introduction.html#looking-ahead",
    "title": "Introduction",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nThe insights gained from the RWIP will guide the development of the full-scale robot, with the eventual integration of state-space models for sophisticated control strategies and enhanced point-to-point navigation.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Introduction"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/introduction.html#development-pages",
    "href": "content/projects/RLUnicycle/introduction.html#development-pages",
    "title": "Introduction",
    "section": "Development Pages",
    "text": "Development Pages\nExplore the detailed development of specific components of our project:\n\n\n\nComponent\nDescription\n\n\n\n\nReal-Time Kernel\nDive into how we handle real-time constraints on the Jetson Nano.\n\n\nTelemetry\nDiscover how our system communicates and processes real-time data.\n\n\nDynamics and Control\nLearn about the dynamic modeling and control of our robot prototype.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Introduction"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/telemetry/telemetry.html",
    "href": "content/projects/RLUnicycle/telemetry/telemetry.html",
    "title": "Telemetry and Database System",
    "section": "",
    "text": "This is a user summary document for our capstone telemetry and database server. It is intended to provide an overview of the different services that are in use for the data pipeline. The capstone project leverages the MING stack as shown in the overview diagram below.\nI recommend using a central server to manage all of these services through a Zerotier Virtual Network. This will allow you to access the services from anywhere via the internet without exposing the server to the public.\n\n\nThe Lenovo M900 series of refurbished tiny PCs are recommended as an affordable option that meets the compute needs for a server. The SSD of the device was set to dual boot into Linux Ubuntu 22.04 for the purposes of running a server.\nThe Raspberry Pi 4B 8GB with a an external SSD was tested as a configuration but the requirements are at the limits of the processing power of the device.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Telemetry and Database System"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/telemetry/telemetry.html#hardware-recommendation",
    "href": "content/projects/RLUnicycle/telemetry/telemetry.html#hardware-recommendation",
    "title": "Telemetry and Database System",
    "section": "",
    "text": "The Lenovo M900 series of refurbished tiny PCs are recommended as an affordable option that meets the compute needs for a server. The SSD of the device was set to dual boot into Linux Ubuntu 22.04 for the purposes of running a server.\nThe Raspberry Pi 4B 8GB with a an external SSD was tested as a configuration but the requirements are at the limits of the processing power of the device.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Telemetry and Database System"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/telemetry/telemetry.html#zerotier-virtual-network",
    "href": "content/projects/RLUnicycle/telemetry/telemetry.html#zerotier-virtual-network",
    "title": "Telemetry and Database System",
    "section": "2.1 ZeroTier Virtual Network",
    "text": "2.1 ZeroTier Virtual Network\nZerotier is a virtual network that allows for secure communication between devices over the internet. It is a VPN that allows for devices to be connected to a virtual network and communicate with each other as if they were on the same local network.\nTo setup a network you should first create a free account at https://my.zerotier.com/. Once you have an account you can create a network and add devices to it. The network ID is a 16 digit number that is used to identify the network.\n\n\n2.1.1 Zerotier Client\nThe Zerotier client is a software that is installed on the devices that you want to connect to the network. Each device intended for the network including the server should have the client installed. Once it is installed, enter the network ID from the Zerotier website and then approve the device to the network. You may wish to set static IP addresses, especially for the server. This can all be done through the Zerotier website.\nhttps://www.zerotier.com/download/",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Telemetry and Database System"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/telemetry/telemetry.html#mqtt-overview",
    "href": "content/projects/RLUnicycle/telemetry/telemetry.html#mqtt-overview",
    "title": "Telemetry and Database System",
    "section": "2.2 MQTT Overview",
    "text": "2.2 MQTT Overview\nThe robot an network is using the MQTT protocol to implement live telemetry. Topics are used on a subscriber publisher basis. All communication is routed through the Lenovo server that is acting as the broker. The Mosquitto MQTT server software is running on the Lenovo server which is the IP address used for routing messages. The default port for MQTT is 1883.\n# Define the MQTT settings\nbroker_address = \"172.22.1.1\" #Lenovo's IP address\nport = 1883\ntopic = \"robot/telemetry\"\nThe MQTT explorer offers comprehensive tools to explore available topics and more: https://mqtt-explorer.com/ This can be a very useful tool for debugging and exploring the MQTT network to check if messages are being sent and received.\nMQTT interfaces with Python, Node-Red, and Grafana to provide a comprehensive data pipeline. The MQTT broker is the central hub for all data that is being sent and received. The broker can be accessed by any device on the ZeroTier network that is subscribed to the topic.\n\n2.2.1 MQTT Summary\nMQTT is a lightweight messaging protocol that provides an efficient and cost-effective method of carrying out telemetry-based communication between devices. It’s especially popular in Internet of Things (IoT) applications due to its minimal bandwidth requirements and ease of implementation on hardware with limited processing capabilities.\n\n2.2.1.1 Key Features of MQTT:\n\nLightweight Protocol: Ideal for constrained devices and networks with limited bandwidth.\nPublish-Subscribe Model: Allows devices to publish messages to a topic and any client subscribed to that topic will receive the messages.\nReliable Message Delivery: Offers various levels of Quality of Service (QoS) to guarantee message delivery.\nMinimal Overhead: Adds only a small overhead to each message, ensuring efficient use of network resources.\nRetained Messages: Supports retaining the last message sent on a topic, making it available immediately to new subscribers.\nLast Will and Testament: Provides a means for a client to notify other clients about an abnormal disconnection.\n\n\n\n2.2.1.2 Application for our Robot\nThe Lenovo acts as a broker for all the data that is streaming out of the robot over a Wi-Fi connection to the internet. This offloads the databasing and broadcasting duty from the robot to the broker which can dedicate more resources to data management. The robot can publish data to a topic, which can be picked up by various subscribers such as the Lenovo’s Grafana server or other laptops, phones, etc that are connected to the broker and subscribed to the topic.\nMore Detailed System Diagram needed \nThe broker can duplicate the published data to many devices in real-time. Another hidden stream for the data is through Node-Red to InfluxDB where the aggregate data can be store more permanently for access to testing records at a later date. Additionally, an MQTT bridge is connected to the Pi’s Grafana server which offers an advanced dashboard service for viewing live telemetry. Note that databased telemetry can also be viewed through Grafana which is connected to InfluDB, but it is a seperate data stream from MQTT and should have a seperate dashboard.\n\n\n\n2.2.2 Publishing Messages\nA sample script for publishing messages with Paho-MQTT client Documentation to the Lenovo server/broker while connected to Zero-Tier is provided, see the mqtt.py file. The script allows for publishing a controller value or a cpu_usage data point.\nPrerequisites\n\nMQTT Broker Setup: Ensure that the MQTT broker, in this case, the Lenovo server, is up and running.\nNetwork Connection: Connect your device to the Zero-Tier network to ensure visibility and access to the broker.\nPython Environment: Make sure Python is installed on your device along with necessary libraries: paho-mqtt, json\nBroker Details: In the script, set the broker_address to the Lenovo server’s IP address and port to 1883 (default MQTT port).\nA client is formed with a ClientID that should be descriptive, we then connect to the MQTT broker A dictionary of values can be converted to JSON format using JSON dump, method. The broker is designed to work with either raw values or JSON The JSON values can be a collection of data types with a Key and Value\nThe publish method publishes the data to the desired stream where other clients can subscribe to the topic.\n\n\n\n2.2.3 Subscribing to messages:\n\nA Grafana server has been setup where some useful dashboards can be maintained. The IoT MQTT Panel for Android also offers some nice services including the ability to send messages.\nMore details on subscribing through the Paho-MQTT client can be found at https://www.emqx.com/en/blog/how-to-use-mqtt-in-python ### QoS The protocol allows for fast transmission of messages and a similar PUB/SUB model as ROS topics. Telemetry read data can be sent using QoS level 0 which only attempts a single delivery of the data and does not wait for confirmation of receipt, this is perfectly acceptable for fast streaming data that is not mission critical.\n\nFor mission critical commands such as parameter adjustments and messages to the robot, a QoS level of 2 can be tied to the message which ensures that it is delivered to the robot exactly once. This avoids duplicate commands or lost commands through the network. It is a slower communication protocol but it is not an issue for lower bandwidth messages that are orginating from control devices to the robot.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Telemetry and Database System"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/telemetry/telemetry.html#grafana-live-telemetry-and-database-dashboards",
    "href": "content/projects/RLUnicycle/telemetry/telemetry.html#grafana-live-telemetry-and-database-dashboards",
    "title": "Telemetry and Database System",
    "section": "2.3 Grafana Live Telemetry and Database Dashboards",
    "text": "2.3 Grafana Live Telemetry and Database Dashboards\nGrafana is a powerful open-source platform for creating dashboards and visualizing time-series data. It is particularly well-suited for monitoring and analyzing real-time data. Grafana supports a wide range of data sources and can be used to display both live and historical data in a variety of formats, including graphs, tables, and gauges. Think of it as graph nirvana.\nWhen it comes to viewing the telemetry data, a plugin can be installed to function as a bridge between the MQTT broker and the Grafana server. https://grafana.com/grafana/plugins/grafana-mqtt-datasource/\nTo setup Grafana, install the software on the Lenovo server first. The default port for Grafana is 3000. The program operates through a web browser and can be accessed by navigating to the IP address of the Lenovo server on port 3000.\nOnce you have logged in, you can add a data source by selecting MQTT from the list of available data sources if you have correctly installed the plugin. The panel will listen to all messages on a particular topic and display them in a graph or table format.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Telemetry and Database System"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/telemetry/telemetry.html#influxdb-databasing",
    "href": "content/projects/RLUnicycle/telemetry/telemetry.html#influxdb-databasing",
    "title": "Telemetry and Database System",
    "section": "2.4 InfluxDB Databasing",
    "text": "2.4 InfluxDB Databasing\nThe database can be accessed through Python API, through Grafana, or even a direct viewer. The database is also to be installed on the Lenovo server. The default port for InfluxDB is 8086. The database can be accessed through a web browser by navigating to the IP address of the Lenovo server on port 8086.\nMQTT gives livestream data but if we want data storage and permanence between runs it needs to be databased. InfluxDB offers this service along with data manipulation services and a special query language. It also includes a data explorer through the web interface.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Telemetry and Database System"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/telemetry/telemetry.html#node-red",
    "href": "content/projects/RLUnicycle/telemetry/telemetry.html#node-red",
    "title": "Telemetry and Database System",
    "section": "2.5 Node Red",
    "text": "2.5 Node Red\nNode Red is a flow-based open source development tool for visual programming developed by IBM. It is used for wiring together hardware devices, APIs, and online services in new and interesting ways. It provides a browser-based editor that makes it easy to wire together flows using the wide range of nodes in the palette that can be deployed to its runtime in a single-click.\nFor this application Node Red is used to bridge the MQTT broker to the InfluxDB database. This allows for the data to be stored in a database for later access. The data can be manipulated and stored in a more permanent format.\nThe Node Red server is installed on the Lenovo server. The default port for Node Red is 1880. The program operates through a web browser and can be accessed by navigating to the IP address of the Lenovo server on port 1880.\n\n2.5.1 Node Red Dashboard\n\n\n\nNode Red Dashboard\n\n\nThe Node Red dashboard is an additional feature that is accessed through the Node Red server. It allows for the creation of custom dashboards that can be used to provide a GUI for a robotics project. The GUI can be used to connect directly to incoming signals and also produce outgoing command signals that can be sent to the robot.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Telemetry and Database System"
    ]
  }
]