[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Simon Ghyselincks Personal Website",
    "section": "",
    "text": "Welcome to my personal site! I’m Simon Ghyselincks, currently a 5th-year Engineering Physics student at the University of British Columbia (UBC), with a minor in Computer Science. I am studying a cross-disciplinary blend of engineering, computer science, and applied mathematics. What I really love is coding to solve tough problems in robotics, machine learning, signal processing, and more."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Simon Ghyselincks Personal Website",
    "section": "",
    "text": "Welcome to my personal site! I’m Simon Ghyselincks, currently a 5th-year Engineering Physics student at the University of British Columbia (UBC), with a minor in Computer Science. I am studying a cross-disciplinary blend of engineering, computer science, and applied mathematics. What I really love is coding to solve tough problems in robotics, machine learning, signal processing, and more."
  },
  {
    "objectID": "index.html#academics-and-projects",
    "href": "index.html#academics-and-projects",
    "title": "Simon Ghyselincks Personal Website",
    "section": "Academics and Projects",
    "text": "Academics and Projects\nI am currently working with Eldad Haber at UBC Earth and Ocean Sciences on generative AI for geophysical applications. Our work explores the application of recent advances in normalizing flows with stochastic interpolants to generate 3d models of the earth’s crust. I am also continuing to develop our Engineering Physics capstone project “Learning to Balance” which explores the application of reinforcement learning to a reaction wheel robot with complex dynamics. Read more about my projects here.\n\nFeel free to connect with me on LinkedIn or check out my GitHub."
  },
  {
    "objectID": "index.html#my-journey",
    "href": "index.html#my-journey",
    "title": "Simon Ghyselincks Personal Website",
    "section": "My Journey",
    "text": "My Journey\nRead more about my journey and past pursuits here."
  },
  {
    "objectID": "content/projects/RLUnicycle/rtkernel/rtpatch.html",
    "href": "content/projects/RLUnicycle/rtkernel/rtpatch.html",
    "title": "RT Kernel on Jetson Nano",
    "section": "",
    "text": "The following guide is intended to provide step-by-step instructions on how to compile a real-time (RT) Linux kernel for the NVIDIA Jetson Nano. The RT kernel is based on the PREEMPT_RT patch, which adds real-time capabilities to the Linux kernel by making it fully preemptible and reducing the latency of the kernel’s interrupt handling.\nThis guide has been modified from some valuable instructions found at: https://forums.developer.nvidia.com/t/applying-a-preempt-rt-patch-to-jetpack-4-5-on-jetson-nano/168428/4\n\n\nFirst download the BSP from the NVIDIA website. The BSP contains the kernel source code, device tree files, and other necessary files for building the kernel. The BSP also contains the sample root filesystem, which is used to create the final image for the Jetson Nano. You may wish to look up the most recent version of the Tegra for Linux, in this case we are using R32.7.4.\nYou can download all of these files onto a Linux machine specifically running Ubuntu 18.04. Another option that has been tested is compiling on the Jetson Nano itself which is running the correct version of Linux by default. For our project we installed 18.04 on a laptop and compiled the kernel there.\n\n\n\n\n\n\nNote\n\n\n\nSource Files:\nhttps://developer.nvidia.com/embedded/linux-tegra-r3274\n\n\nDownload:\n\nDriver Package (BSP)\nSample Root File System\nDriver Package (BSP) Sources\nGCC Tool Chain can also be obtained via the command line:\nwget http://releases.linaro.org/components/toolchain/binaries/7.3-2018.05/aarch64-linux-gnu/gcc-linaro-7.3.1-2018.05-x86_64_aarch64-linux-gnu.tar.xz\n\nPile all the files into a single directory and install packages\nsudo apt-get update \nsudo apt-get install libncurses5-dev \nsudo apt-get install build-essential \nsudo apt-get install bc \nsudo apt-get install lbzip2 \nsudo apt-get install qemu-user-static \nsudo apt-get install python\n\nmkdir $HOME/jetson_nano \ncd $HOME/jetson_nano\nExtract all of the files\nsudo tar xpf jetson-210_linux_r32.7.4_aarch64.tbz2\ncd Linux_for_Tegra/rootfs/ \nsudo tar xpf ../../tegra_linux_sample-root-filesystem_r32.7.4_aarch64.tbz2\ncd ../../ \ntar -xvf gcc-linaro-7.3.1-2018.05-x86_64_aarch64-linux-gnu.tar.xz \nsudo tar -xjf public_sources.tbz2 \ntar -xjf Linux_for_Tegra/source/public/kernel_src.tbz2\n\n\n\nGo into extracted kernel source and apply RT patch\ncd kernel/kernel-4.9/\n./scripts/rt-patch.sh apply-patches\nConfigure and compile:\nTEGRA_KERNEL_OUT=jetson_nano_kernel \nmkdir $TEGRA_KERNEL_OUT \nexport CROSS_COMPILE=$HOME/jetson_nano/gcc-linaro-7.3.1-2018.05-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu- \nmake ARCH=arm64 O=$TEGRA_KERNEL_OUT tegra_defconfig \nmake ARCH=arm64 O=$TEGRA_KERNEL_OUT menuconfig\nThe menu config opens an old school BIOS menu. Set the proper settings for the RT kernel:\n\nGeneral setup → Timer subsystem → Timer tick handling → Full dynticks system (tickless)\nKernel Features → Preemption Model: Fully Preemptible Kernel (RT)\nKernel Features → Timer frequency: 1000 HZ\n\nAt this point you can go tamper with device tree files (.dtsi) or other things, next step is the compile stage!\n\n\nI tried to modify\ntegra210-porg-gpio-p3448-0000-b00.dtsi \nthe source file, found using a find file function in terminal. It did not fix things. In general the P3450 model requires the p3448-0000-3449-b00 series of files. This was confirmed by looking at all the source configs and scripts.\n\n\n\nmake ARCH=arm64 O=$TEGRA_KERNEL_OUT -j4\n\nsudo cp jetson_nano_kernel/arch/arm64/boot/Image $HOME/jetson_nano/Linux_for_Tegra/kernel/Image\nsudo cp -r jetson_nano_kernel/arch/arm64/boot/dts/* $HOME/jetson_nano/Linux_for_Tegra/kernel/dtb/\nsudo make ARCH=arm64 O=$TEGRA_KERNEL_OUT modules_install INSTALL_MOD_PATH=$HOME/jetson_nano/Linux_for_Tegra/rootfs/\n\ncd $HOME/jetson_nano/Linux_for_Tegra/rootfs/\nsudo tar --owner root --group root -cjf kernel_supplements.tbz2 lib/modules\nsudo mv kernel_supplements.tbz2  ../kernel/\n\ncd ..\nsudo ./apply_binaries.sh\nThe image creator requires the device model. For the 4GB Jetson nano it is -r 300. This will select the correct dtb:\ncd tools\nsudo ./jetson-disk-image-creator.sh -o jetson_nano.img -b jetson-nano -r 300\nIt is crucial to select the correct device tree since it will not boot otherwise. If you are unsure of which to select, follow through the source cocde in the jetson-disk-image-creator.sh to find what the different flags do. Or try the NVIDIA forums but good luck over there!\nUse Balena etcher to put image in $HOME/jetson_nano/Linux_for_Tegra/tools/jetson_nano.img onto the SD card",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "RT Kernel on Jetson Nano"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/rtkernel/rtpatch.html#download-source-files-and-install-packages",
    "href": "content/projects/RLUnicycle/rtkernel/rtpatch.html#download-source-files-and-install-packages",
    "title": "RT Kernel on Jetson Nano",
    "section": "",
    "text": "First download the BSP from the NVIDIA website. The BSP contains the kernel source code, device tree files, and other necessary files for building the kernel. The BSP also contains the sample root filesystem, which is used to create the final image for the Jetson Nano. You may wish to look up the most recent version of the Tegra for Linux, in this case we are using R32.7.4.\nYou can download all of these files onto a Linux machine specifically running Ubuntu 18.04. Another option that has been tested is compiling on the Jetson Nano itself which is running the correct version of Linux by default. For our project we installed 18.04 on a laptop and compiled the kernel there.\n\n\n\n\n\n\nNote\n\n\n\nSource Files:\nhttps://developer.nvidia.com/embedded/linux-tegra-r3274\n\n\nDownload:\n\nDriver Package (BSP)\nSample Root File System\nDriver Package (BSP) Sources\nGCC Tool Chain can also be obtained via the command line:\nwget http://releases.linaro.org/components/toolchain/binaries/7.3-2018.05/aarch64-linux-gnu/gcc-linaro-7.3.1-2018.05-x86_64_aarch64-linux-gnu.tar.xz\n\nPile all the files into a single directory and install packages\nsudo apt-get update \nsudo apt-get install libncurses5-dev \nsudo apt-get install build-essential \nsudo apt-get install bc \nsudo apt-get install lbzip2 \nsudo apt-get install qemu-user-static \nsudo apt-get install python\n\nmkdir $HOME/jetson_nano \ncd $HOME/jetson_nano\nExtract all of the files\nsudo tar xpf jetson-210_linux_r32.7.4_aarch64.tbz2\ncd Linux_for_Tegra/rootfs/ \nsudo tar xpf ../../tegra_linux_sample-root-filesystem_r32.7.4_aarch64.tbz2\ncd ../../ \ntar -xvf gcc-linaro-7.3.1-2018.05-x86_64_aarch64-linux-gnu.tar.xz \nsudo tar -xjf public_sources.tbz2 \ntar -xjf Linux_for_Tegra/source/public/kernel_src.tbz2",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "RT Kernel on Jetson Nano"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/rtkernel/rtpatch.html#apply-rt-patch",
    "href": "content/projects/RLUnicycle/rtkernel/rtpatch.html#apply-rt-patch",
    "title": "RT Kernel on Jetson Nano",
    "section": "",
    "text": "Go into extracted kernel source and apply RT patch\ncd kernel/kernel-4.9/\n./scripts/rt-patch.sh apply-patches\nConfigure and compile:\nTEGRA_KERNEL_OUT=jetson_nano_kernel \nmkdir $TEGRA_KERNEL_OUT \nexport CROSS_COMPILE=$HOME/jetson_nano/gcc-linaro-7.3.1-2018.05-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu- \nmake ARCH=arm64 O=$TEGRA_KERNEL_OUT tegra_defconfig \nmake ARCH=arm64 O=$TEGRA_KERNEL_OUT menuconfig\nThe menu config opens an old school BIOS menu. Set the proper settings for the RT kernel:\n\nGeneral setup → Timer subsystem → Timer tick handling → Full dynticks system (tickless)\nKernel Features → Preemption Model: Fully Preemptible Kernel (RT)\nKernel Features → Timer frequency: 1000 HZ\n\nAt this point you can go tamper with device tree files (.dtsi) or other things, next step is the compile stage!\n\n\nI tried to modify\ntegra210-porg-gpio-p3448-0000-b00.dtsi \nthe source file, found using a find file function in terminal. It did not fix things. In general the P3450 model requires the p3448-0000-3449-b00 series of files. This was confirmed by looking at all the source configs and scripts.\n\n\n\nmake ARCH=arm64 O=$TEGRA_KERNEL_OUT -j4\n\nsudo cp jetson_nano_kernel/arch/arm64/boot/Image $HOME/jetson_nano/Linux_for_Tegra/kernel/Image\nsudo cp -r jetson_nano_kernel/arch/arm64/boot/dts/* $HOME/jetson_nano/Linux_for_Tegra/kernel/dtb/\nsudo make ARCH=arm64 O=$TEGRA_KERNEL_OUT modules_install INSTALL_MOD_PATH=$HOME/jetson_nano/Linux_for_Tegra/rootfs/\n\ncd $HOME/jetson_nano/Linux_for_Tegra/rootfs/\nsudo tar --owner root --group root -cjf kernel_supplements.tbz2 lib/modules\nsudo mv kernel_supplements.tbz2  ../kernel/\n\ncd ..\nsudo ./apply_binaries.sh\nThe image creator requires the device model. For the 4GB Jetson nano it is -r 300. This will select the correct dtb:\ncd tools\nsudo ./jetson-disk-image-creator.sh -o jetson_nano.img -b jetson-nano -r 300\nIt is crucial to select the correct device tree since it will not boot otherwise. If you are unsure of which to select, follow through the source cocde in the jetson-disk-image-creator.sh to find what the different flags do. Or try the NVIDIA forums but good luck over there!\nUse Balena etcher to put image in $HOME/jetson_nano/Linux_for_Tegra/tools/jetson_nano.img onto the SD card",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "RT Kernel on Jetson Nano"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/rtkernel/rtpatch.html#setting-python-scheduling-privileges",
    "href": "content/projects/RLUnicycle/rtkernel/rtpatch.html#setting-python-scheduling-privileges",
    "title": "RT Kernel on Jetson Nano",
    "section": "2.1 Setting Python Scheduling Privileges",
    "text": "2.1 Setting Python Scheduling Privileges\nNote that for this description our team is using Python 3.8 in a virtual environment, the instructions path files may change slightly if using a different version.\nThe scheduling priority is a top-level system command and is usually locked behind ‘sudo’. This is problematic when running a Python script because we don’t want to run it as sudo allowing it full access to wreak havoc on the OS. The solution is to grant only the scheduling part of ‘sudo’ to the Python interpreter:\nThis command only needs to be set once after Python 3.8 is installed (the same in use in our venv): sudo setcap 'cap_sys_nice=eip' /usr/bin/python3.8\n\nsetcap: This is a utility that sets or changes the capabilities of a file/executable. Capabilities are a Linux feature that allow for more fine-grained access control; they provide a way to grant specific privileges to executables that normally only the root user would have.\n'cap_sys_nice=eip': This argument specifies the capabilities to be set on the file, in this case, /usr/bin/python3.8. It’s composed of three parts:\n\ncap_sys_nice: This is the specific capability being set. cap_sys_nice allows the program to raise process nice values (which can deprioritize processes) and change real-time scheduling priorities and policies, without requiring full root privileges.\ne: This stands for “effective” and means the capability is “activated” and can be used by the executable.\ni: This stands for “inheritable”, meaning this capability can be inherited by child processes created by the executable.\np: This stands for “permitted”, which means the capability is allowed for the executable. It’s a part of the set of capabilities that the executable is permitted to use.\n\n/usr/bin/python3.8: This is the path to the Python 3.8 executable. The command sets the specified capabilities on this specific file.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "RT Kernel on Jetson Nano"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/rtkernel/rtpatch.html#setting-script-specific-rt",
    "href": "content/projects/RLUnicycle/rtkernel/rtpatch.html#setting-script-specific-rt",
    "title": "RT Kernel on Jetson Nano",
    "section": "2.2 Setting Script Specific RT",
    "text": "2.2 Setting Script Specific RT\nThe ‘RT’ scheduling priority is code 99. Some imported C implementation allows for resetting the scheduling for the process. The function is wrapped in try/except block to ensure it activates.\n# Define constants for the scheduling policy\nSCHED_FIFO = 1  # FIFO real-time policy\n\nclass SchedParam(ctypes.Structure):\n    _fields_ = [('sched_priority', ctypes.c_int)]\n\ndef set_realtime_priority(priority=99):\n    libc = ctypes.CDLL('libc.so.6')\n    param = SchedParam(priority)\n    # Set the scheduling policy to FIFO and priority for the entire process (0 refers to the current process)\n    if libc.sched_setscheduler(0, SCHED_FIFO, ctypes.byref(param)) != 0:\n        raise ValueError(\"Failed to set real-time priority. Check permissions.\") \nWe run this function at the start of the script which will reassign the scheduling priority to the highest level. This can be verified to work by opening the system monitor and checking the priority of the script such as with htop.\n\n\n\nRT Priority Enabled",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "RT Kernel on Jetson Nano"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html",
    "title": "Dynamics and Control",
    "section": "",
    "text": "The following is a demonstration of the derivation for the equations of motion for a single degree of freedom reaction wheel inverted pendulum. The approach used is energy methods via the Lagrangian using classical mechanics.\nAn automated derivation sequence using MATLAB is presented, which allows for parsing the equations of motion for an arbitrary system such as a 4-DOF unicycle robot. The code for the auto-derivation has been tested by hand against known solutions in the literature, as explored by (Brevik 2017), (Montoya and Gil-González 2020).",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html#mass-and-center-of-mass-measurements",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html#mass-and-center-of-mass-measurements",
    "title": "Dynamics and Control",
    "section": "Mass and Center of Mass Measurements",
    "text": "Mass and Center of Mass Measurements\nThe mass and center of mass (CM) were measured using a lab scale and a balancing method, respectively.\n\nFlywheel: The wheel and rings mass (denoted as \\(m_w\\)) was measured to be 346g. The CM of the wheel from the pendulum hinge (denoted as \\(l_w\\)) is 180mm. This was measured in CAD and also with a ruler.\nPendulum and Motor: The combined mass of the pendulum and motor with stator (denoted as \\(m_p\\)) was measured to be 531g. The CM of the pendulum with motor and stator (denoted as \\(l_p\\)) is 100mm. The pendulum CM is found by balancing the apparatus with removed flywheel overtop of a fulcrum and finding the stable resting point position.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html#inertia-calculations",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html#inertia-calculations",
    "title": "Dynamics and Control",
    "section": "Inertia Calculations",
    "text": "Inertia Calculations\nThe moment of inertia for each component was calculated using the parallel axis theorem and the physical dimensions provided by CAD models and direct measurement.\n\nWheel Inertia\nThe wheel inertia (denoted as \\(I_w\\)) was found by comparing the CAD weight to the measured weight of the flywheel to find agreement: \\[I_w = 725\\ \\text{kg}\\cdot\\text{mm}^2\\] In particular the metal rings were weighed and set to be the same weight in CAD which is the most influential part of the moment in question.\n\n\nPendulum Inertia\nThe pendulum moment of inertia (denoted as \\(I_p\\)) is a composite value derived from the inertia of individual components:\n\nBattery: The battery contributes an inertia of: \\[I_{\\text{battery}} = \\frac{1}{12} \\cdot 0.185 \\cdot (70^2 + 35^2) + 0.185 \\cdot 50^2 = 446\\ \\text{kg}\\cdot\\text{mm}^2\\]\nPendulum Arm: The corrected inertia for the pendulum arm is: \\[I_{\\text{arm}} = 346\\ \\text{kg}\\cdot\\text{mm}^2 + 0.102 \\cdot 45^2 = 552\\ \\text{kg}\\cdot\\text{mm}^2\\]\nMotor and Mount: The combined inertia for the motor and mount is: \\[I_{\\text{motor}} = 0.5 \\cdot 0.206 \\cdot 30^2 + 0.206 \\cdot 75^2 = 1251.75\\ \\text{kg}\\cdot\\text{mm}^2\\]\n\nThe total pendulum inertia is then calculated as the sum of the components: \\[I_p = I_{\\text{battery}} + I_{\\text{arm}} + I_{\\text{motor}} = 2250\\ \\text{kg}\\cdot\\text{mm}^2\\]",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html#kinetic-energy",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html#kinetic-energy",
    "title": "Dynamics and Control",
    "section": "Kinetic Energy",
    "text": "Kinetic Energy\n\\[\\begin{aligned}\nT &= T_p+T_w \\\\\nT_p &= \\frac{1}{2}(\\underbrace{I_p + m_pl_p^2}_{\\text{Parallel Axis Theorem}})\\dot{\\varphi}^2\\\\\nT_w&=\\frac{1}{2}m_w(\\underbrace{l_w\\dot{\\varphi}}_{\\text{Speed of CM}})^2 +  \\frac{1}{2}I_w(\\underbrace{\\dot{\\varphi}+\\dot{\\theta}}_{\\text{net rotation earth frame}})^2\\\\\nT_{net} &= \\frac{1}{2} \\left(I_p + m_p l_p^2 + I_w + m_w l_w^2\\right) \\dot{\\varphi}^2 + \\frac{1}{2} I_w (\\dot{\\varphi} + \\dot{\\theta})^2 \\\\\n&= \\frac{1}{2} \\left(I_p + m_p l_p^2\\right) \\dot{\\varphi}^2 + \\frac{1}{2} I_w \\left(\\dot{\\varphi}^2 + 2\\dot{\\varphi}\\dot{\\theta} + \\dot{\\theta}^2\\right)\\\\\nT_{net}&=\\frac{1}{2} [\\dot{\\varphi}, \\dot{\\theta}] \\begin{bmatrix}\n    I_p + m_pl_p^2 + I_w + m_wl_w^2 & I_w \\\\\n    I_w & I_w\n\\end{bmatrix} \\begin{bmatrix}\n    \\dot{\\varphi} \\\\\n\\dot{\\theta}\n\\end{bmatrix}\n\\end{aligned}\\]\nThis gives the form using the inertia matrix M, note the matrix is always symmetric.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html#potential-energy",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html#potential-energy",
    "title": "Dynamics and Control",
    "section": "Potential Energy",
    "text": "Potential Energy\nThe potential energy is taken by projecting the position of the center of masses onto the vertical axis using \\(\\cos(\\varphi)\\), noting that the angle \\(\\theta\\) has no impact on the potential since the wheel is radially symmetric. \\[U = (m_pl_p + m_wl_w)g \\cos (\\varphi) = m_0 \\cos (\\varphi)\\] We can simplify future equations by assigning an equivalent variable \\(m_0 = (m_p l_p + m_w l_w)g\\)\nThis gives the complete Lagrangian \\[\\mathcal{L}(\\varphi,\\theta,\\dot \\varphi,\\dot \\theta)= KE - PE = \\frac{1}{2}\\mathbf{\\dot q}^{T}\\mathbf{M}\\mathbf{\\dot q}-m_0cos(\\varphi)\\]",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html#matlab-derivation",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html#matlab-derivation",
    "title": "Dynamics and Control",
    "section": "Matlab Derivation",
    "text": "Matlab Derivation\nThe required files to run this code are included at https://github.com/Team-2411-RL-Unicycle/pid-control The automated E-L solver uses a modified version of a file made by (Veng 2023). It is incorporated into the RWIPpid_derivation.m file. The derivation technique is validated against the equations derived by (Brevik 2017).\nThe first step is to define symbolic variables for all of the parameters, states, and inputs\nsyms mp lp Ip mw lw Iw real\nparams = [mp, lp, Ip, mw, lw, Iw];\n% Define numerical values for the parameters\nvalues = [.531, 0.100, 0.002250, .346, 0.180, 0.000725];\ng=9.81;\n% State variables\nsyms phi theta dphi dtheta real\nq = [phi, theta];\ndq = [dphi, dtheta];\n% Input\nsyms tau real\n\n% Potential energy mass\nm0 = (mp*lp + mw*lw)*g; % Effective U=mgh for combined parts\n% Mass matrix\nM = [(Ip + mp*lp^2 + Iw +mw*lw^2), Iw;\n    Iw, Iw];\nlagrangian = (1/2)*([dphi, dtheta])*M*([dphi, dtheta]') - m0 * cos(phi);\n% Non-conservative forces in each coordinate q\nQ = [0, tau];\nThe Lagrangian and its non-conservative forces are fully defined now. The equations are solved using the modified imported library and the solution equations for each second time derivative is solved giving \\(\\frac{d}{dt}\\dot{q}\\), these solutions can be packed into a single array to form a matrix.\n[eqs, ddq] = EulerLagrange(q,dq,lagrangian,Q);\n% Explicit equations:\nexp_eqs = ddq == eqs;\n% Solve equations to isolate ddphi and ddtheta\nddqSolutions = solve(ddq == eqs, ddq);\n% Convert solutions to cell array\nddqSolutionEquations = struct2cell(ddqSolutions) ;\nddqArray = [ddqSolutionEquations{:}].';",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html#derived-equations-of-motion",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html#derived-equations-of-motion",
    "title": "Dynamics and Control",
    "section": "Derived Equations of Motion",
    "text": "Derived Equations of Motion\nOnce we have \\(n\\) 2nd order ODEs for \\(n\\) general coordinates and their \\(n\\) general time derivatives we have enough to make a first order system of ODEs that characterize the system. The time-domain non-linearized result from the derivation is given below.\n\\[\\frac{d}{dt}\\vec{x} = \\vec{G}(\\vec{x}, t) = \\begin{bmatrix}\nd\\varphi \\\\\nd\\theta \\\\\n\\frac{g_0 l_p m_p \\sin(\\phi) - \\tau + g_0 l_w m_w \\sin(\\phi)}{m_p l_p^2 + m_w l_w^2 + I_p} \\\\\n\\frac{m_p \\tau l_p^2 - I_w g_0 m_p \\sin(\\phi) l_p + m_w \\tau l_w^2 - I_w g_0 m_w \\sin(\\phi) l_w + I_p \\tau + I_w \\tau}{I_w (m_p l_p^2 + m_w l_w^2 + I_p)}\n\\end{bmatrix}, \\quad x = \\begin{bmatrix}\n\\varphi \\\\ \\theta \\\\ \\dot{\\varphi} \\\\ \\dot{\\theta}\n\\end{bmatrix}\\]\nNote that there is no explicit time dependence in the function \\(G\\) the inverted pendulum dynamics and rigid body characteristics are constant over time. From inspection of the solutions we see that \\(\\theta\\), the angle of the wheel does not play a role in the function \\(G\\) and can be removed entirely if desired.\nThese system dynamics can be used to create a time-domain non-linear simulation using Euler’s method to get numerical solutions. Friction can be added as a damping coefficient \\(\\beta\\) such that we superimpose \\(\\ddot{\\varphi} = - \\beta \\dot{\\varphi}\\) onto the solution for example.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html#linearization",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html#linearization",
    "title": "Dynamics and Control",
    "section": "Linearization",
    "text": "Linearization\nWe wish to convert \\[\\vec{G}(\\vec{x}, t) \\approx Ax + Bu\\] via linearization about the operating point. We choose the upright position as the target and note that \\(\\varphi\\) is the only variable present in \\(G\\). \\(\\hat{\\vec{x}}=0\\) is the chosen linearization point:\n\\[\\frac{d}{dt}\\vec{x} \\approx \\hat{\\vec{x}} + \\left. \\text{Jacobian}\\{\\vec{G}(\\vec{x}, t)\\} \\right|_{\\vec{x}=\\hat{\\vec{x}}} (\\vec{x}-\\hat{\\vec{x}}) = \\left. \\Big(A\\Big) \\right|_{\\vec{x}=\\hat{\\vec{x}}} \\vec{x}\\]\nWe perform a similar linearization to get the effect of the system inputs by taking the Jacobian with respect to \\(\\tau\\). The two combined give the cannonical \\(\\frac{d}{dt}x = Ax + Bu\\) of controls engineering. The final step is to take the Laplace transform of the entire equation and then solve for the transfer function between the system inputs \\(u\\) or in this case \\(\\tau\\) and the observables we want (mainly the system state \\(x\\)) but this generalizes to any observable that is a function of \\(x\\) and \\(u\\)\nState Vector \\[{\\mathbf{x}} =\n\\begin{bmatrix}\n    x_1 \\\\\n    x_2 \\\\\n    \\vdots\n\\end{bmatrix}\\] Input Vector \\[{\\mathbf{u}} =\n\\begin{bmatrix}\n    u_1 \\\\\n    u_2 \\\\\n    \\vdots\n\\end{bmatrix}\\] Output Vector \\[{\\mathbf{y}} =\n\\begin{bmatrix}\n    y_1 \\\\\n    y_2 \\\\\n    \\vdots\n\\end{bmatrix}\\]\nState Equation \\[\\dot{\\mathbf{x}} =\n\\begin{bmatrix}\n    \\dot{x}_1 \\\\\n    \\dot{x}_2 \\\\\n    \\vdots\n\\end{bmatrix}\n= \\mathbf{A}{\\mathbf{x}} + \\mathbf{B}{\\mathbf{u}}\\] Output Equation \\[{\\mathbf{y}} = \\mathbf{C}{\\mathbf{x}} + \\mathbf{D}{\\mathbf{u}}\\]\nState Transition Matrix \\[\\mathbf{\\Phi} = (s\\mathbf{I} - \\mathbf{A})^{-1}\\] Transfer Functions \\[\\frac{{\\mathbf{y}}}{{\\mathbf{u}}} = \\mathbf{C}\\mathbf{\\Phi}\\mathbf{B} + \\mathbf{D}\\]\nWe solve for the transfer matrix \\(y = Gu\\) at \\(x=0\\), noting that in our case \\(y=x\\)",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html#matlab-derivation-1",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html#matlab-derivation-1",
    "title": "Dynamics and Control",
    "section": "MATLAB Derivation",
    "text": "MATLAB Derivation\n% Phi, dPhi, dTheta\nX = [q(1)' ; dq']\n% The inputs are non-zero entries of Q (non-conservative forces)\nU = Q(Q ~= 0);\n% Vector functionn for the derivative of the state vector\ndX = [dphi; ddqArray]\n\n% Compute the Jacobian matrices to get nonlinear state matrices dX = Ax + Bu\nA = jacobian(dX, X);\nB = jacobian(dX, U);\n\n% Substitute or linearize about an equilibrium point\n% Define equilibrium point (for example, all zeros)\nx0 = [0; 0; 0];\n% Substitute equilibrium values x0 into A and B\nAeq = subs(A, X, x0)\nBeq = subs(B, X, x0)\n\n% U to X transfer function\n% dX = Ax + Bu implies  sX = Ax + Bu, solve for x = Gtf*u\nsyms s\nGtf = (s*eye(length(X)) - Aeq)^(-1)*Beq",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html#system-transfer-function",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html#system-transfer-function",
    "title": "Dynamics and Control",
    "section": "System Transfer Function",
    "text": "System Transfer Function\n\\[\\begin{pmatrix}\n\\varphi(s)\\\\\n\\dot{\\varphi}(s)\\\\\n\\dot\\theta(s)\n\\end{pmatrix}=\n\\begin{pmatrix}\n-\\frac{1}{{m_p l_p^2 s^2 - g_0 m_p l_p + m_w l_w^2 s^2 - g_0 m_w l_w + I_p s^2}} \\\\\n-\\frac{s}{{m_p l_p^2 s^2 - g_0 m_p l_p + m_w l_w^2 s^2 - g_0 m_w l_w + I_p s^2}} \\\\\n\\frac{{m_p l_p^2 + m_w l_w^2 + I_p + I_w}}{{I_w s (m_p l_p^2 + m_w l_w^2 + I_p)}} + \\frac{{g_0 l_p m_p + g_0 l_w m_w}}{{s (m_p l_p^2 + m_w l_w^2 + I_p)(m_p l_p^2 s^2 - g_0 m_p l_p + m_w l_w^2 s^2 - g_0 m_w l_w + I_p s^2)}}\n\\end{pmatrix}\n\\tau(s)\\]\nWe note that for our control problem we are trying to control the angle \\(\\varphi\\) using torque, so the function of interest is the upper row equation:\n\\[\\varphi(s) = \\Big(-\\frac{1}{{m_p l_p^2 s^2 - g_0 m_p l_p + m_w l_w^2 s^2 - g_0 m_w l_w + I_p s^2}}\\Big) \\tau(s)\\]\nOr rearranging we see that we have function of the form \\(\\frac{1}{s^2+a^2}\\): \\[\\varphi(s) = \\left(-\\frac{1}{s^2(m_p l_p^2 + m_w l_w^2 + I_p) - m_0}\\right) \\tau(s)\\]\nThis is a function with one pole in the RH plane making it unstable.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html#pd-controller-for-pendulum-angle",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html#pd-controller-for-pendulum-angle",
    "title": "Dynamics and Control",
    "section": "PD Controller for Pendulum Angle",
    "text": "PD Controller for Pendulum Angle\nThe PD Controller for \\(\\varphi\\) is tuned using the assumption that the torque requests have little delay before reaching the intended value. This is because the motor controller is running at 100 times faster than the main control loop frequency of 100Hz. Thus we model the feedback loop of Controller -&gt; \\(G(s)\\) -&gt; \\(H(s)\\) Sensor Fusion. The sensor fusion and torque request mechanism are modeled as a delay of one \\(100Hz\\) control cycle.\nA PD controller is selected because of the dynamic setpoint that is being controlled by the cascade arrangement. If we were to include an I term then the controller would not be memoryless and would have undesirable response characteristics to the dynamic \\(\\varphi\\) setpoint being requested by the higher level controller. The PD control model is a robust choice for a controller for this robot state parameter, (Brevik 2017).\nThe MATLAB pid tuner is used to get feasible starting values based on this loop. The experimental parameters applied to the robot were found to closely match the predicted values.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/dynamics/dynamics.html#pi-controller-for-wheel-velocity",
    "href": "content/projects/RLUnicycle/dynamics/dynamics.html#pi-controller-for-wheel-velocity",
    "title": "Dynamics and Control",
    "section": "PI Controller for Wheel Velocity",
    "text": "PI Controller for Wheel Velocity\nThe PI controller is tuned heuristically once a good underlying PD controller for the angle is found. A starting value of around \\(K_p = 0.1\\) was found to be helpful. Blending of integral term with a corresponding reduction of \\(P\\) is one approach to further tuning.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Dynamics and Control"
    ]
  },
  {
    "objectID": "content/eosc555/lectures/lecture5/index.html",
    "href": "content/eosc555/lectures/lecture5/index.html",
    "title": "Lecture 5: Autodiff and Gauss Newton Optimization",
    "section": "",
    "text": "$$ \n$$"
  },
  {
    "objectID": "content/eosc555/lectures/lecture5/index.html#a-non-linear-dynamics-problem",
    "href": "content/eosc555/lectures/lecture5/index.html#a-non-linear-dynamics-problem",
    "title": "Lecture 5: Autodiff and Gauss Newton Optimization",
    "section": "A Non-Linear Dynamics Problem",
    "text": "A Non-Linear Dynamics Problem\nA well studied problem in non-linear dynamics involves the predator-prey model that is described by the Lotka-Volterra equations. The equations are given by:\n\\[\n\\begin{aligned}\n\\frac{dx}{dt} &= \\alpha x - \\beta xy \\\\\n\\frac{dy}{dt} &= \\delta xy - \\gamma y\n\\end{aligned}\n\\]\nwhere \\(x\\) and \\(y\\) are the populations of the prey and predator respectively. The parameters \\(\\alpha, \\beta, \\gamma, \\delta\\) are positive constants. The goal is to find the values of these parameters that best fit the data.\nThere is no closed form analytic solution that is known to this remarkably simple system of equations, which is why we must resort to numerical solutions to compute the model.\nMore information about the model can be found at the Wikipedia page.\n\nThe Forward Problem\nWe start with an initial time \\(t_0\\) and initial conditions \\(x_0, y_0\\), with parameters \\(\\alpha, \\beta, \\gamma, \\delta\\) to run a forward version of the problem using a variant of the forward Euler method, the RK4.\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\n\nclass LotkaVolterraModel(nn.Module):\n    def __init__(self, alpha, beta, gamma, delta):\n        super(LotkaVolterraModel, self).__init__()\n        # Define parameters as torch tensors that require gradients\n        self.alpha = nn.Parameter(torch.tensor(alpha, dtype=torch.float32))\n        self.beta = nn.Parameter(torch.tensor(beta, dtype=torch.float32))\n        self.gamma = nn.Parameter(torch.tensor(gamma, dtype=torch.float32))\n        self.delta = nn.Parameter(torch.tensor(delta, dtype=torch.float32))\n\n    def forward(self, x, y):\n        # Ensure x and y are tensors\n        if not isinstance(x, torch.Tensor):\n            x = torch.tensor(x, dtype=torch.float32)\n        if not isinstance(y, torch.Tensor):\n            y = torch.tensor(y, dtype=torch.float32)\n\n        # Compute dx and dy based on the current parameters\n        dx = self.alpha * x - self.beta * x * y\n        dy = self.delta * x * y - self.gamma * y\n\n        return dx, dy\n\nclass RK4Solver:\n    def __init__(self, model):\n        self.model = model\n\n    def step(self, x, y, dt):\n        \"\"\"\n        Perform a single RK4 step.\n        \"\"\"\n        # Convert x and y to tensors if they are not already\n        if not isinstance(x, torch.Tensor):\n            x = torch.tensor(x, dtype=torch.float32)\n        if not isinstance(y, torch.Tensor):\n            y = torch.tensor(y, dtype=torch.float32)\n\n        # RK4 Step calculations\n        k1_x, k1_y = self.model.forward(x, y)\n        k2_x, k2_y = self.model.forward(x + 0.5 * dt * k1_x, y + 0.5 * dt * k1_y)\n        k3_x, k3_y = self.model.forward(x + 0.5 * dt * k2_x, y + 0.5 * dt * k2_y)\n        k4_x, k4_y = self.model.forward(x + dt * k3_x, y + dt * k3_y)\n\n        # Update x and y using weighted averages of the slopes\n        x_new = x + (dt / 6) * (k1_x + 2 * k2_x + 2 * k3_x + k4_x)\n        y_new = y + (dt / 6) * (k1_y + 2 * k2_y + 2 * k3_y + k4_y)\n\n        return x_new, y_new\n\n    def solve(self, x0, y0, time_steps):\n        \"\"\"\n        Solve the system over a serie of time steps.\n        Parameters:\n            x0: Initial value of prey population\n            y0: Initial value of predator population\n            time_steps: List or numpy array of time steps to solve over\n        \"\"\"\n        x, y = x0, y0\n        DT = time_steps[1:] - time_steps[:-1]\n        trajectory = torch.zeros(len(time_steps), 2)\n        trajectory[0] = torch.tensor([x, y])\n\n        for i, dt in enumerate(DT):\n            x, y = self.step(x, y, dt)\n            trajectory[i+1] = torch.tensor([x, y])            \n\n        return trajectory\n\n# Define the model parameters\nalpha = 1.0\nbeta = .1\ngamma = 1.5\ndelta = 0.1\n\n# Create the model and solver\nmodel = LotkaVolterraModel(alpha, beta, gamma, delta)\nsolver = RK4Solver(model)\n\n# Define the initial conditions and time steps\nx0 = 5\ny0 = 1\ntime_steps = np.linspace(0, 20, 1000)\n\n# Solve the system\ntrajectory = solver.solve(x0, y0, time_steps)\n\nx_values = trajectory[:, 0].detach().numpy()\ny_values = trajectory[:, 1].detach().numpy()\n\nplt.plot(time_steps, x_values, label='Prey')\nplt.plot(time_steps, y_values, label='Predator')\nplt.xlabel('Time')\nplt.ylabel('Population')\nplt.legend()\nplt.savefig('imgs/lotka_volterra.png')\nplt.show()\n\n\n\n\n\nThe time evolution of the prey and predator populations.\n\n\n\n\nWe can additionally look at the phase space of the system for various initial conditions to see how the different solutions are periodic.\n\n\nShow the code\n# Define the initial conditions\nx0 = 5\ny0 = [.2,.5,1, 2, 3, 4, 5]\n\n# Create the model and solver\nmodel = LotkaVolterraModel(alpha, beta, gamma, delta)\nsolver = RK4Solver(model)\n\n# Define the time steps\ntime_steps = np.linspace(0, 10, 1000)\n\n# Plot the phase space\nplt.figure(figsize=(8, 6))\nfor y in y0:\n    trajectory = solver.solve(x0, y, time_steps)\n    x_values = trajectory[:, 0].detach().numpy()\n    y_values = trajectory[:, 1].detach().numpy()\n    plt.plot(x_values, y_values, label=f'y0={y}')\n\nplt.xlabel('Prey Population')\nplt.ylabel('Predator Population')\nplt.legend()\nplt.title('Lotka-Volterra Phase Space')\nplt.savefig('imgs/lotka_volterra_phase_space.png')\nplt.show()\n\n\n\n\n\nThe phase space of the predator-prey model."
  },
  {
    "objectID": "content/eosc555/lectures/lecture5/index.html#the-inverse-problem",
    "href": "content/eosc555/lectures/lecture5/index.html#the-inverse-problem",
    "title": "Lecture 5: Autodiff and Gauss Newton Optimization",
    "section": "The Inverse Problem",
    "text": "The Inverse Problem\nThe inverse problem in this case is to find the parameters \\(\\alpha, \\beta, \\gamma, \\delta\\) that best fit the data. We suppose that we have a model with parameters that takes in the initial conditions and time steps and returns the trajectory of the system.\n\\[\\frac{d \\vec{x}}{dt} = f(\\vec{x}; \\vec{p}), \\quad \\vec{x}_0 = \\vec{x}(0)\\]\nwhere \\(\\vec{x}\\) is the state of the system and \\(\\vec{p}\\) are the parameters. The goal is to form an esimate of \\(\\vec{p}\\), while the data that we have collected may be sparse, noisy, or incomplete. We represent the incompleteness in the data using the \\(Q\\) sampling operator which is applied to the true underlying data to give \\(Qx\\). If \\(x\\) is fully given then \\(Q=I\\).\n\\[f(\\vec{x}; \\vec{p}) \\cong \\frac{x_{n+1} - x_n}{\\Delta t}\\]\nand we can apply non-linear least squares to try and produce a fit. \\(F(\\vec{p}, x_0) = x(t, \\vec{p})\\) and the observed data is \\(Qx(t)\\). We also make an assumption here that \\(F\\) does not depend on the particular solver that we are using for the forward ODE and that all of the \\(p\\) are physical parameters, we assume that the parameters are faithful enough.\n\\[ \\min_{\\vec{p}} \\frac{1}{2}\\|QF(p)-d\\|^2 = \\min_{p}\\|G(\\vec{p})\\|^2\\]\nSo what we have is a non-linear least squares problem, where we are trying to minimize some mean squared error of a function of the parameters \\(p\\) and the data. The data is fixed for a given problem, so it is only the optimal \\(p\\) that we are trying to find."
  },
  {
    "objectID": "content/eosc555/lectures/lecture5/index.html#minimization-of-the-objective-function",
    "href": "content/eosc555/lectures/lecture5/index.html#minimization-of-the-objective-function",
    "title": "Lecture 5: Autodiff and Gauss Newton Optimization",
    "section": "Minimization of the Objective Function",
    "text": "Minimization of the Objective Function\n\\[ \\min_{p \\in \\mathbb{R}^m} \\biggl\\{ \\sum_{i=1}^n (QF_i(\\mathbf{p}) - d_i) ^2\\biggr\\}\\]\nwhere \\(d_i\\) is the observed data. This is the same as\n\\[ \\min_{p \\in \\mathbb{R}^m} \\|G(\\mathbf{p})\\|^2\\]\nwhere \\(G(\\mathbf{p}) = QF(\\mathbf{p}) - d\\) and \\(d \\in \\mathbb{R}^n\\). We are minimizing the norm of a non-linear function of the parameters. Supposing that we want to find the minimizer, one approach would be by gradient descent.\n\nThe Jacobian: A quick review\n\nThe Jacobian is a multivariate extension of the derivative that extends to functions \\(f : \\mathbb{R}^m \\to \\mathbb{R}^n\\). Because there are \\(n\\) function outputs and \\(m\\) input variables, the Jacobian is an \\(n \\times m\\) matrix that contains the information of how each of the \\(n\\) functions changes with respect to each of the \\(m\\) variables. In an abuse of notation, it can be seen as \\(\\frac{\\partial \\vec{f}}{\\partial \\vec{x}}\\).\n\\[\n\\mathbf{J_f} =\n\\left[\n    \\frac{\\partial f}{\\partial x_1} \\cdots \\frac{\\partial f}{\\partial x_n}\n\\right]\n=\n\\begin{bmatrix}\n    \\nabla^\\top f_1\n    \\\\\n    \\vdots\n    \\\\\n    \\nabla^\\top f_m\n\\end{bmatrix}\n=\n\\left[\n    \\begin{array}{ccc}\n    \\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    \\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n    \\end{array}\n\\right]\n\\]\nNote that like the derivative, the Jacobian is a function of the input variables. The Jacobian is a linear approximation of the function \\(f\\) at a point \\(x_0\\) and can be used to approximate the function at a point \\(x_0 + \\Delta x\\).\n\\[ f(x_0 + \\Delta x) \\approx f(x_0) + J_f(x_0) \\Delta x\\]\nNoting that we are applying matrix multiplication using \\(J_f\\) evaluated at \\(x_0\\) and the vector \\(\\Delta x = \\vec{x} - \\vec{x_0}\\).\n\nWe can compute the gradient of \\(\\|G(\\mathbf{p})\\|^2\\)\n\\[\\begin{align}\n\\nabla_p \\|G(p)\\|^2 &= \\nabla_p G(p)^T G(p)\\\\\n&= \\sum_{i=1}^n \\nabla_p G_i(p)^2\\\\\n&= \\sum_{i=1}^n 2 G_i(p) \\nabla_p G_i(p)\\\\\n&= 2 J_G(p)^T G(p)\n\\end{align}\n\\]\nFrom this stage we could apply gradient descent to find the minimum of the function. However, the function \\(G(p)\\) is non-linear and so the gradient descent method may not converge quickly or the problem may have poor conditioning. The celebrated Newton’s Method addresses some of these issues, but requires computing the Hessian \\(\\nabla^2 \\|G(p)\\|^2\\) of the function, which can be expensive.\nThe true Hessian of the function is: \\[\n\\nabla^2 \\|G(p)\\|^2 = 2 J_G(p)^T J_G(p) + 2 \\sum_{i=1}^n G_i(p) \\nabla^2 G_i(p)\n\\]\nSo we’d have to compute the Hessian of each of the \\(G_i(p)\\) functions, of which there are \\(n\\), not good in practice. If we did have this Hessian, then the steps with Newton’s method would be:\n\\[ p_{k+1} = p_k - (\\nabla^2 \\|G(p_k)\\|^2)^{-1} \\nabla (\\|G(p_k)\\|^2)\\]"
  },
  {
    "objectID": "content/eosc555/lectures/lecture5/index.html#gauss-newton-optimization",
    "href": "content/eosc555/lectures/lecture5/index.html#gauss-newton-optimization",
    "title": "Lecture 5: Autodiff and Gauss Newton Optimization",
    "section": "Gauss-Newton Optimization",
    "text": "Gauss-Newton Optimization\nRather than solve this problem directly we can linearize inside of the norm and solve the linearized problem using the normal equations. We approximate the function \\(G(p) = QF(p) - d\\) by a linear function \\(G(p) \\approx (QF(p_k) - d) + QJ_k(p-p_k)\\) where \\(J_k\\) is the Jacobian of \\(F(p)\\) at \\(p_k\\).\n\\[ \\min_{p \\in \\mathbb{R}^m} \\|QF(p_k) - d + QJ_k(p-p_k)\\|^2\\]\nThen rearranging this we get a form that is a linear least squares problem:\n\\[ \\begin{align}\n& \\min_{p \\in \\mathbb{R}^m} \\| QJ_kp - (d + QJ_k p_k- QF(p_k) )\\|^2\\\\\n=& \\min_{p \\in \\mathbb{R}^m} \\| Ap - r_k\\|^2\\\\\nA^T A p =& A^T r_k\n\\end{align}\n\\]\nwhere \\(A = QJ_k\\) and \\(r_k = d + QJ_k p_k - QF(p_k)\\). This is the normal equations for the linear least squares problem. This gives us\n\\[\n\\begin{align}\np_{k+1} &= (A^T A)^{-1} A^T (r_k)\\\\\n&= (J_k^T Q^T Q J_k)^{-1} J_k^T Q^T (d + QJ_k p_k - QF(p_k))\\\\\n&= p_k + (J_k^T Q^T Q J_k)^{-1} J_k^T Q^T (d - QF(p_k))\\\\\np_{k+1} &= p_k - (J_k^T Q^T Q J_k)^{-1} J_k^T Q^T (QF(p_k) - d)\\\\\n\\end{align}\n\\]\nThis could be written in a more tidy and general way, reacalling that \\(G(p) = QF(p) - d\\) and let \\(J_G(p) = QJ(p)\\), then we have:\n\\[ p_{k+1} = p_k - (J_G(p_k)^TJ_G(p_k))^{-1} J_G(p_k) G(p_k)\\]\n\nComparison with Newton’s Method\nSo this resembles a scaled gradient descent. In Newton’s method we have the Hessian, in Gauss-Newton we have the Jacobian of the function. As a comparison:\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nNewton’s Method\nStep Update\n\n\n\n\\(p_{k+1} = p_k - (\\nabla^2 \\|G(p_k)\\|^2)^{-1} \\nabla \\|G(p_k)\\|^2\\)\n\n\n\nScaling Matrix\n\n\n\n\\(\\nabla^2 \\|G(p_k)\\|^2 = 2 J_G(p_k)^T J(p_k) + 2 \\sum_{i=1}^n G_i(p_k) \\nabla^2 G_i(p_k)\\)\n\n\nGauss-Newton Method\nStep Update\n\n\n\n\\(p_{k+1} = p_k - (J_G(p_k)^T J_G(p_k))^{-1} J_G(p_k)^T G(p_k)\\)\n\n\n\nScaling Matrix\n\n\n\n\\(J_G(p_k)^T J_G(p_k)\\)\n\n\n\nThe direction of change between iterations in Newton’s method can be rewritten as \\[d_k = \\left(J_G(p_k)^T J(p_k) + \\sum_{i=1}^n G_i(p_k) \\nabla^2 G_i(p_k)\\right)^{-1} J_G(p_k)^T G(p_k)\\]\nWhile the direction in the case of Gauss-Newton is \\[d_k = \\left(J_G(p_k)^T J(p_k)\\right)^{-1} J_G(p_k)^T G(p_k)\\]\nSo we can see that the difference between the two is the omission of the computationally expensive \\(\\sum_{i=1}^n G_i(p) \\nabla^2 G_i(p)\\) terms. The Gauss-Newton method is approximating the second-order approach of Newton’s method by only considering the first-order terms inside of the norm.\n\\[J_G(p_k)^T J(p_k) \\sum_{i=1}^n G_i(p_k) \\nabla^2 G_i(p_k) \\approx J_G(p_k)^T J(p_k)\\]\nRecall that \\(G(p) = QF(p) - d\\) which is the difference between the observed data and the model. If the difference is small then \\(G_i\\) is also small and the approximation is good.\n\n\nAlgorithm for Gauss-Newton\nWe have derived the algorithm for the Gauss-Newton method for solving the non-linear least squares problem. The algorithm is as follows:\n\n\n\\begin{algorithm} \\caption{Gauss-Newton Algorithm for Non-linear Least Squares}\\begin{algorithmic} \\State \\textbf{Input:} Initial guess $p_0$, maximum iterations $K$, tolerance $\\epsilon$ \\State \\textbf{Initialize} $p_0$ \\For{$k = 0, 1, 2, \\ldots$} \\State Compute the Jacobian $J_G$ of $G(p)$ at $p_k$ \\State Compute the transpose $J_G^T$ of the Jacobian \\State Compute the residual $r_k =G(p_k)$ (forward model) \\State Compute the step $s_k = (J_G(p_k)^T J_G(p_k) )^{-1} J_G(p_k) r_k$ \\State Update the parameters $p_{k+1} = p_k + \\mu_k s_k$ \\If{$\\|s_k\\| &lt; \\epsilon$} \\State \\textbf{Stop} \\EndIf \\EndFor \\State \\textbf{Output:} $p_{k+1}$ as the optimal solution \\end{algorithmic} \\end{algorithm}\n\n\n\n\nMatrix Inversions\nIn practice it may be computationally expensive to invert the matrix \\(J_k^T Q^T Q J_k\\). We can use a conjugate gradient method to solve the normal equations instead. \\[J_k^T Q^T Q J_k s_k = J_k^T Q^T r_k\\]\nWe developed a conjugate gradient method in the last lecture, so we can use that along with the computed values for \\(J_k^T, J_k, r_k\\) to solve the normal equations and get the step \\(s_k\\).\n\n\nSummary\n\n\n\n\n\n\n\n\nComponent\nDescription\nDimensions\n\n\n\n\n\\(d\\)\nObserved data\n\\(\\mathbb{R}^{n}\\)\n\n\n\\(p_k\\)\nParameters\n\\(\\mathbb{R}^{m}\\)\n\n\n\\(Q\\)\nWeight matrix\n\\(\\mathbb{R}^{n \\times n}\\)\n\n\n\\(J_k\\)\nJacobian of \\(F(p_k)\\)\n\\(\\mathbb{R}^{n \\times m}\\)\n\n\n\\(r_k\\)\nResidual \\(d - QF(p_k)\\)\n\\(\\mathbb{R}^{n}\\)\n\n\n\\(F(p_k)\\)\nForward model output\n\\(\\mathbb{R}^{n}\\)\n\n\n\\(s_k\\)\nStep direction\n\\(\\mathbb{R}^{m}\\)\n\n\n\\(J_k^T\\)\nTranspose of the Jacobian\n\\(\\mathbb{R}^{m \\times n}\\)\n\n\n\\(J_k^T Q^T Q J_k\\)\nNormal equations matrix\n\\(\\mathbb{R}^{m \\times m}\\)"
  },
  {
    "objectID": "content/eosc555/lectures/lecture5/index.html#automatic-differentiation",
    "href": "content/eosc555/lectures/lecture5/index.html#automatic-differentiation",
    "title": "Lecture 5: Autodiff and Gauss Newton Optimization",
    "section": "Automatic Differentiation",
    "text": "Automatic Differentiation\nIn practice the Jacobian and its transpose can be computed using automatic differentiation.\nTake the forward model \\(F(p)\\) for which we want a Jacobian matrix at \\(p_k\\). We can write the Taylor expansion of the forward model as:\n\\[ F(p_k + \\epsilon v) = F(p_k) + J_k \\epsilon v + \\mathcal{O}(\\epsilon^2)\\]\nwhere \\(J_k\\) is the Jacobian of \\(F(p_k)\\). If we take the derivative of both sides in this expansion with respect to \\(\\epsilon\\) we get:\n\\[ \\frac{d}{d \\epsilon} F(p_k + \\epsilon v) = J_k v + \\mathcal{O}(\\epsilon)\\]\nIf we make \\(\\epsilon\\) very small then the Jacobian of the forward problem can be numerically approximated and bounded by a small \\(\\mathcal{O}(\\epsilon)\\). The next step to fully recover the Jacobian is to take the gradient with respect to \\(v\\) of the left-hand side of the equation.\n\\[ \\nabla_v \\frac{d}{d \\epsilon} F(p_k + \\epsilon v) = J_k\\]\nThe gradient with respect to \\(v\\) can be traced through with automatic differentiation. So we apply a chain of operations, the pytorch Jacobian vector product, followed by backpropagation on a surrogate \\(v\\) that was passed to the function to get the Jacobian of the forward model. The same principles can be used to recover \\(J_k^T\\).\nThere is also the direct method that is avaible for computing the Jacobian matrix using the torch library. Both cases are shown below. Note that the tensors have a requires_grad=True flag set to allow for the gradients to be computed, it indicates that the tensor is part of the computational graph for backpropagation and tracing by how much each element of \\(v\\) contributed to the jvp result.\n\n\nShow the code\nimport torch\nfrom torch.autograd.functional import jvp\nfrom torch.autograd.functional import jacobian\n\n# Define a simple forward function\ndef F(p):\n    return torch.stack([p[0]**2 + p[1], p[1]**3 + p[0]])\n\n# Input point p_k\np_k = torch.tensor([1.0, 1.0])\n\n# Arbitrary vector v, same size as p_k\nv = torch.tensor([1.0,1.0], requires_grad=True)\n\n# Compute the Jacobian-vector product (J(p) * v)\nF_output, jvp_result = jvp(F, (p_k,), v, create_graph=True)\nprint(\"Function output:\")\nprint(F_output)\nprint(\"Jacobian-vector product:\")\nprint(jvp_result)\n\n# Initialize a list to store each row of the Jacobian\njacobian_rows = []\n# Compute the gradient of each component of the JVP result separately, retaining the graph to avoid re-computation\nfor i in range(F_output.shape[0]):\n    v.grad = None  # Clear the gradient\n    jvp_result.backward(torch.tensor([1.0 if i == j else 0.0 for j in range(F_output.shape[0])]), retain_graph=True)\n    jacobian_rows.append(v.grad.clone())  # Append the gradient (row of the Jacobian)\n\n# Stack the rows to get the full Jacobian matrix\njacobian_matrix = torch.stack(jacobian_rows, dim=0)\n\n# Print the Jacobian matrix\nprint(\"Jacobian matrix at p_k:\")\nprint(jacobian_matrix)\n\n# Compute the full Jacobian matrix directly\njacobian_matrix = jacobian(F, p_k)\n\n# Print the Jacobian matrix\nprint(\"Jacobian matrix at p_k:\")\nprint(jacobian_matrix)\n\n\nFunction output:\ntensor([2., 2.], grad_fn=&lt;StackBackward0&gt;)\nJacobian-vector product:\ntensor([3., 4.], grad_fn=&lt;AddBackward0&gt;)\nJacobian matrix at p_k:\ntensor([[2., 1.],\n        [1., 3.]])\nJacobian matrix at p_k:\ntensor([[2., 1.],\n        [1., 3.]])"
  },
  {
    "objectID": "content/eosc555/lectures/lecture3/index.html",
    "href": "content/eosc555/lectures/lecture3/index.html",
    "title": "Lecture 3: Image Denoising with Gradient Descent and Early Stopping",
    "section": "",
    "text": "Often times we wish to find the gradient of a multi-variable function that is formulated as a linear algebra operation. In this case there are some useful “vector” derivatives and rules that can simplify the process of calculating more complex expressions. The gradient with respect to vector \\(\\mathbf{x}\\) is generally denoted as \\(\\nabla_{\\mathbf{x}}\\) or alternatively \\(\\partial_{\\mathbf{x}}\\), somewhat of an abuse of notation.\n\n\n\\[\\phi(x) = a^\\top x = \\sum_i a_i x_i\\]\nThis is a vector dotproduct and the gradient is simply the vector \\(a\\). There is a subtlety here in that the vector is usually transposed to be a column vector, but this is not always the case. Some people in the field of statistics prefer to use row vector, this can cause some confusion. The general convention is a column vector.\n\\[\\nabla_{\\mathbf{x}} \\phi = a\\]\n\n\n\n\\[\\phi(x) = Ax\\]\nBased on the previous process we are expecting to potentially get \\(A^\\top\\) as the gradient, however the transpose does not occur in this case because we are not returning a vector that needs to be reshaped into a column form.\n\\[\\nabla_{\\mathbf{x}} \\phi = A\\]\n\n\n\nOften we may encounter quadratic linear functions that are of the form: \\[ \\phi(x) = x^\\top A x\\]\nOne way to determine the gradient is to expand the expression and evaluate for a single \\(\\frac{\\partial}{\\partial x_i}\\) term. This method can be found at Mark Schmidt Notes Instead we can apply a chain rule for matrix differentiation that is based on the product rule for differentiation. The chain rule for matrix differentiation is as follows:\n\\[\\frac{d f(g,h)}{d x} = \\frac{d (g(x)^\\top)}{d x} \\frac{\\partial f(g,h)}{\\partial g} + \\frac{d (h(x)^\\top)}{d x} \\frac{\\partial f(g,h)}{\\partial h}\\]\n\\[ \\begin {align*}\n\\phi(x) &= x^\\top A x \\\\\n\\nabla_{\\mathbf{x}} \\phi &= \\nabla_{\\mathbf{x}} (x^\\top A x) \\\\\n&= \\nabla_{\\mathbf{x}} x^\\top (A x) =  \\nabla_{\\mathbf{x}} x^\\top y\\\\\n&= (\\nabla_{\\mathbf{x}} x) \\nabla_{\\mathbf{x}} x^\\top y + \\nabla_{\\mathbf{x}} y^\\top \\nabla_{\\mathbf{y}} x^\\top y\\\\\n&= I y + \\nabla_{\\mathbf{x}} (x^\\top A^\\top) x\\\\\n&= (A x) + A^\\top x\\\\\n&= (A + A^\\top) x\n\\end {align*}\n\\]\nThis fits with the generalization for a scalar quadratic form where we end up with \\((cx^2)' = (c + c^\\top)x = 2cx\\) where \\(c\\) is a scalar.\n\n\n\nAnother form of interest is the hadamard product of two vectors. \\[\\phi(x) = (Ax)^2 = Ax \\odot Ax\\]\nFor this one let \\(y=Ax\\) and we can index each element of the vector \\(y\\) as \\(y_i = \\sum_j A_{ij} x_j\\). The hadamard product is a vector \\(z\\) where \\(z_i = y_i^2\\), we can compute the jacobian since now we are taking the gradient with respect to a vector.\nThe Jacobian will contain the partial derivatives:\n\\[\\frac{d\\vec{z}}{d\\vec{x}} = \\begin{bmatrix} \\frac{\\partial z_1}{\\partial x_1} & \\frac{\\partial z_1}{\\partial x_2} & \\cdots & \\frac{\\partial z_1}{\\partial x_n} \\\\\n\\frac{\\partial z_2}{\\partial x_1} & \\frac{\\partial z_2}{\\partial x_2} & \\cdots & \\frac{\\partial z_2}{\\partial x_n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial z_n}{\\partial x_1} & \\frac{\\partial z_n}{\\partial x_2} & \\cdots & \\frac{\\partial z_n}{\\partial x_n} \\end{bmatrix}\n\\]\nIf we can recover this then we have the gradient of the hadamard product.\n\\[\n\\begin{align*}\nz_i &= y_i^2 = \\left( \\sum_j A_{ij} x_j \\right)^2\\\\\n\\frac{\\partial}{\\partial x_j} y_i^2 &= 2 y_i \\frac{\\partial y_i}{\\partial x_j} = 2 y_i A_{ij}\\\\\n\\frac{d\\vec{z}}{d\\vec{x}} &= 2 \\begin{bmatrix} y_1 A_{1j} & y_1 A_{2j} & \\cdots & y_1 A_{nj} \\\\\ny_2 A_{1j} & y_2 A_{2j} & \\cdots & y_2 A_{nj} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ny_n A_{1j} & y_n A_{2j} & \\cdots & y_n A_{nj} \\end{bmatrix}\\\\\n&= 2 \\cdot \\text{diag}(\\vec{y})A\\\\\n&= 2 \\cdot \\text{diag}(Ax)A\n\\end{align*}\n\\]\n\n\n\nWe look at taking the gradient of the expansion of least squares to find the gradient for this optimization objective.\n\\[\\phi(x) = \\frac{1}{2} ||Ax - b||^2 = \\frac{1}{2} (x^\\top A^\\top A x - 2 b^\\top A x + b^\\top b)\\]\n\\[ \\begin{align*}\n\\nabla_{\\mathbf{x}} \\phi &= \\nabla_{\\mathbf{x}} \\left( \\frac{1}{2} (x^\\top A^\\top A x - 2 b^\\top A x + b^\\top b) \\right)\\\\\n&= \\nabla_{\\mathbf{x}} \\left( \\frac{1}{2} x^\\top A^\\top A x \\right) - \\nabla_{\\mathbf{x}} \\left( b^\\top A x \\right)\\\\\n&= \\frac{1}{2} (A^\\top A + A^\\top A) x - A^\\top b\\\\\n&= A^\\top A x - A^\\top b\\\\\n\\end{align*}\n\\]\nReturning to the first-order optimality condition we have: \\[A^\\top A x = A^\\top b\\]\nAt which point it is in question if \\(A^\\top A\\) is invertible. The invertibility of \\(A^\\top A\\) is determined by the rank of \\(A\\). The rank of A for a non-square matrix is the number of independent columns. If we examine \\(A^\\top Ax = 0\\) then we see that this is only true where the range of \\(A\\) is in the nullspace of \\(A^\\top\\). But \\(N(A^\\top) = R(A)^\\perp\\) so they are orthogonal subspaces and will never coincide unless \\(Ax=0\\). So then \\(A^\\top A x = 0\\) implies that \\(Ax = 0\\) which means that if the null space of \\(A=\\{0\\}\\) then the null space of \\(A^\\top A = \\{0\\}\\) and \\(A^\\top A\\) is invertible. Since \\(A^\\top A\\) is symmetric and positive definite, it is invertible.\n\\(A^\\top A\\) is invertible \\(\\iff\\) \\(A\\) is full rank, that is all the columns are independent. For non-square matrices, an \\(m&gt;n\\) matrix that is wide will trivially not satisfy this condition. A tall matrix \\(m&lt;n\\) will satisfy the condition if the columns are independent."
  },
  {
    "objectID": "content/eosc555/lectures/lecture3/index.html#derivations-of-linear-algebra-gradients",
    "href": "content/eosc555/lectures/lecture3/index.html#derivations-of-linear-algebra-gradients",
    "title": "Lecture 3: Image Denoising with Gradient Descent and Early Stopping",
    "section": "",
    "text": "Often times we wish to find the gradient of a multi-variable function that is formulated as a linear algebra operation. In this case there are some useful “vector” derivatives and rules that can simplify the process of calculating more complex expressions. The gradient with respect to vector \\(\\mathbf{x}\\) is generally denoted as \\(\\nabla_{\\mathbf{x}}\\) or alternatively \\(\\partial_{\\mathbf{x}}\\), somewhat of an abuse of notation.\n\n\n\\[\\phi(x) = a^\\top x = \\sum_i a_i x_i\\]\nThis is a vector dotproduct and the gradient is simply the vector \\(a\\). There is a subtlety here in that the vector is usually transposed to be a column vector, but this is not always the case. Some people in the field of statistics prefer to use row vector, this can cause some confusion. The general convention is a column vector.\n\\[\\nabla_{\\mathbf{x}} \\phi = a\\]\n\n\n\n\\[\\phi(x) = Ax\\]\nBased on the previous process we are expecting to potentially get \\(A^\\top\\) as the gradient, however the transpose does not occur in this case because we are not returning a vector that needs to be reshaped into a column form.\n\\[\\nabla_{\\mathbf{x}} \\phi = A\\]\n\n\n\nOften we may encounter quadratic linear functions that are of the form: \\[ \\phi(x) = x^\\top A x\\]\nOne way to determine the gradient is to expand the expression and evaluate for a single \\(\\frac{\\partial}{\\partial x_i}\\) term. This method can be found at Mark Schmidt Notes Instead we can apply a chain rule for matrix differentiation that is based on the product rule for differentiation. The chain rule for matrix differentiation is as follows:\n\\[\\frac{d f(g,h)}{d x} = \\frac{d (g(x)^\\top)}{d x} \\frac{\\partial f(g,h)}{\\partial g} + \\frac{d (h(x)^\\top)}{d x} \\frac{\\partial f(g,h)}{\\partial h}\\]\n\\[ \\begin {align*}\n\\phi(x) &= x^\\top A x \\\\\n\\nabla_{\\mathbf{x}} \\phi &= \\nabla_{\\mathbf{x}} (x^\\top A x) \\\\\n&= \\nabla_{\\mathbf{x}} x^\\top (A x) =  \\nabla_{\\mathbf{x}} x^\\top y\\\\\n&= (\\nabla_{\\mathbf{x}} x) \\nabla_{\\mathbf{x}} x^\\top y + \\nabla_{\\mathbf{x}} y^\\top \\nabla_{\\mathbf{y}} x^\\top y\\\\\n&= I y + \\nabla_{\\mathbf{x}} (x^\\top A^\\top) x\\\\\n&= (A x) + A^\\top x\\\\\n&= (A + A^\\top) x\n\\end {align*}\n\\]\nThis fits with the generalization for a scalar quadratic form where we end up with \\((cx^2)' = (c + c^\\top)x = 2cx\\) where \\(c\\) is a scalar.\n\n\n\nAnother form of interest is the hadamard product of two vectors. \\[\\phi(x) = (Ax)^2 = Ax \\odot Ax\\]\nFor this one let \\(y=Ax\\) and we can index each element of the vector \\(y\\) as \\(y_i = \\sum_j A_{ij} x_j\\). The hadamard product is a vector \\(z\\) where \\(z_i = y_i^2\\), we can compute the jacobian since now we are taking the gradient with respect to a vector.\nThe Jacobian will contain the partial derivatives:\n\\[\\frac{d\\vec{z}}{d\\vec{x}} = \\begin{bmatrix} \\frac{\\partial z_1}{\\partial x_1} & \\frac{\\partial z_1}{\\partial x_2} & \\cdots & \\frac{\\partial z_1}{\\partial x_n} \\\\\n\\frac{\\partial z_2}{\\partial x_1} & \\frac{\\partial z_2}{\\partial x_2} & \\cdots & \\frac{\\partial z_2}{\\partial x_n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial z_n}{\\partial x_1} & \\frac{\\partial z_n}{\\partial x_2} & \\cdots & \\frac{\\partial z_n}{\\partial x_n} \\end{bmatrix}\n\\]\nIf we can recover this then we have the gradient of the hadamard product.\n\\[\n\\begin{align*}\nz_i &= y_i^2 = \\left( \\sum_j A_{ij} x_j \\right)^2\\\\\n\\frac{\\partial}{\\partial x_j} y_i^2 &= 2 y_i \\frac{\\partial y_i}{\\partial x_j} = 2 y_i A_{ij}\\\\\n\\frac{d\\vec{z}}{d\\vec{x}} &= 2 \\begin{bmatrix} y_1 A_{1j} & y_1 A_{2j} & \\cdots & y_1 A_{nj} \\\\\ny_2 A_{1j} & y_2 A_{2j} & \\cdots & y_2 A_{nj} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ny_n A_{1j} & y_n A_{2j} & \\cdots & y_n A_{nj} \\end{bmatrix}\\\\\n&= 2 \\cdot \\text{diag}(\\vec{y})A\\\\\n&= 2 \\cdot \\text{diag}(Ax)A\n\\end{align*}\n\\]\n\n\n\nWe look at taking the gradient of the expansion of least squares to find the gradient for this optimization objective.\n\\[\\phi(x) = \\frac{1}{2} ||Ax - b||^2 = \\frac{1}{2} (x^\\top A^\\top A x - 2 b^\\top A x + b^\\top b)\\]\n\\[ \\begin{align*}\n\\nabla_{\\mathbf{x}} \\phi &= \\nabla_{\\mathbf{x}} \\left( \\frac{1}{2} (x^\\top A^\\top A x - 2 b^\\top A x + b^\\top b) \\right)\\\\\n&= \\nabla_{\\mathbf{x}} \\left( \\frac{1}{2} x^\\top A^\\top A x \\right) - \\nabla_{\\mathbf{x}} \\left( b^\\top A x \\right)\\\\\n&= \\frac{1}{2} (A^\\top A + A^\\top A) x - A^\\top b\\\\\n&= A^\\top A x - A^\\top b\\\\\n\\end{align*}\n\\]\nReturning to the first-order optimality condition we have: \\[A^\\top A x = A^\\top b\\]\nAt which point it is in question if \\(A^\\top A\\) is invertible. The invertibility of \\(A^\\top A\\) is determined by the rank of \\(A\\). The rank of A for a non-square matrix is the number of independent columns. If we examine \\(A^\\top Ax = 0\\) then we see that this is only true where the range of \\(A\\) is in the nullspace of \\(A^\\top\\). But \\(N(A^\\top) = R(A)^\\perp\\) so they are orthogonal subspaces and will never coincide unless \\(Ax=0\\). So then \\(A^\\top A x = 0\\) implies that \\(Ax = 0\\) which means that if the null space of \\(A=\\{0\\}\\) then the null space of \\(A^\\top A = \\{0\\}\\) and \\(A^\\top A\\) is invertible. Since \\(A^\\top A\\) is symmetric and positive definite, it is invertible.\n\\(A^\\top A\\) is invertible \\(\\iff\\) \\(A\\) is full rank, that is all the columns are independent. For non-square matrices, an \\(m&gt;n\\) matrix that is wide will trivially not satisfy this condition. A tall matrix \\(m&lt;n\\) will satisfy the condition if the columns are independent."
  },
  {
    "objectID": "content/eosc555/lectures/lecture3/index.html#gradient-descent-analysis",
    "href": "content/eosc555/lectures/lecture3/index.html#gradient-descent-analysis",
    "title": "Lecture 3: Image Denoising with Gradient Descent and Early Stopping",
    "section": "Gradient Descent Analysis",
    "text": "Gradient Descent Analysis\nThe standard form of the gradient descent algorithm comes from the field of optimization and can be written as:\n\\[ x_{k+1} = x_k - \\alpha \\nabla_x \\phi(x_k)\\]\nWhere \\(\\alpha\\) is the learning rate, which can be dependent on the problem and the gradient. Substituting the gradient of the least squares problem we have:\n\\[ \\begin{align}\nx_{k+1} &= x_k - \\alpha (A^\\top A x_k - A^\\top b)\\\\\n\\frac{x_{k+1}-x_k}{\\alpha} &= A^\\top b - A^\\top A x_k\\\\\n\\lim_{\\alpha \\to 0} \\frac{x_{k+1}-x_k}{\\alpha} &= \\frac{dx}{dt} = A^\\top (b -A x), \\quad x(0) = x_0\n\\end{align}\n\\]\nThis ODE is the continuous version of the gradient descent algorithm, also known as the gradient flow. Since this a linear first-order ODE we can solve it analytically. The general method for a linear system ODE would be to find the homogeneous solution and the particular solution:\n\\[ \\begin{align}\nx' + A^\\top A x &= A^\\top b\\\\\n\\text{Guess:} x &= v e^{\\lambda t}\\\\\n\\lambda v e^{\\lambda t} + A^\\top A v e^{\\lambda t} &= A^\\top b e^{\\lambda t}\\\\\n\\lambda v + A^\\top A v &= 0 \\qquad \\text{Homogeneous}\\\\\n(\\lambda I + A^\\top A) v &= 0\\\\\n\\lambda &= \\text{eigenvalues of } A^\\top A, \\quad v = \\text{eigenvectors of } A^\\top A\n\\end{align}\n\\]\nBefore continuing further with this line, we can see that the solutions will be closely related to the SVD because it contains the information on these eigenvalues and vectors. So we can try to solve the ODE with the SVD.\n\nSolving the ODE with SVD\n\\[\\begin{align}\nA &= U \\Sigma V^\\top\\\\\nA^TA &= V \\Sigma^2 V^\\top\\\\\n\\frac{d}{dt}x &= V \\Sigma U^\\top b - V \\Sigma^2 V^\\top x\\\\\n\\end{align}\n\\]\nNow let \\(z = V^\\top x\\) and \\(\\hat b = U ^ \\top b\\) then we have:\n\\[\\begin{align}\n\\frac{d}{dt} (V^\\top x) &= \\Sigma \\hat b - \\Sigma^2 (V^\\top x)\\\\\n\\frac{d}{dt} z &= \\Sigma \\hat b - \\Sigma^2 z\\\\\nz' + \\Sigma^2 z &= \\Sigma \\hat b\\\\\n\\end{align}\n\\]\nAt this stage since everything has been diagonalized, all of the equations are decoupled and independent so we can solve for the \\(\\lambda_i\\) cases independently. We find the homogeneous \\(z_h\\) and particular \\(z_p\\) solutions:\n\\[\n\\begin{align}\nz_h' + \\lambda^2 z_h &= 0\\\\\nz_h &= c e^{-\\lambda^2 t}\\\\\nz_p' + \\lambda^2 z_p &= \\lambda \\hat b\\\\\nz_p &= D \\hat b \\\\\n\\lambda^2 D \\hat b &= \\lambda \\hat b\\\\\nD &= \\frac{1}{\\lambda}\\\\\nz_p &= \\frac{1}{\\lambda} \\hat b\n\\end{align}\n\\]\nSo the general solution for the \\(i^{th}\\) component is:\n\\[z_i = c_i e^{-\\lambda_i^2 t} + \\frac{1}{\\lambda_i} \\hat b_i\\]\nSupposing that we start at \\(x=0\\) then we have \\(z=0\\) at all elements and can solve the coefficients \\(c_i\\):\n\\[c_i = -\\frac{1}{\\lambda_i} \\hat b_i\\]\nThen putting it all back together with all the equations we have that\n\\[Z = \\text{diag}\\left( \\lambda_i^{-1} (1 - \\exp (-\\lambda_i t)) \\right) \\hat b\\]\nSubstituting back in for \\(x\\) and \\(b\\) we get:\n\\[x = V \\text{diag}\\left( \\lambda_i^{-1} (1 - \\exp (-\\lambda_i t)) \\right) U^\\top b\\]\nIf we stare at this long enough it begins to look a lot like the pseudoinverse of \\(A\\) from earlier:\n\\(x = V \\Sigma^{-1} U^\\top b\\) except in this case there is a time dependence. At the limit as \\(t \\rightarrow \\infty\\) we have that the exponential term goes to zero and we are left with the pseudoinverse solution. This is a nice way to see that the pseudoinverse is the limit of the gradient descent algorithm. What we may be interested in is what happens at earlier stages since each decay term is dependent on the eigenvalues.\nFor a simple matrix problem we can create a matrix and plot out the time evolution of the diagonals of the matrix that are of interest. In a sense, we have singular values that are time evolving at different rates.\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Seed for reproducibility\nnp.random.seed(4)\n# Create a 5x10 matrix A with random values\nA = np.random.randn(5, 10)\n# Create a vector b of size 5 with random values\nb = np.random.randn(5)\n\n# Compute the SVD of A\nU, S, Vt = np.linalg.svd(A, full_matrices=False)\n\n# Create a time dependent vector of the singular values\ndef St(t):\n    Sdim = S[:, np.newaxis]\n    return (1 - np.exp(-Sdim**2*t)) / Sdim\n\n# Compute the time evolution of the values and plot them on a log scale y axis with a linear time x axis\nt = np.linspace(0, .6, 100)\nT = t[np.newaxis, :]\n\nsingular_vals_t = St(T)\n\n# Initialize the plot\nplt.figure(figsize=(7.5, 4))\n\n# Create a color palette\npalette = sns.color_palette(\"husl\", len(S))\n\n# Plot the singular values and their asymptotes\nfor i in range(len(S)):\n    # Plot the time evolution of each singular value\n    sns.lineplot(x=t, y=singular_vals_t[i, :], color=palette[i], linewidth=2, label=f'$1/S_{i}$ ')\n    \n    Sinv = 1/S[i]\n\n    # Add a horizontal asymptote at the original singular value\n    plt.axhline(y=Sinv, color=palette[i], linestyle='--', linewidth=1)\n    \n    # Annotate the asymptote with the singular value\n    plt.text(t[-1] + 0.02, Sinv, f'{Sinv:.2f}', color=palette[i], va='center')\n\n# Configure plot aesthetics\nplt.xlabel('Time', fontsize=14)\nplt.ylabel('Inverse Singular Vals', fontsize=14)\nplt.title('Time Evolution of Pseudo Inverse in Gradient Flow', fontsize=16)\nplt.legend(title='Inverse Singular Vals', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.xlim(t[0], t[-1] + 0.1)\nplt.tight_layout()\nplt.savefig('imgs/pseudo_inverse_time_evolution.png')\nplt.show()\n\n\n\n\n\n\n\n\n\nSo we can use early stopping to prevent the flow from reaching the optimal point, a very useful technique. When it comes to inverse theory, often we are not interested in the optimal solution, but more interested in getting somewhere close that is not too noisy. This method differs from the thresholded pseudoinverse from the previous lecture, in that it allows some blending of the the smaller singular values, but their propensity for blowing up is controlled by the time exponent and early stopping.\n\n\nExample for Image Recovery using Analytic Solution\nReferring back to the problem of estimating the original image based on a noisy point spread function. We can monitor the time evolution of the estimate using gradient flow. Some code below defines the problem again, with recovery of the SVD decomposition for the 32x32 image, which will be used to solve the ODE for the gradient flow.\n\n\nShow the code\nimport matplotlib.pyplot as plt\nimport matplotlib\n#matplotlib.use('TkAgg')\nimport numpy as np\nimport torch.optim\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nimport copy\n\nimport seaborn as sns\n\nimport math\nimport os\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch.fft\n\nclass gaussianConv(nn.Module):\n    \"\"\"\n    A PyTorch module that applies a Gaussian convolution to an input image using \n    a parameterized Gaussian Point Spread Function (PSF). The PSF is derived \n    from a covariance matrix and the derivatives of the Gaussian are computed \n    for edge detection.\n\n    Args:\n        C (torch.Tensor): Inverse of covariance matrix used to define the shape of the Gaussian.\n        t (float, optional): Scaling factor for the Gaussian, default is np.exp(5).\n        n0 (float, optional): Scaling factor for the original PSF, default is 1.\n        nx (float, optional): Scaling factor for the derivative along the x-axis, default is 1.\n        ny (float, optional): Scaling factor for the derivative along the y-axis, default is 1.\n    \"\"\"\n    def __init__(self, C, t=np.exp(5), n0=1, nx=1, ny=1):\n        super(gaussianConv, self).__init__()\n\n        self.C = C\n        self.t = t\n        self.n0 = n0\n        self.nx = nx\n        self.ny = ny\n\n    def forward(self, image):\n        P, center = self.psfGauss(image.shape[-1], image.device)\n        P_shifted = torch.roll(P, shifts=center, dims=[2, 3])\n        S = torch.fft.fft2(P_shifted)\n        I_fft = torch.fft.fft2(image)\n        B_fft = S * I_fft\n        B = torch.real(torch.fft.ifft2(B_fft))\n\n        return B\n\n    def psfGauss(self, dim, device='cpu'):\n        m = dim\n        n = dim\n\n        # Create a meshgrid of (X, Y) coordinates\n        x = torch.arange(-m // 2 + 1, m // 2 + 1, device=device)\n        y = torch.arange(-n // 2 + 1, n // 2 + 1, device=device)\n        X, Y = torch.meshgrid(x, y, indexing='ij')\n        X = X.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, m, n)\n        Y = Y.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, m, n)\n\n        cx, cy, cxy = self.C[0, 0], self.C[1, 1], self.C[0, 1]\n\n        PSF = torch.exp(-self.t * (cx * X ** 2 + cy * Y ** 2 + 2 * cxy * X * Y))\n        PSF0 = PSF / torch.sum(PSF.abs())\n\n        Kdx = torch.tensor([[-1, 0, 1],\n                            [-2, 0, 2],\n                            [-1, 0, 1]], dtype=PSF0.dtype, device=device) / 4\n        Kdy = torch.tensor([[-1, -2, -1],\n                            [0, 0, 0],\n                            [1, 2, 1]], dtype=PSF0.dtype, device=device) / 4\n\n        Kdx = Kdx.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, 3, 3)\n        Kdy = Kdy.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, 3, 3)\n\n        PSFdx = F.conv2d(PSF0, Kdx, padding=1)\n        PSFdy = F.conv2d(PSF0, Kdy, padding=1)\n\n        PSF_combined = self.n0 * PSF0 + self.nx * PSFdx + self.ny * PSFdy\n\n        center = [1 - m // 2, 1 - n // 2]\n\n        return PSF_combined, center\n\ndim = 32\nx = torch.zeros(1, 1, dim, dim)\nx[:,:, 12:14, 12:14] = 1.0\nx[:,:, 10:12, 10:12] = -1.0\n\nC = torch.tensor([[1, 0],[0, 1]])\nAmv = gaussianConv(C, t=0.1,n0=1, nx=0.1,  ny=0.1)\n\nn=(len(x.flatten()))\nAmat = torch.zeros(n,n)\n\nk=0\nfor i in range(x.shape[-2]):\n  for j in range(x.shape[-1]):\n    e_ij = torch.zeros_like(x)\n    e_ij[:,:, i, j] = 1.0\n    y = Amv(e_ij)\n    Amat[:, k] = y.flatten()\n    k = k+1\n\nU, S, V = torch.svd(Amat.to(torch.float64))\nb = Amv(x)\n\n\nNow that we have the matrix form of the forward operator Amat defined, along with the forward result b and the the decomposition U, S, V we can run the pseudo-inverse gradient flow method as before. So in this case we will be computing:\n\\[ x = V \\text{diag}\\left( \\lambda_i^{-1} (1 - \\exp (-\\lambda_i t)) \\right) U^\\top b\\]\nSince these represents an evolution over time, an animation can be created to show the time evolution of the image recovery, along with the effect of continuing into a region where noise is amplified and dominates.\nRecalling the original and distorted images with a small amount of noise \\(\\epsilon\\) are as follows:\n\n\nShow the code\nplt.figure(figsize=(6, 3))\nplt.subplot(1, 2, 1)\nplt.imshow(x[0, 0], cmap='viridis', vmin=-1, vmax=1)\nplt.title('Original Image')\nplt.axis('off')\nplt.subplot(1, 2, 2)\n\nb_noisy = b+ 0.01 * torch.randn_like(b)\nplt.imshow(b_noisy[0, 0], cmap='viridis', vmin=-1, vmax=1)\nplt.title('Distorted Image')\nplt.axis('off')\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nThe distorted image has had much of its intensity spread out diffusely, so it is only visible as a faint outline. The noise is also visible in the image as a grainy texture. The gradient flow method will attempt to recover the original image from this distorted image.\n\n\nShow the code\nfrom matplotlib import animation\n\nb_flat = b.flatten().to(torch.float64)\nx_flat = x.flatten().to(torch.float64)\nb_noisy = b_flat + 0.001 * torch.randn_like(b_flat)\n\ndef get_xhat(t):\n    Sinv_t = (1 - torch.exp(-S**2 * t)) / S\n    A_pinv = V @ torch.diag(Sinv_t) @ U.T\n    xhat = A_pinv @ b_noisy\n    return xhat\n\n# Time evolution parameters\nnum_frames = 50\nt_vals = np.logspace(0, 6, num_frames)\n\n# Prepare the plot\nfig, ax = plt.subplots(figsize=(6, 6))\nim = ax.imshow(np.zeros((dim, dim)), cmap='viridis', vmin=-1, vmax=1)\nax.set_title('Time Evolution of Pseudo-Inverse Gradient Flow')\nplt.axis('off')\n\n# Initialize the error text\nerror_text = ax.text(0.02, 0.95, '', transform=ax.transAxes, color='blue', fontsize=12,\n                     verticalalignment='top')\n\ntime_text = ax.text(0.5, 0.95, '', transform=ax.transAxes, color='blue', fontsize=12,\n                        verticalalignment='top')\n\n# Initialize containers to track min error and best time\ntracking = {'min_error': float('inf'), 'best_t': 0.0}\n\n# Animation update function\ndef update_frame(t):\n    # Compute time-dependent singular values\n    Sinv_t = (1 - torch.exp(-S ** 2 * t)) / S\n    # Construct the pseudoinverse of Amat at time t\n    A_pinv = V @ torch.diag(Sinv_t) @ U.t()\n    # Reconstruct the image estimate x(t)\n    xt = A_pinv @ b_noisy\n    # Compute the relative error\n    error = torch.norm(x_flat - xt) / torch.norm(x_flat)\n    \n    # Update min_error and best_t if current error is lower\n    if error.item() &lt; tracking['min_error']:\n        tracking['min_error'] = error.item()\n        tracking['best_t'] = t\n\n    # Reshape to image dimensions\n    x_image = xt.reshape(dim, dim).detach().numpy()\n\n    # Update the image data\n    im.set_data(x_image)\n\n    # Update the error text\n    error_text.set_text(f'Relative Error: {error.item():.4f}')\n    time_text.set_text(f'Time: {t:.2f}')\n\n    return [im, error_text, time_text]\n\n# Create the animation\nani = animation.FuncAnimation(fig, update_frame, frames=t_vals, blit=True, interval=100)\n\nani.save('imgs/gradient_flow.gif', writer='pillow', fps=5)\nplt.close(fig)\n\n\n\nAnd we saved the best time that was discovered for the recovery (with prior knowledge of the ground truth). So we can inspect that image, this was the best that we could do with the gradient flow method.\n\n\nShow the code\nbest_img = get_xhat(tracking['best_t']).reshape(dim, dim).detach().numpy()\n\nplt.figure(figsize=(6, 6))\nplt.imshow(best_img / np.max(np.abs(best_img)), cmap='viridis', vmin=-1, vmax=1)\nplt.title(f'Best Reconstruction at t={tracking[\"best_t\"]:.2f}\\nRelative Error: {tracking[\"min_error\"]:.4f}')\nplt.axis('off')\nplt.show()"
  },
  {
    "objectID": "content/eosc555/lectures/lecture3/index.html#recovery-of-the-adjoint-operator-using-autograd",
    "href": "content/eosc555/lectures/lecture3/index.html#recovery-of-the-adjoint-operator-using-autograd",
    "title": "Lecture 3: Image Denoising with Gradient Descent and Early Stopping",
    "section": "Recovery of the Adjoint Operator using Autograd",
    "text": "Recovery of the Adjoint Operator using Autograd\nIn this case we were able to compute the matrix form of \\(A\\) and use its transpose to compute the SVD, but in many cases this might be too expensive or there may not be a closed form analytic solution to the early stopping technique. In such cases we wish to recover the adjoint. The question then is how to recover the adjoint operator from the Amv operator? There are helpful tools available through the use of automatic differentiation to track the gradients of the forward operator and recover the adjoint operator. This is a very powerful tool that can be used to recover the adjoint operator in a very general way.\nBy definition the adjoint has the property that: \\[\\langle Ax, v \\rangle = \\langle x, A^\\top v \\rangle\\]\n\nExplicit Computation of the Adjoint\nWe can compute the adjoint explicitly for the Amv operator based on its computation from earlier. The discrete fourier transform matrix operator \\(F\\) has the property that \\(F^{-1} = F^\\top\\) so we can use this to compute the adjoint.\n\\[\n\\begin{align}\nA(x) &= \\mathcal{F}^-1 \\left( \\mathcal{F}(P) \\odot \\mathcal{F}(x) \\right)\\\\\n&= F^\\top \\left( \\text{diag} (F(P)) F(x) \\right)\\\\\nA^\\top(v) &= F^\\top \\text{diag} (F(P))^* F v\\\\\n\\end{align}\n\\]\nWhere the hadamard operation of the two vectors has been modified to a matrix form by diagonalizing the vector \\(F(P)\\) that is the Fourier transform of the point spread function. From this form it is posible to take the adjoint of the operator by taking the complex conjugate of the transpose of the entire operation.\n\n\nAutograd Computation of the Adjoint\nWe start with a new function \\(h = v^\\top A(x)\\) and we wish to compute the gradient of \\(h\\) with respect to \\(x\\).\n\\[ \\nabla_x h = \\nabla_x (v^\\top A(x)) = A^\\top(v)\\]\nThe gradient of \\(h\\) with respect to \\(x\\) is the adjoint operator \\(A^\\top(v)\\). We can use the torch.autograd.grad function to compute the gradient of \\(h\\) with respect to \\(x\\).\n\n\nShow the code\ndef Amv_adjoint(v):\n    x = torch.zeros(1, 1, dim, dim)\n    x.requires_grad = True\n    b = Amv(x)\n    # Compute the dot product of the forward operator with the input vector\n    h = torch.sum(b * v)\n    # Compute the gradient of the dot product with respect to the input image\n    adjoint = torch.autograd.grad(h, x, create_graph=True)[0]\n    return adjoint\n\n\nWe can use this to recover \\(A^\\top\\) for the general case if we run the operator on the set of basis vectors in the image space. This will give us the adjoint operator in the form of a matrix. We can also use it to confirm that it recovers the matrix transpose of the forward operator if we are working with a simple matrix, reusing the Amat matrix from earlier to take its transpose and compare it to the adjoint operator.\n\n\nShow the code\nAmat_adj = torch.zeros(n,n)\n\ndim = 32 # Same as earlier\nk=0\nfor i in range(dim):\n  for j in range(dim):\n    e_ij = torch.zeros_like(x)\n    e_ij[:,:, i, j] = 1.0\n    y = Amv_adjoint(e_ij)\n    Amat_adj[:, k] = y.flatten()\n    k = k+1\n\ndiff = torch.norm(Amat_adj - Amat.T)\nprint(f'Norm of difference between adjoint and transpose: {diff:.2e}')\n\n\nNorm of difference between adjoint and transpose: 4.43e-07\n\n\nSo the difference is within the bounds of numerical precison and the code appears to be working correctly."
  },
  {
    "objectID": "content/eosc555/lectures/lecture3/index.html#gradient-descent-with-adjoint",
    "href": "content/eosc555/lectures/lecture3/index.html#gradient-descent-with-adjoint",
    "title": "Lecture 3: Image Denoising with Gradient Descent and Early Stopping",
    "section": "Gradient Descent with Adjoint",
    "text": "Gradient Descent with Adjoint\nWe can now use the defined operators (functions) from earlier to setup a simple gradient descent algorithm with a step size and early stopping to produce a recovery image that bypasses the need to compute the SVD decomposition, which may be very expensive for large matrices.\n\n\nShow the code\nfrom tqdm import tqdm\n\ndef least_squares_sol(x0, b, Amv, Amv_adjoint, max_iter=1000, alpha=1e-3, tol=1e-6):\n    \"\"\"\n    Solves the least squares problem using gradient descent with progress tracking.\n\n    Parameters:\n    - x0 (torch.Tensor): Initial guess for the solution.\n    - b (torch.Tensor): Observation vector.\n    - Amv (callable): Function to compute A @ x.\n    - Amv_adjoint (callable): Function to compute A^T @ v.\n    - max_iter (int): Maximum number of iterations.\n    - alpha (float): Learning rate.\n    - tol (float): Tolerance for convergence.\n\n    Returns:\n    - x (torch.Tensor): Approximated solution vector.\n    \"\"\"\n    x = x0.clone()\n    x.requires_grad = True\n    b_noisy = b.clone() + 0.01 * torch.randn_like(b)\n\n    # Initialize the progress bar\n    with tqdm(total=max_iter, desc='Least Squares Iteration', unit='iter') as pbar:\n        for i in range(max_iter):\n            # Gradient descent update\n            residual = Amv(x) - b_noisy\n            gradient = Amv_adjoint(residual)\n            xnext = x - alpha * gradient\n\n            # Compute relative error\n            error = torch.norm(xnext - x) \n\n            # Update the progress bar with the current error\n            pbar.set_postfix({'Error': f'{error.item():.4e}'})\n            pbar.update(1);\n\n            # Check for convergence\n            if error &lt; tol:\n                pbar.write(f'Converged at iteration {i+1} with error {error.item():.4e}')\n                x = xnext\n                break\n\n            x = xnext\n\n    return x\n\nb = Amv(x)\nx0 = torch.zeros_like(x)\nxhat = least_squares_sol(x0, b, Amv, Amv_adjoint, max_iter=1000, alpha=1, tol=1e-6)\n\nplt.figure(figsize=(6,3))\nplt.subplot(1, 2, 1)\nplt.imshow(x[0, 0], cmap='viridis', vmin=-1, vmax=1)\nplt.title('Original Image')\nplt.axis('off')\nplt.subplot(1, 2, 2)\nplt.imshow(xhat.detach().numpy()[0, 0], cmap='viridis', vmin=-1, vmax=1)\nplt.title('Recovered Image')\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n\nLeast Squares Iteration:   0%|          | 0/1000 [00:00&lt;?, ?iter/s]Least Squares Iteration:   0%|          | 0/1000 [00:00&lt;?, ?iter/s, Error=2.1864e-01]Least Squares Iteration:   0%|          | 1/1000 [00:00&lt;00:01, 499.80iter/s, Error=1.2388e-01]Least Squares Iteration:   0%|          | 2/1000 [00:00&lt;00:01, 500.10iter/s, Error=8.7774e-02]Least Squares Iteration:   0%|          | 3/1000 [00:00&lt;00:01, 600.44iter/s, Error=6.8322e-02]Least Squares Iteration:   0%|          | 4/1000 [00:00&lt;00:01, 666.79iter/s, Error=5.6250e-02]Least Squares Iteration:   0%|          | 5/1000 [00:00&lt;00:01, 624.88iter/s, Error=4.8034e-02]Least Squares Iteration:   1%|          | 6/1000 [00:00&lt;00:01, 665.94iter/s, Error=4.2031e-02]Least Squares Iteration:   1%|          | 7/1000 [00:00&lt;00:01, 635.80iter/s, Error=3.7418e-02]Least Squares Iteration:   1%|          | 8/1000 [00:00&lt;00:01, 666.21iter/s, Error=3.3751e-02]Least Squares Iteration:   1%|          | 9/1000 [00:00&lt;00:01, 691.88iter/s, Error=3.0767e-02]Least Squares Iteration:   1%|          | 10/1000 [00:00&lt;00:01, 666.37iter/s, Error=2.8298e-02]Least Squares Iteration:   1%|          | 11/1000 [00:00&lt;00:01, 687.18iter/s, Error=2.6226e-02]Least Squares Iteration:   1%|          | 12/1000 [00:00&lt;00:01, 705.62iter/s, Error=2.4467e-02]Least Squares Iteration:   1%|▏         | 13/1000 [00:00&lt;00:01, 684.01iter/s, Error=2.2956e-02]Least Squares Iteration:   1%|▏         | 14/1000 [00:00&lt;00:01, 699.76iter/s, Error=2.1645e-02]Least Squares Iteration:   2%|▏         | 15/1000 [00:00&lt;00:01, 714.11iter/s, Error=2.0495e-02]Least Squares Iteration:   2%|▏         | 16/1000 [00:00&lt;00:01, 727.01iter/s, Error=1.9477e-02]Least Squares Iteration:   2%|▏         | 17/1000 [00:00&lt;00:01, 723.03iter/s, Error=1.8568e-02]Least Squares Iteration:   2%|▏         | 18/1000 [00:00&lt;00:01, 734.22iter/s, Error=1.7750e-02]Least Squares Iteration:   2%|▏         | 19/1000 [00:00&lt;00:01, 744.64iter/s, Error=1.7009e-02]Least Squares Iteration:   2%|▏         | 20/1000 [00:00&lt;00:01, 754.28iter/s, Error=1.6332e-02]Least Squares Iteration:   2%|▏         | 21/1000 [00:00&lt;00:01, 736.44iter/s, Error=1.5711e-02]Least Squares Iteration:   2%|▏         | 22/1000 [00:00&lt;00:01, 745.23iter/s, Error=1.5138e-02]Least Squares Iteration:   2%|▏         | 23/1000 [00:00&lt;00:01, 729.79iter/s, Error=1.4607e-02]Least Squares Iteration:   2%|▏         | 24/1000 [00:00&lt;00:01, 738.10iter/s, Error=1.4114e-02]Least Squares Iteration:   2%|▎         | 25/1000 [00:00&lt;00:01, 745.90iter/s, Error=1.3653e-02]Least Squares Iteration:   3%|▎         | 26/1000 [00:00&lt;00:01, 732.08iter/s, Error=1.3222e-02]Least Squares Iteration:   3%|▎         | 27/1000 [00:00&lt;00:01, 739.41iter/s, Error=1.2817e-02]Least Squares Iteration:   3%|▎         | 28/1000 [00:00&lt;00:01, 726.97iter/s, Error=1.2437e-02]Least Squares Iteration:   3%|▎         | 29/1000 [00:00&lt;00:01, 733.87iter/s, Error=1.2078e-02]Least Squares Iteration:   3%|▎         | 30/1000 [00:00&lt;00:01, 740.45iter/s, Error=1.1739e-02]Least Squares Iteration:   3%|▎         | 31/1000 [00:00&lt;00:01, 746.71iter/s, Error=1.1418e-02]Least Squares Iteration:   3%|▎         | 32/1000 [00:00&lt;00:01, 735.36iter/s, Error=1.1115e-02]Least Squares Iteration:   3%|▎         | 33/1000 [00:00&lt;00:01, 725.00iter/s, Error=1.0826e-02]Least Squares Iteration:   3%|▎         | 34/1000 [00:00&lt;00:01, 715.52iter/s, Error=1.0553e-02]Least Squares Iteration:   4%|▎         | 35/1000 [00:00&lt;00:01, 721.38iter/s, Error=1.0292e-02]Least Squares Iteration:   4%|▎         | 36/1000 [00:00&lt;00:01, 712.60iter/s, Error=1.0044e-02]Least Squares Iteration:   4%|▎         | 37/1000 [00:00&lt;00:01, 718.17iter/s, Error=9.8078e-03]Least Squares Iteration:   4%|▍         | 38/1000 [00:00&lt;00:01, 710.00iter/s, Error=9.5824e-03]Least Squares Iteration:   4%|▍         | 39/1000 [00:00&lt;00:01, 715.34iter/s, Error=9.3672e-03]Least Squares Iteration:   4%|▍         | 40/1000 [00:00&lt;00:01, 720.49iter/s, Error=9.1616e-03]Least Squares Iteration:   4%|▍         | 41/1000 [00:00&lt;00:01, 712.81iter/s, Error=8.9650e-03]Least Squares Iteration:   4%|▍         | 42/1000 [00:00&lt;00:01, 717.73iter/s, Error=8.7769e-03]Least Squares Iteration:   4%|▍         | 43/1000 [00:00&lt;00:01, 710.55iter/s, Error=8.5968e-03]Least Squares Iteration:   4%|▍         | 44/1000 [00:00&lt;00:01, 703.78iter/s, Error=8.4243e-03]Least Squares Iteration:   4%|▍         | 45/1000 [00:00&lt;00:01, 697.48iter/s, Error=8.2589e-03]Least Squares Iteration:   5%|▍         | 46/1000 [00:00&lt;00:01, 681.30iter/s, Error=8.1003e-03]Least Squares Iteration:   5%|▍         | 47/1000 [00:00&lt;00:01, 676.07iter/s, Error=7.9480e-03]Least Squares Iteration:   5%|▍         | 48/1000 [00:00&lt;00:01, 671.15iter/s, Error=7.8018e-03]Least Squares Iteration:   5%|▍         | 49/1000 [00:00&lt;00:01, 675.70iter/s, Error=7.6613e-03]Least Squares Iteration:   5%|▌         | 50/1000 [00:00&lt;00:01, 670.99iter/s, Error=7.5263e-03]Least Squares Iteration:   5%|▌         | 51/1000 [00:00&lt;00:01, 675.34iter/s, Error=7.3965e-03]Least Squares Iteration:   5%|▌         | 52/1000 [00:00&lt;00:01, 679.59iter/s, Error=7.2716e-03]Least Squares Iteration:   5%|▌         | 53/1000 [00:00&lt;00:01, 675.00iter/s, Error=7.1513e-03]Least Squares Iteration:   5%|▌         | 54/1000 [00:00&lt;00:01, 670.65iter/s, Error=7.0355e-03]Least Squares Iteration:   6%|▌         | 55/1000 [00:00&lt;00:01, 666.54iter/s, Error=6.9238e-03]Least Squares Iteration:   6%|▌         | 56/1000 [00:00&lt;00:01, 670.51iter/s, Error=6.8162e-03]Least Squares Iteration:   6%|▌         | 57/1000 [00:00&lt;00:01, 666.53iter/s, Error=6.7125e-03]Least Squares Iteration:   6%|▌         | 58/1000 [00:00&lt;00:01, 670.36iter/s, Error=6.6123e-03]Least Squares Iteration:   6%|▌         | 59/1000 [00:00&lt;00:01, 674.15iter/s, Error=6.5156e-03]Least Squares Iteration:   6%|▌         | 60/1000 [00:00&lt;00:01, 670.26iter/s, Error=6.4222e-03]Least Squares Iteration:   6%|▌         | 61/1000 [00:00&lt;00:01, 673.91iter/s, Error=6.3320e-03]Least Squares Iteration:   6%|▌         | 62/1000 [00:00&lt;00:01, 670.15iter/s, Error=6.2448e-03]Least Squares Iteration:   6%|▋         | 63/1000 [00:00&lt;00:01, 666.54iter/s, Error=6.1604e-03]Least Squares Iteration:   6%|▋         | 64/1000 [00:00&lt;00:01, 663.10iter/s, Error=6.0788e-03]Least Squares Iteration:   6%|▋         | 65/1000 [00:00&lt;00:01, 666.55iter/s, Error=5.9998e-03]Least Squares Iteration:   7%|▋         | 66/1000 [00:00&lt;00:01, 663.21iter/s, Error=5.9232e-03]Least Squares Iteration:   7%|▋         | 67/1000 [00:00&lt;00:01, 666.55iter/s, Error=5.9232e-03]Least Squares Iteration:   7%|▋         | 67/1000 [00:00&lt;00:01, 666.55iter/s, Error=5.8491e-03]Least Squares Iteration:   7%|▋         | 68/1000 [00:00&lt;00:01, 666.55iter/s, Error=5.7772e-03]Least Squares Iteration:   7%|▋         | 69/1000 [00:00&lt;00:01, 666.55iter/s, Error=5.7075e-03]Least Squares Iteration:   7%|▋         | 70/1000 [00:00&lt;00:01, 666.55iter/s, Error=5.6399e-03]Least Squares Iteration:   7%|▋         | 71/1000 [00:00&lt;00:01, 666.55iter/s, Error=5.5742e-03]Least Squares Iteration:   7%|▋         | 72/1000 [00:00&lt;00:01, 666.55iter/s, Error=5.5105e-03]Least Squares Iteration:   7%|▋         | 73/1000 [00:00&lt;00:01, 666.55iter/s, Error=5.4486e-03]Least Squares Iteration:   7%|▋         | 74/1000 [00:00&lt;00:01, 666.55iter/s, Error=5.3884e-03]Least Squares Iteration:   8%|▊         | 75/1000 [00:00&lt;00:01, 666.55iter/s, Error=5.3298e-03]Least Squares Iteration:   8%|▊         | 76/1000 [00:00&lt;00:01, 666.55iter/s, Error=5.2729e-03]Least Squares Iteration:   8%|▊         | 77/1000 [00:00&lt;00:01, 666.55iter/s, Error=5.2175e-03]Least Squares Iteration:   8%|▊         | 78/1000 [00:00&lt;00:01, 666.55iter/s, Error=5.1636e-03]Least Squares Iteration:   8%|▊         | 79/1000 [00:00&lt;00:01, 666.55iter/s, Error=5.1110e-03]Least Squares Iteration:   8%|▊         | 80/1000 [00:00&lt;00:01, 666.55iter/s, Error=5.0598e-03]Least Squares Iteration:   8%|▊         | 81/1000 [00:00&lt;00:01, 666.55iter/s, Error=5.0099e-03]Least Squares Iteration:   8%|▊         | 82/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.9613e-03]Least Squares Iteration:   8%|▊         | 83/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.9138e-03]Least Squares Iteration:   8%|▊         | 84/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.8675e-03]Least Squares Iteration:   8%|▊         | 85/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.8222e-03]Least Squares Iteration:   9%|▊         | 86/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.7780e-03]Least Squares Iteration:   9%|▊         | 87/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.7349e-03]Least Squares Iteration:   9%|▉         | 88/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.6927e-03]Least Squares Iteration:   9%|▉         | 89/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.6514e-03]Least Squares Iteration:   9%|▉         | 90/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.6110e-03]Least Squares Iteration:   9%|▉         | 91/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.5715e-03]Least Squares Iteration:   9%|▉         | 92/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.5329e-03]Least Squares Iteration:   9%|▉         | 93/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.4951e-03]Least Squares Iteration:   9%|▉         | 94/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.4580e-03]Least Squares Iteration:  10%|▉         | 95/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.4217e-03]Least Squares Iteration:  10%|▉         | 96/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.3861e-03]Least Squares Iteration:  10%|▉         | 97/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.3512e-03]Least Squares Iteration:  10%|▉         | 98/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.3169e-03]Least Squares Iteration:  10%|▉         | 99/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.2834e-03]Least Squares Iteration:  10%|█         | 100/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.2504e-03]Least Squares Iteration:  10%|█         | 101/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.2181e-03]Least Squares Iteration:  10%|█         | 102/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.1863e-03]Least Squares Iteration:  10%|█         | 103/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.1552e-03]Least Squares Iteration:  10%|█         | 104/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.1245e-03]Least Squares Iteration:  10%|█         | 105/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.0944e-03]Least Squares Iteration:  11%|█         | 106/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.0649e-03]Least Squares Iteration:  11%|█         | 107/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.0358e-03]Least Squares Iteration:  11%|█         | 108/1000 [00:00&lt;00:01, 666.55iter/s, Error=4.0072e-03]Least Squares Iteration:  11%|█         | 109/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.9791e-03]Least Squares Iteration:  11%|█         | 110/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.9514e-03]Least Squares Iteration:  11%|█         | 111/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.9242e-03]Least Squares Iteration:  11%|█         | 112/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.8974e-03]Least Squares Iteration:  11%|█▏        | 113/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.8711e-03]Least Squares Iteration:  11%|█▏        | 114/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.8451e-03]Least Squares Iteration:  12%|█▏        | 115/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.8196e-03]Least Squares Iteration:  12%|█▏        | 116/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.7944e-03]Least Squares Iteration:  12%|█▏        | 117/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.7696e-03]Least Squares Iteration:  12%|█▏        | 118/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.7451e-03]Least Squares Iteration:  12%|█▏        | 119/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.7211e-03]Least Squares Iteration:  12%|█▏        | 120/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.6973e-03]Least Squares Iteration:  12%|█▏        | 121/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.6739e-03]Least Squares Iteration:  12%|█▏        | 122/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.6509e-03]Least Squares Iteration:  12%|█▏        | 123/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.6281e-03]Least Squares Iteration:  12%|█▏        | 124/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.6057e-03]Least Squares Iteration:  12%|█▎        | 125/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.5836e-03]Least Squares Iteration:  13%|█▎        | 126/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.5617e-03]Least Squares Iteration:  13%|█▎        | 127/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.5402e-03]Least Squares Iteration:  13%|█▎        | 128/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.5189e-03]Least Squares Iteration:  13%|█▎        | 129/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.4980e-03]Least Squares Iteration:  13%|█▎        | 130/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.4773e-03]Least Squares Iteration:  13%|█▎        | 131/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.4568e-03]Least Squares Iteration:  13%|█▎        | 132/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.4366e-03]Least Squares Iteration:  13%|█▎        | 133/1000 [00:00&lt;00:01, 666.55iter/s, Error=3.4167e-03]Least Squares Iteration:  13%|█▎        | 134/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.4167e-03]Least Squares Iteration:  13%|█▎        | 134/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.3970e-03]Least Squares Iteration:  14%|█▎        | 135/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.3776e-03]Least Squares Iteration:  14%|█▎        | 136/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.3584e-03]Least Squares Iteration:  14%|█▎        | 137/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.3395e-03]Least Squares Iteration:  14%|█▍        | 138/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.3208e-03]Least Squares Iteration:  14%|█▍        | 139/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.3022e-03]Least Squares Iteration:  14%|█▍        | 140/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.2840e-03]Least Squares Iteration:  14%|█▍        | 141/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.2659e-03]Least Squares Iteration:  14%|█▍        | 142/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.2480e-03]Least Squares Iteration:  14%|█▍        | 143/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.2304e-03]Least Squares Iteration:  14%|█▍        | 144/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.2130e-03]Least Squares Iteration:  14%|█▍        | 145/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.1957e-03]Least Squares Iteration:  15%|█▍        | 146/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.1787e-03]Least Squares Iteration:  15%|█▍        | 147/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.1618e-03]Least Squares Iteration:  15%|█▍        | 148/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.1452e-03]Least Squares Iteration:  15%|█▍        | 149/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.1287e-03]Least Squares Iteration:  15%|█▌        | 150/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.1124e-03]Least Squares Iteration:  15%|█▌        | 151/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.0963e-03]Least Squares Iteration:  15%|█▌        | 152/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.0804e-03]Least Squares Iteration:  15%|█▌        | 153/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.0646e-03]Least Squares Iteration:  15%|█▌        | 154/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.0490e-03]Least Squares Iteration:  16%|█▌        | 155/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.0336e-03]Least Squares Iteration:  16%|█▌        | 156/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.0184e-03]Least Squares Iteration:  16%|█▌        | 157/1000 [00:00&lt;00:01, 591.95iter/s, Error=3.0033e-03]Least Squares Iteration:  16%|█▌        | 158/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.9883e-03]Least Squares Iteration:  16%|█▌        | 159/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.9736e-03]Least Squares Iteration:  16%|█▌        | 160/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.9590e-03]Least Squares Iteration:  16%|█▌        | 161/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.9445e-03]Least Squares Iteration:  16%|█▌        | 162/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.9302e-03]Least Squares Iteration:  16%|█▋        | 163/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.9160e-03]Least Squares Iteration:  16%|█▋        | 164/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.9020e-03]Least Squares Iteration:  16%|█▋        | 165/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.8882e-03]Least Squares Iteration:  17%|█▋        | 166/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.8745e-03]Least Squares Iteration:  17%|█▋        | 167/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.8609e-03]Least Squares Iteration:  17%|█▋        | 168/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.8474e-03]Least Squares Iteration:  17%|█▋        | 169/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.8341e-03]Least Squares Iteration:  17%|█▋        | 170/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.8209e-03]Least Squares Iteration:  17%|█▋        | 171/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.8079e-03]Least Squares Iteration:  17%|█▋        | 172/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.7950e-03]Least Squares Iteration:  17%|█▋        | 173/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.7822e-03]Least Squares Iteration:  17%|█▋        | 174/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.7696e-03]Least Squares Iteration:  18%|█▊        | 175/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.7570e-03]Least Squares Iteration:  18%|█▊        | 176/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.7446e-03]Least Squares Iteration:  18%|█▊        | 177/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.7324e-03]Least Squares Iteration:  18%|█▊        | 178/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.7202e-03]Least Squares Iteration:  18%|█▊        | 179/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.7082e-03]Least Squares Iteration:  18%|█▊        | 180/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.6963e-03]Least Squares Iteration:  18%|█▊        | 181/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.6844e-03]Least Squares Iteration:  18%|█▊        | 182/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.6728e-03]Least Squares Iteration:  18%|█▊        | 183/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.6612e-03]Least Squares Iteration:  18%|█▊        | 184/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.6497e-03]Least Squares Iteration:  18%|█▊        | 185/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.6384e-03]Least Squares Iteration:  19%|█▊        | 186/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.6271e-03]Least Squares Iteration:  19%|█▊        | 187/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.6160e-03]Least Squares Iteration:  19%|█▉        | 188/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.6050e-03]Least Squares Iteration:  19%|█▉        | 189/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.5940e-03]Least Squares Iteration:  19%|█▉        | 190/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.5832e-03]Least Squares Iteration:  19%|█▉        | 191/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.5725e-03]Least Squares Iteration:  19%|█▉        | 192/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.5619e-03]Least Squares Iteration:  19%|█▉        | 193/1000 [00:00&lt;00:01, 591.95iter/s, Error=2.5514e-03]Least Squares Iteration:  19%|█▉        | 194/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.5514e-03]Least Squares Iteration:  19%|█▉        | 194/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.5410e-03]Least Squares Iteration:  20%|█▉        | 195/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.5307e-03]Least Squares Iteration:  20%|█▉        | 196/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.5205e-03]Least Squares Iteration:  20%|█▉        | 197/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.5103e-03]Least Squares Iteration:  20%|█▉        | 198/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.5003e-03]Least Squares Iteration:  20%|█▉        | 199/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.4904e-03]Least Squares Iteration:  20%|██        | 200/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.4805e-03]Least Squares Iteration:  20%|██        | 201/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.4708e-03]Least Squares Iteration:  20%|██        | 202/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.4611e-03]Least Squares Iteration:  20%|██        | 203/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.4515e-03]Least Squares Iteration:  20%|██        | 204/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.4421e-03]Least Squares Iteration:  20%|██        | 205/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.4327e-03]Least Squares Iteration:  21%|██        | 206/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.4234e-03]Least Squares Iteration:  21%|██        | 207/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.4141e-03]Least Squares Iteration:  21%|██        | 208/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.4050e-03]Least Squares Iteration:  21%|██        | 209/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.3959e-03]Least Squares Iteration:  21%|██        | 210/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.3870e-03]Least Squares Iteration:  21%|██        | 211/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.3781e-03]Least Squares Iteration:  21%|██        | 212/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.3693e-03]Least Squares Iteration:  21%|██▏       | 213/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.3606e-03]Least Squares Iteration:  21%|██▏       | 214/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.3519e-03]Least Squares Iteration:  22%|██▏       | 215/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.3433e-03]Least Squares Iteration:  22%|██▏       | 216/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.3348e-03]Least Squares Iteration:  22%|██▏       | 217/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.3264e-03]Least Squares Iteration:  22%|██▏       | 218/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.3181e-03]Least Squares Iteration:  22%|██▏       | 219/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.3098e-03]Least Squares Iteration:  22%|██▏       | 220/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.3016e-03]Least Squares Iteration:  22%|██▏       | 221/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.2935e-03]Least Squares Iteration:  22%|██▏       | 222/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.2855e-03]Least Squares Iteration:  22%|██▏       | 223/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.2775e-03]Least Squares Iteration:  22%|██▏       | 224/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.2696e-03]Least Squares Iteration:  22%|██▎       | 225/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.2617e-03]Least Squares Iteration:  23%|██▎       | 226/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.2540e-03]Least Squares Iteration:  23%|██▎       | 227/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.2463e-03]Least Squares Iteration:  23%|██▎       | 228/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.2386e-03]Least Squares Iteration:  23%|██▎       | 229/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.2311e-03]Least Squares Iteration:  23%|██▎       | 230/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.2236e-03]Least Squares Iteration:  23%|██▎       | 231/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.2162e-03]Least Squares Iteration:  23%|██▎       | 232/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.2088e-03]Least Squares Iteration:  23%|██▎       | 233/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.2015e-03]Least Squares Iteration:  23%|██▎       | 234/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.1943e-03]Least Squares Iteration:  24%|██▎       | 235/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.1871e-03]Least Squares Iteration:  24%|██▎       | 236/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.1800e-03]Least Squares Iteration:  24%|██▎       | 237/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.1729e-03]Least Squares Iteration:  24%|██▍       | 238/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.1659e-03]Least Squares Iteration:  24%|██▍       | 239/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.1590e-03]Least Squares Iteration:  24%|██▍       | 240/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.1521e-03]Least Squares Iteration:  24%|██▍       | 241/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.1453e-03]Least Squares Iteration:  24%|██▍       | 242/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.1386e-03]Least Squares Iteration:  24%|██▍       | 243/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.1319e-03]Least Squares Iteration:  24%|██▍       | 244/1000 [00:00&lt;00:01, 489.05iter/s, Error=2.1253e-03]Least Squares Iteration:  24%|██▍       | 245/1000 [00:00&lt;00:01, 434.42iter/s, Error=2.1253e-03]Least Squares Iteration:  24%|██▍       | 245/1000 [00:00&lt;00:01, 434.42iter/s, Error=2.1187e-03]Least Squares Iteration:  25%|██▍       | 246/1000 [00:00&lt;00:01, 434.42iter/s, Error=2.1121e-03]Least Squares Iteration:  25%|██▍       | 247/1000 [00:00&lt;00:01, 434.42iter/s, Error=2.1057e-03]Least Squares Iteration:  25%|██▍       | 248/1000 [00:00&lt;00:01, 434.42iter/s, Error=2.0993e-03]Least Squares Iteration:  25%|██▍       | 249/1000 [00:00&lt;00:01, 434.42iter/s, Error=2.0929e-03]Least Squares Iteration:  25%|██▌       | 250/1000 [00:00&lt;00:01, 434.42iter/s, Error=2.0866e-03]Least Squares Iteration:  25%|██▌       | 251/1000 [00:00&lt;00:01, 434.42iter/s, Error=2.0804e-03]Least Squares Iteration:  25%|██▌       | 252/1000 [00:00&lt;00:01, 434.42iter/s, Error=2.0742e-03]Least Squares Iteration:  25%|██▌       | 253/1000 [00:00&lt;00:01, 434.42iter/s, Error=2.0680e-03]Least Squares Iteration:  25%|██▌       | 254/1000 [00:00&lt;00:01, 434.42iter/s, Error=2.0619e-03]Least Squares Iteration:  26%|██▌       | 255/1000 [00:00&lt;00:01, 434.42iter/s, Error=2.0559e-03]Least Squares Iteration:  26%|██▌       | 256/1000 [00:00&lt;00:01, 434.42iter/s, Error=2.0499e-03]Least Squares Iteration:  26%|██▌       | 257/1000 [00:00&lt;00:01, 434.42iter/s, Error=2.0439e-03]Least Squares Iteration:  26%|██▌       | 258/1000 [00:00&lt;00:01, 434.42iter/s, Error=2.0380e-03]Least Squares Iteration:  26%|██▌       | 259/1000 [00:00&lt;00:01, 434.42iter/s, Error=2.0322e-03]Least Squares Iteration:  26%|██▌       | 260/1000 [00:00&lt;00:01, 434.42iter/s, Error=2.0264e-03]Least Squares Iteration:  26%|██▌       | 261/1000 [00:00&lt;00:01, 434.42iter/s, Error=2.0206e-03]Least Squares Iteration:  26%|██▌       | 262/1000 [00:00&lt;00:01, 434.42iter/s, Error=2.0149e-03]Least Squares Iteration:  26%|██▋       | 263/1000 [00:00&lt;00:01, 434.42iter/s, Error=2.0093e-03]Least Squares Iteration:  26%|██▋       | 264/1000 [00:00&lt;00:01, 434.42iter/s, Error=2.0037e-03]Least Squares Iteration:  26%|██▋       | 265/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.9981e-03]Least Squares Iteration:  27%|██▋       | 266/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.9926e-03]Least Squares Iteration:  27%|██▋       | 267/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.9871e-03]Least Squares Iteration:  27%|██▋       | 268/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.9817e-03]Least Squares Iteration:  27%|██▋       | 269/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.9763e-03]Least Squares Iteration:  27%|██▋       | 270/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.9710e-03]Least Squares Iteration:  27%|██▋       | 271/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.9657e-03]Least Squares Iteration:  27%|██▋       | 272/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.9604e-03]Least Squares Iteration:  27%|██▋       | 273/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.9552e-03]Least Squares Iteration:  27%|██▋       | 274/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.9501e-03]Least Squares Iteration:  28%|██▊       | 275/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.9449e-03]Least Squares Iteration:  28%|██▊       | 276/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.9398e-03]Least Squares Iteration:  28%|██▊       | 277/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.9348e-03]Least Squares Iteration:  28%|██▊       | 278/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.9298e-03]Least Squares Iteration:  28%|██▊       | 279/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.9248e-03]Least Squares Iteration:  28%|██▊       | 280/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.9199e-03]Least Squares Iteration:  28%|██▊       | 281/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.9150e-03]Least Squares Iteration:  28%|██▊       | 282/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.9101e-03]Least Squares Iteration:  28%|██▊       | 283/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.9053e-03]Least Squares Iteration:  28%|██▊       | 284/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.9006e-03]Least Squares Iteration:  28%|██▊       | 285/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.8958e-03]Least Squares Iteration:  29%|██▊       | 286/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.8911e-03]Least Squares Iteration:  29%|██▊       | 287/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.8864e-03]Least Squares Iteration:  29%|██▉       | 288/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.8818e-03]Least Squares Iteration:  29%|██▉       | 289/1000 [00:00&lt;00:01, 434.42iter/s, Error=1.8772e-03]Least Squares Iteration:  29%|██▉       | 290/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.8772e-03]Least Squares Iteration:  29%|██▉       | 290/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.8727e-03]Least Squares Iteration:  29%|██▉       | 291/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.8681e-03]Least Squares Iteration:  29%|██▉       | 292/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.8636e-03]Least Squares Iteration:  29%|██▉       | 293/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.8592e-03]Least Squares Iteration:  29%|██▉       | 294/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.8548e-03]Least Squares Iteration:  30%|██▉       | 295/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.8504e-03]Least Squares Iteration:  30%|██▉       | 296/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.8460e-03]Least Squares Iteration:  30%|██▉       | 297/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.8417e-03]Least Squares Iteration:  30%|██▉       | 298/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.8374e-03]Least Squares Iteration:  30%|██▉       | 299/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.8332e-03]Least Squares Iteration:  30%|███       | 300/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.8289e-03]Least Squares Iteration:  30%|███       | 301/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.8247e-03]Least Squares Iteration:  30%|███       | 302/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.8206e-03]Least Squares Iteration:  30%|███       | 303/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.8165e-03]Least Squares Iteration:  30%|███       | 304/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.8124e-03]Least Squares Iteration:  30%|███       | 305/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.8083e-03]Least Squares Iteration:  31%|███       | 306/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.8042e-03]Least Squares Iteration:  31%|███       | 307/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.8002e-03]Least Squares Iteration:  31%|███       | 308/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.7962e-03]Least Squares Iteration:  31%|███       | 309/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.7923e-03]Least Squares Iteration:  31%|███       | 310/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.7884e-03]Least Squares Iteration:  31%|███       | 311/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.7845e-03]Least Squares Iteration:  31%|███       | 312/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.7806e-03]Least Squares Iteration:  31%|███▏      | 313/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.7768e-03]Least Squares Iteration:  31%|███▏      | 314/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.7730e-03]Least Squares Iteration:  32%|███▏      | 315/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.7692e-03]Least Squares Iteration:  32%|███▏      | 316/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.7654e-03]Least Squares Iteration:  32%|███▏      | 317/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.7617e-03]Least Squares Iteration:  32%|███▏      | 318/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.7580e-03]Least Squares Iteration:  32%|███▏      | 319/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.7543e-03]Least Squares Iteration:  32%|███▏      | 320/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.7507e-03]Least Squares Iteration:  32%|███▏      | 321/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.7470e-03]Least Squares Iteration:  32%|███▏      | 322/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.7434e-03]Least Squares Iteration:  32%|███▏      | 323/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.7398e-03]Least Squares Iteration:  32%|███▏      | 324/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.7363e-03]Least Squares Iteration:  32%|███▎      | 325/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.7328e-03]Least Squares Iteration:  33%|███▎      | 326/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.7293e-03]Least Squares Iteration:  33%|███▎      | 327/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.7258e-03]Least Squares Iteration:  33%|███▎      | 328/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.7223e-03]Least Squares Iteration:  33%|███▎      | 329/1000 [00:00&lt;00:01, 382.52iter/s, Error=1.7189e-03]Least Squares Iteration:  33%|███▎      | 330/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.7189e-03]Least Squares Iteration:  33%|███▎      | 330/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.7155e-03]Least Squares Iteration:  33%|███▎      | 331/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.7121e-03]Least Squares Iteration:  33%|███▎      | 332/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.7087e-03]Least Squares Iteration:  33%|███▎      | 333/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.7054e-03]Least Squares Iteration:  33%|███▎      | 334/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.7021e-03]Least Squares Iteration:  34%|███▎      | 335/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6988e-03]Least Squares Iteration:  34%|███▎      | 336/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6955e-03]Least Squares Iteration:  34%|███▎      | 337/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6923e-03]Least Squares Iteration:  34%|███▍      | 338/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6890e-03]Least Squares Iteration:  34%|███▍      | 339/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6858e-03]Least Squares Iteration:  34%|███▍      | 340/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6826e-03]Least Squares Iteration:  34%|███▍      | 341/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6795e-03]Least Squares Iteration:  34%|███▍      | 342/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6763e-03]Least Squares Iteration:  34%|███▍      | 343/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6732e-03]Least Squares Iteration:  34%|███▍      | 344/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6701e-03]Least Squares Iteration:  34%|███▍      | 345/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6670e-03]Least Squares Iteration:  35%|███▍      | 346/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6640e-03]Least Squares Iteration:  35%|███▍      | 347/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6609e-03]Least Squares Iteration:  35%|███▍      | 348/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6579e-03]Least Squares Iteration:  35%|███▍      | 349/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6549e-03]Least Squares Iteration:  35%|███▌      | 350/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6519e-03]Least Squares Iteration:  35%|███▌      | 351/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6489e-03]Least Squares Iteration:  35%|███▌      | 352/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6460e-03]Least Squares Iteration:  35%|███▌      | 353/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6430e-03]Least Squares Iteration:  35%|███▌      | 354/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6401e-03]Least Squares Iteration:  36%|███▌      | 355/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6372e-03]Least Squares Iteration:  36%|███▌      | 356/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6344e-03]Least Squares Iteration:  36%|███▌      | 357/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6315e-03]Least Squares Iteration:  36%|███▌      | 358/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6287e-03]Least Squares Iteration:  36%|███▌      | 359/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6258e-03]Least Squares Iteration:  36%|███▌      | 360/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6230e-03]Least Squares Iteration:  36%|███▌      | 361/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6202e-03]Least Squares Iteration:  36%|███▌      | 362/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6175e-03]Least Squares Iteration:  36%|███▋      | 363/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6147e-03]Least Squares Iteration:  36%|███▋      | 364/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6120e-03]Least Squares Iteration:  36%|███▋      | 365/1000 [00:00&lt;00:01, 345.09iter/s, Error=1.6092e-03]Least Squares Iteration:  37%|███▋      | 366/1000 [00:00&lt;00:02, 285.61iter/s, Error=1.6092e-03]Least Squares Iteration:  37%|███▋      | 366/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.6065e-03]Least Squares Iteration:  37%|███▋      | 367/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.6038e-03]Least Squares Iteration:  37%|███▋      | 368/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.6012e-03]Least Squares Iteration:  37%|███▋      | 369/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5985e-03]Least Squares Iteration:  37%|███▋      | 370/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5959e-03]Least Squares Iteration:  37%|███▋      | 371/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5932e-03]Least Squares Iteration:  37%|███▋      | 372/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5906e-03]Least Squares Iteration:  37%|███▋      | 373/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5880e-03]Least Squares Iteration:  37%|███▋      | 374/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5854e-03]Least Squares Iteration:  38%|███▊      | 375/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5829e-03]Least Squares Iteration:  38%|███▊      | 376/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5803e-03]Least Squares Iteration:  38%|███▊      | 377/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5778e-03]Least Squares Iteration:  38%|███▊      | 378/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5752e-03]Least Squares Iteration:  38%|███▊      | 379/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5727e-03]Least Squares Iteration:  38%|███▊      | 380/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5702e-03]Least Squares Iteration:  38%|███▊      | 381/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5678e-03]Least Squares Iteration:  38%|███▊      | 382/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5653e-03]Least Squares Iteration:  38%|███▊      | 383/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5628e-03]Least Squares Iteration:  38%|███▊      | 384/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5604e-03]Least Squares Iteration:  38%|███▊      | 385/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5580e-03]Least Squares Iteration:  39%|███▊      | 386/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5556e-03]Least Squares Iteration:  39%|███▊      | 387/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5531e-03]Least Squares Iteration:  39%|███▉      | 388/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5508e-03]Least Squares Iteration:  39%|███▉      | 389/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5484e-03]Least Squares Iteration:  39%|███▉      | 390/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5460e-03]Least Squares Iteration:  39%|███▉      | 391/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5437e-03]Least Squares Iteration:  39%|███▉      | 392/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5413e-03]Least Squares Iteration:  39%|███▉      | 393/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5390e-03]Least Squares Iteration:  39%|███▉      | 394/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5367e-03]Least Squares Iteration:  40%|███▉      | 395/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5344e-03]Least Squares Iteration:  40%|███▉      | 396/1000 [00:01&lt;00:02, 285.61iter/s, Error=1.5321e-03]Least Squares Iteration:  40%|███▉      | 397/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.5321e-03]Least Squares Iteration:  40%|███▉      | 397/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.5298e-03]Least Squares Iteration:  40%|███▉      | 398/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.5276e-03]Least Squares Iteration:  40%|███▉      | 399/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.5253e-03]Least Squares Iteration:  40%|████      | 400/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.5231e-03]Least Squares Iteration:  40%|████      | 401/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.5209e-03]Least Squares Iteration:  40%|████      | 402/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.5186e-03]Least Squares Iteration:  40%|████      | 403/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.5164e-03]Least Squares Iteration:  40%|████      | 404/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.5142e-03]Least Squares Iteration:  40%|████      | 405/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.5121e-03]Least Squares Iteration:  41%|████      | 406/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.5099e-03]Least Squares Iteration:  41%|████      | 407/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.5077e-03]Least Squares Iteration:  41%|████      | 408/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.5056e-03]Least Squares Iteration:  41%|████      | 409/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.5034e-03]Least Squares Iteration:  41%|████      | 410/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.5013e-03]Least Squares Iteration:  41%|████      | 411/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.4992e-03]Least Squares Iteration:  41%|████      | 412/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.4971e-03]Least Squares Iteration:  41%|████▏     | 413/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.4950e-03]Least Squares Iteration:  41%|████▏     | 414/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.4929e-03]Least Squares Iteration:  42%|████▏     | 415/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.4908e-03]Least Squares Iteration:  42%|████▏     | 416/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.4888e-03]Least Squares Iteration:  42%|████▏     | 417/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.4867e-03]Least Squares Iteration:  42%|████▏     | 418/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.4847e-03]Least Squares Iteration:  42%|████▏     | 419/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.4826e-03]Least Squares Iteration:  42%|████▏     | 420/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.4806e-03]Least Squares Iteration:  42%|████▏     | 421/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.4786e-03]Least Squares Iteration:  42%|████▏     | 422/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.4766e-03]Least Squares Iteration:  42%|████▏     | 423/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.4746e-03]Least Squares Iteration:  42%|████▏     | 424/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.4726e-03]Least Squares Iteration:  42%|████▎     | 425/1000 [00:01&lt;00:02, 275.40iter/s, Error=1.4706e-03]Least Squares Iteration:  43%|████▎     | 426/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4706e-03]Least Squares Iteration:  43%|████▎     | 426/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4686e-03]Least Squares Iteration:  43%|████▎     | 427/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4667e-03]Least Squares Iteration:  43%|████▎     | 428/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4647e-03]Least Squares Iteration:  43%|████▎     | 429/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4628e-03]Least Squares Iteration:  43%|████▎     | 430/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4608e-03]Least Squares Iteration:  43%|████▎     | 431/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4589e-03]Least Squares Iteration:  43%|████▎     | 432/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4570e-03]Least Squares Iteration:  43%|████▎     | 433/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4551e-03]Least Squares Iteration:  43%|████▎     | 434/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4532e-03]Least Squares Iteration:  44%|████▎     | 435/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4513e-03]Least Squares Iteration:  44%|████▎     | 436/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4494e-03]Least Squares Iteration:  44%|████▎     | 437/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4475e-03]Least Squares Iteration:  44%|████▍     | 438/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4457e-03]Least Squares Iteration:  44%|████▍     | 439/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4438e-03]Least Squares Iteration:  44%|████▍     | 440/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4420e-03]Least Squares Iteration:  44%|████▍     | 441/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4401e-03]Least Squares Iteration:  44%|████▍     | 442/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4383e-03]Least Squares Iteration:  44%|████▍     | 443/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4365e-03]Least Squares Iteration:  44%|████▍     | 444/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4347e-03]Least Squares Iteration:  44%|████▍     | 445/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4328e-03]Least Squares Iteration:  45%|████▍     | 446/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4310e-03]Least Squares Iteration:  45%|████▍     | 447/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4292e-03]Least Squares Iteration:  45%|████▍     | 448/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4275e-03]Least Squares Iteration:  45%|████▍     | 449/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4257e-03]Least Squares Iteration:  45%|████▌     | 450/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4239e-03]Least Squares Iteration:  45%|████▌     | 451/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4221e-03]Least Squares Iteration:  45%|████▌     | 452/1000 [00:01&lt;00:02, 258.64iter/s, Error=1.4204e-03]Least Squares Iteration:  45%|████▌     | 453/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.4204e-03]Least Squares Iteration:  45%|████▌     | 453/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.4186e-03]Least Squares Iteration:  45%|████▌     | 454/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.4169e-03]Least Squares Iteration:  46%|████▌     | 455/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.4152e-03]Least Squares Iteration:  46%|████▌     | 456/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.4134e-03]Least Squares Iteration:  46%|████▌     | 457/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.4117e-03]Least Squares Iteration:  46%|████▌     | 458/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.4100e-03]Least Squares Iteration:  46%|████▌     | 459/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.4083e-03]Least Squares Iteration:  46%|████▌     | 460/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.4066e-03]Least Squares Iteration:  46%|████▌     | 461/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.4049e-03]Least Squares Iteration:  46%|████▌     | 462/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.4032e-03]Least Squares Iteration:  46%|████▋     | 463/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.4015e-03]Least Squares Iteration:  46%|████▋     | 464/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.3999e-03]Least Squares Iteration:  46%|████▋     | 465/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.3982e-03]Least Squares Iteration:  47%|████▋     | 466/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.3965e-03]Least Squares Iteration:  47%|████▋     | 467/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.3949e-03]Least Squares Iteration:  47%|████▋     | 468/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.3932e-03]Least Squares Iteration:  47%|████▋     | 469/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.3916e-03]Least Squares Iteration:  47%|████▋     | 470/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.3900e-03]Least Squares Iteration:  47%|████▋     | 471/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.3883e-03]Least Squares Iteration:  47%|████▋     | 472/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.3867e-03]Least Squares Iteration:  47%|████▋     | 473/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.3851e-03]Least Squares Iteration:  47%|████▋     | 474/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.3835e-03]Least Squares Iteration:  48%|████▊     | 475/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.3819e-03]Least Squares Iteration:  48%|████▊     | 476/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.3803e-03]Least Squares Iteration:  48%|████▊     | 477/1000 [00:01&lt;00:02, 242.19iter/s, Error=1.3787e-03]Least Squares Iteration:  48%|████▊     | 478/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3787e-03]Least Squares Iteration:  48%|████▊     | 478/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3771e-03]Least Squares Iteration:  48%|████▊     | 479/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3755e-03]Least Squares Iteration:  48%|████▊     | 480/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3740e-03]Least Squares Iteration:  48%|████▊     | 481/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3724e-03]Least Squares Iteration:  48%|████▊     | 482/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3708e-03]Least Squares Iteration:  48%|████▊     | 483/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3693e-03]Least Squares Iteration:  48%|████▊     | 484/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3677e-03]Least Squares Iteration:  48%|████▊     | 485/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3662e-03]Least Squares Iteration:  49%|████▊     | 486/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3647e-03]Least Squares Iteration:  49%|████▊     | 487/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3631e-03]Least Squares Iteration:  49%|████▉     | 488/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3616e-03]Least Squares Iteration:  49%|████▉     | 489/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3601e-03]Least Squares Iteration:  49%|████▉     | 490/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3586e-03]Least Squares Iteration:  49%|████▉     | 491/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3570e-03]Least Squares Iteration:  49%|████▉     | 492/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3555e-03]Least Squares Iteration:  49%|████▉     | 493/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3540e-03]Least Squares Iteration:  49%|████▉     | 494/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3525e-03]Least Squares Iteration:  50%|████▉     | 495/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3511e-03]Least Squares Iteration:  50%|████▉     | 496/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3496e-03]Least Squares Iteration:  50%|████▉     | 497/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3481e-03]Least Squares Iteration:  50%|████▉     | 498/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3466e-03]Least Squares Iteration:  50%|████▉     | 499/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3451e-03]Least Squares Iteration:  50%|█████     | 500/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3437e-03]Least Squares Iteration:  50%|█████     | 501/1000 [00:01&lt;00:02, 229.96iter/s, Error=1.3422e-03]Least Squares Iteration:  50%|█████     | 502/1000 [00:01&lt;00:02, 215.55iter/s, Error=1.3422e-03]Least Squares Iteration:  50%|█████     | 502/1000 [00:01&lt;00:02, 215.55iter/s, Error=1.3408e-03]Least Squares Iteration:  50%|█████     | 503/1000 [00:01&lt;00:02, 215.55iter/s, Error=1.3393e-03]Least Squares Iteration:  50%|█████     | 504/1000 [00:01&lt;00:02, 215.55iter/s, Error=1.3379e-03]Least Squares Iteration:  50%|█████     | 505/1000 [00:01&lt;00:02, 215.55iter/s, Error=1.3364e-03]Least Squares Iteration:  51%|█████     | 506/1000 [00:01&lt;00:02, 215.55iter/s, Error=1.3350e-03]Least Squares Iteration:  51%|█████     | 507/1000 [00:01&lt;00:02, 215.55iter/s, Error=1.3336e-03]Least Squares Iteration:  51%|█████     | 508/1000 [00:01&lt;00:02, 215.55iter/s, Error=1.3322e-03]Least Squares Iteration:  51%|█████     | 509/1000 [00:01&lt;00:02, 215.55iter/s, Error=1.3307e-03]Least Squares Iteration:  51%|█████     | 510/1000 [00:01&lt;00:02, 215.55iter/s, Error=1.3293e-03]Least Squares Iteration:  51%|█████     | 511/1000 [00:01&lt;00:02, 215.55iter/s, Error=1.3279e-03]Least Squares Iteration:  51%|█████     | 512/1000 [00:01&lt;00:02, 215.55iter/s, Error=1.3265e-03]Least Squares Iteration:  51%|█████▏    | 513/1000 [00:01&lt;00:02, 215.55iter/s, Error=1.3251e-03]Least Squares Iteration:  51%|█████▏    | 514/1000 [00:01&lt;00:02, 215.55iter/s, Error=1.3237e-03]Least Squares Iteration:  52%|█████▏    | 515/1000 [00:01&lt;00:02, 215.55iter/s, Error=1.3223e-03]Least Squares Iteration:  52%|█████▏    | 516/1000 [00:01&lt;00:02, 215.55iter/s, Error=1.3209e-03]Least Squares Iteration:  52%|█████▏    | 517/1000 [00:01&lt;00:02, 215.55iter/s, Error=1.3195e-03]Least Squares Iteration:  52%|█████▏    | 518/1000 [00:01&lt;00:02, 215.55iter/s, Error=1.3182e-03]Least Squares Iteration:  52%|█████▏    | 519/1000 [00:01&lt;00:02, 215.55iter/s, Error=1.3168e-03]Least Squares Iteration:  52%|█████▏    | 520/1000 [00:01&lt;00:02, 215.55iter/s, Error=1.3154e-03]Least Squares Iteration:  52%|█████▏    | 521/1000 [00:01&lt;00:02, 215.55iter/s, Error=1.3141e-03]Least Squares Iteration:  52%|█████▏    | 522/1000 [00:01&lt;00:02, 215.55iter/s, Error=1.3127e-03]Least Squares Iteration:  52%|█████▏    | 523/1000 [00:01&lt;00:02, 215.55iter/s, Error=1.3113e-03]Least Squares Iteration:  52%|█████▏    | 524/1000 [00:01&lt;00:02, 197.33iter/s, Error=1.3113e-03]Least Squares Iteration:  52%|█████▏    | 524/1000 [00:01&lt;00:02, 197.33iter/s, Error=1.3100e-03]Least Squares Iteration:  52%|█████▎    | 525/1000 [00:01&lt;00:02, 197.33iter/s, Error=1.3086e-03]Least Squares Iteration:  53%|█████▎    | 526/1000 [00:01&lt;00:02, 197.33iter/s, Error=1.3073e-03]Least Squares Iteration:  53%|█████▎    | 527/1000 [00:01&lt;00:02, 197.33iter/s, Error=1.3060e-03]Least Squares Iteration:  53%|█████▎    | 528/1000 [00:01&lt;00:02, 197.33iter/s, Error=1.3046e-03]Least Squares Iteration:  53%|█████▎    | 529/1000 [00:01&lt;00:02, 197.33iter/s, Error=1.3033e-03]Least Squares Iteration:  53%|█████▎    | 530/1000 [00:01&lt;00:02, 197.33iter/s, Error=1.3020e-03]Least Squares Iteration:  53%|█████▎    | 531/1000 [00:01&lt;00:02, 197.33iter/s, Error=1.3006e-03]Least Squares Iteration:  53%|█████▎    | 532/1000 [00:01&lt;00:02, 197.33iter/s, Error=1.2993e-03]Least Squares Iteration:  53%|█████▎    | 533/1000 [00:01&lt;00:02, 197.33iter/s, Error=1.2980e-03]Least Squares Iteration:  53%|█████▎    | 534/1000 [00:01&lt;00:02, 197.33iter/s, Error=1.2967e-03]Least Squares Iteration:  54%|█████▎    | 535/1000 [00:01&lt;00:02, 197.33iter/s, Error=1.2954e-03]Least Squares Iteration:  54%|█████▎    | 536/1000 [00:01&lt;00:02, 197.33iter/s, Error=1.2941e-03]Least Squares Iteration:  54%|█████▎    | 537/1000 [00:01&lt;00:02, 197.33iter/s, Error=1.2928e-03]Least Squares Iteration:  54%|█████▍    | 538/1000 [00:01&lt;00:02, 197.33iter/s, Error=1.2915e-03]Least Squares Iteration:  54%|█████▍    | 539/1000 [00:01&lt;00:02, 197.33iter/s, Error=1.2902e-03]Least Squares Iteration:  54%|█████▍    | 540/1000 [00:01&lt;00:02, 197.33iter/s, Error=1.2889e-03]Least Squares Iteration:  54%|█████▍    | 541/1000 [00:01&lt;00:02, 197.33iter/s, Error=1.2876e-03]Least Squares Iteration:  54%|█████▍    | 542/1000 [00:01&lt;00:02, 197.33iter/s, Error=1.2863e-03]Least Squares Iteration:  54%|█████▍    | 543/1000 [00:01&lt;00:02, 197.33iter/s, Error=1.2851e-03]Least Squares Iteration:  54%|█████▍    | 544/1000 [00:01&lt;00:02, 179.14iter/s, Error=1.2851e-03]Least Squares Iteration:  54%|█████▍    | 544/1000 [00:01&lt;00:02, 179.14iter/s, Error=1.2838e-03]Least Squares Iteration:  55%|█████▍    | 545/1000 [00:01&lt;00:02, 179.14iter/s, Error=1.2825e-03]Least Squares Iteration:  55%|█████▍    | 546/1000 [00:01&lt;00:02, 179.14iter/s, Error=1.2813e-03]Least Squares Iteration:  55%|█████▍    | 547/1000 [00:01&lt;00:02, 179.14iter/s, Error=1.2800e-03]Least Squares Iteration:  55%|█████▍    | 548/1000 [00:01&lt;00:02, 179.14iter/s, Error=1.2787e-03]Least Squares Iteration:  55%|█████▍    | 549/1000 [00:01&lt;00:02, 179.14iter/s, Error=1.2775e-03]Least Squares Iteration:  55%|█████▌    | 550/1000 [00:01&lt;00:02, 179.14iter/s, Error=1.2762e-03]Least Squares Iteration:  55%|█████▌    | 551/1000 [00:01&lt;00:02, 179.14iter/s, Error=1.2750e-03]Least Squares Iteration:  55%|█████▌    | 552/1000 [00:02&lt;00:02, 179.14iter/s, Error=1.2737e-03]Least Squares Iteration:  55%|█████▌    | 553/1000 [00:02&lt;00:02, 179.14iter/s, Error=1.2725e-03]Least Squares Iteration:  55%|█████▌    | 554/1000 [00:02&lt;00:02, 179.14iter/s, Error=1.2713e-03]Least Squares Iteration:  56%|█████▌    | 555/1000 [00:02&lt;00:02, 179.14iter/s, Error=1.2700e-03]Least Squares Iteration:  56%|█████▌    | 556/1000 [00:02&lt;00:02, 179.14iter/s, Error=1.2688e-03]Least Squares Iteration:  56%|█████▌    | 557/1000 [00:02&lt;00:02, 179.14iter/s, Error=1.2676e-03]Least Squares Iteration:  56%|█████▌    | 558/1000 [00:02&lt;00:02, 179.14iter/s, Error=1.2664e-03]Least Squares Iteration:  56%|█████▌    | 559/1000 [00:02&lt;00:02, 179.14iter/s, Error=1.2651e-03]Least Squares Iteration:  56%|█████▌    | 560/1000 [00:02&lt;00:02, 179.14iter/s, Error=1.2639e-03]Least Squares Iteration:  56%|█████▌    | 561/1000 [00:02&lt;00:02, 179.14iter/s, Error=1.2627e-03]Least Squares Iteration:  56%|█████▌    | 562/1000 [00:02&lt;00:02, 179.14iter/s, Error=1.2615e-03]Least Squares Iteration:  56%|█████▋    | 563/1000 [00:02&lt;00:02, 154.83iter/s, Error=1.2615e-03]Least Squares Iteration:  56%|█████▋    | 563/1000 [00:02&lt;00:02, 154.83iter/s, Error=1.2603e-03]Least Squares Iteration:  56%|█████▋    | 564/1000 [00:02&lt;00:02, 154.83iter/s, Error=1.2591e-03]Least Squares Iteration:  56%|█████▋    | 565/1000 [00:02&lt;00:02, 154.83iter/s, Error=1.2579e-03]Least Squares Iteration:  57%|█████▋    | 566/1000 [00:02&lt;00:02, 154.83iter/s, Error=1.2567e-03]Least Squares Iteration:  57%|█████▋    | 567/1000 [00:02&lt;00:02, 154.83iter/s, Error=1.2555e-03]Least Squares Iteration:  57%|█████▋    | 568/1000 [00:02&lt;00:02, 154.83iter/s, Error=1.2543e-03]Least Squares Iteration:  57%|█████▋    | 569/1000 [00:02&lt;00:02, 154.83iter/s, Error=1.2531e-03]Least Squares Iteration:  57%|█████▋    | 570/1000 [00:02&lt;00:02, 154.83iter/s, Error=1.2520e-03]Least Squares Iteration:  57%|█████▋    | 571/1000 [00:02&lt;00:02, 154.83iter/s, Error=1.2508e-03]Least Squares Iteration:  57%|█████▋    | 572/1000 [00:02&lt;00:02, 154.83iter/s, Error=1.2496e-03]Least Squares Iteration:  57%|█████▋    | 573/1000 [00:02&lt;00:02, 154.83iter/s, Error=1.2484e-03]Least Squares Iteration:  57%|█████▋    | 574/1000 [00:02&lt;00:02, 154.83iter/s, Error=1.2473e-03]Least Squares Iteration:  57%|█████▊    | 575/1000 [00:02&lt;00:02, 154.83iter/s, Error=1.2461e-03]Least Squares Iteration:  58%|█████▊    | 576/1000 [00:02&lt;00:02, 154.83iter/s, Error=1.2449e-03]Least Squares Iteration:  58%|█████▊    | 577/1000 [00:02&lt;00:02, 154.83iter/s, Error=1.2438e-03]Least Squares Iteration:  58%|█████▊    | 578/1000 [00:02&lt;00:02, 154.83iter/s, Error=1.2426e-03]Least Squares Iteration:  58%|█████▊    | 579/1000 [00:02&lt;00:02, 154.73iter/s, Error=1.2426e-03]Least Squares Iteration:  58%|█████▊    | 579/1000 [00:02&lt;00:02, 154.73iter/s, Error=1.2415e-03]Least Squares Iteration:  58%|█████▊    | 580/1000 [00:02&lt;00:02, 154.73iter/s, Error=1.2403e-03]Least Squares Iteration:  58%|█████▊    | 581/1000 [00:02&lt;00:02, 154.73iter/s, Error=1.2392e-03]Least Squares Iteration:  58%|█████▊    | 582/1000 [00:02&lt;00:02, 154.73iter/s, Error=1.2380e-03]Least Squares Iteration:  58%|█████▊    | 583/1000 [00:02&lt;00:02, 154.73iter/s, Error=1.2369e-03]Least Squares Iteration:  58%|█████▊    | 584/1000 [00:02&lt;00:02, 154.73iter/s, Error=1.2357e-03]Least Squares Iteration:  58%|█████▊    | 585/1000 [00:02&lt;00:02, 154.73iter/s, Error=1.2346e-03]Least Squares Iteration:  59%|█████▊    | 586/1000 [00:02&lt;00:02, 154.73iter/s, Error=1.2335e-03]Least Squares Iteration:  59%|█████▊    | 587/1000 [00:02&lt;00:02, 154.73iter/s, Error=1.2323e-03]Least Squares Iteration:  59%|█████▉    | 588/1000 [00:02&lt;00:02, 154.73iter/s, Error=1.2312e-03]Least Squares Iteration:  59%|█████▉    | 589/1000 [00:02&lt;00:02, 154.73iter/s, Error=1.2301e-03]Least Squares Iteration:  59%|█████▉    | 590/1000 [00:02&lt;00:02, 154.73iter/s, Error=1.2290e-03]Least Squares Iteration:  59%|█████▉    | 591/1000 [00:02&lt;00:02, 154.73iter/s, Error=1.2278e-03]Least Squares Iteration:  59%|█████▉    | 592/1000 [00:02&lt;00:02, 154.73iter/s, Error=1.2267e-03]Least Squares Iteration:  59%|█████▉    | 593/1000 [00:02&lt;00:02, 154.73iter/s, Error=1.2256e-03]Least Squares Iteration:  59%|█████▉    | 594/1000 [00:02&lt;00:02, 154.73iter/s, Error=1.2245e-03]Least Squares Iteration:  60%|█████▉    | 595/1000 [00:02&lt;00:02, 153.95iter/s, Error=1.2245e-03]Least Squares Iteration:  60%|█████▉    | 595/1000 [00:02&lt;00:02, 153.95iter/s, Error=1.2234e-03]Least Squares Iteration:  60%|█████▉    | 596/1000 [00:02&lt;00:02, 153.95iter/s, Error=1.2223e-03]Least Squares Iteration:  60%|█████▉    | 597/1000 [00:02&lt;00:02, 153.95iter/s, Error=1.2212e-03]Least Squares Iteration:  60%|█████▉    | 598/1000 [00:02&lt;00:02, 153.95iter/s, Error=1.2201e-03]Least Squares Iteration:  60%|█████▉    | 599/1000 [00:02&lt;00:02, 153.95iter/s, Error=1.2190e-03]Least Squares Iteration:  60%|██████    | 600/1000 [00:02&lt;00:02, 153.95iter/s, Error=1.2179e-03]Least Squares Iteration:  60%|██████    | 601/1000 [00:02&lt;00:02, 153.95iter/s, Error=1.2168e-03]Least Squares Iteration:  60%|██████    | 602/1000 [00:02&lt;00:02, 153.95iter/s, Error=1.2157e-03]Least Squares Iteration:  60%|██████    | 603/1000 [00:02&lt;00:02, 153.95iter/s, Error=1.2146e-03]Least Squares Iteration:  60%|██████    | 604/1000 [00:02&lt;00:02, 153.95iter/s, Error=1.2136e-03]Least Squares Iteration:  60%|██████    | 605/1000 [00:02&lt;00:02, 153.95iter/s, Error=1.2125e-03]Least Squares Iteration:  61%|██████    | 606/1000 [00:02&lt;00:02, 153.95iter/s, Error=1.2114e-03]Least Squares Iteration:  61%|██████    | 607/1000 [00:02&lt;00:02, 153.95iter/s, Error=1.2103e-03]Least Squares Iteration:  61%|██████    | 608/1000 [00:02&lt;00:02, 153.95iter/s, Error=1.2092e-03]Least Squares Iteration:  61%|██████    | 609/1000 [00:02&lt;00:02, 153.95iter/s, Error=1.2082e-03]Least Squares Iteration:  61%|██████    | 610/1000 [00:02&lt;00:02, 153.95iter/s, Error=1.2071e-03]Least Squares Iteration:  61%|██████    | 611/1000 [00:02&lt;00:02, 152.00iter/s, Error=1.2071e-03]Least Squares Iteration:  61%|██████    | 611/1000 [00:02&lt;00:02, 152.00iter/s, Error=1.2060e-03]Least Squares Iteration:  61%|██████    | 612/1000 [00:02&lt;00:02, 152.00iter/s, Error=1.2050e-03]Least Squares Iteration:  61%|██████▏   | 613/1000 [00:02&lt;00:02, 152.00iter/s, Error=1.2039e-03]Least Squares Iteration:  61%|██████▏   | 614/1000 [00:02&lt;00:02, 152.00iter/s, Error=1.2029e-03]Least Squares Iteration:  62%|██████▏   | 615/1000 [00:02&lt;00:02, 152.00iter/s, Error=1.2018e-03]Least Squares Iteration:  62%|██████▏   | 616/1000 [00:02&lt;00:02, 152.00iter/s, Error=1.2008e-03]Least Squares Iteration:  62%|██████▏   | 617/1000 [00:02&lt;00:02, 152.00iter/s, Error=1.1997e-03]Least Squares Iteration:  62%|██████▏   | 618/1000 [00:02&lt;00:02, 152.00iter/s, Error=1.1987e-03]Least Squares Iteration:  62%|██████▏   | 619/1000 [00:02&lt;00:02, 152.00iter/s, Error=1.1976e-03]Least Squares Iteration:  62%|██████▏   | 620/1000 [00:02&lt;00:02, 152.00iter/s, Error=1.1966e-03]Least Squares Iteration:  62%|██████▏   | 621/1000 [00:02&lt;00:02, 152.00iter/s, Error=1.1955e-03]Least Squares Iteration:  62%|██████▏   | 622/1000 [00:02&lt;00:02, 152.00iter/s, Error=1.1945e-03]Least Squares Iteration:  62%|██████▏   | 623/1000 [00:02&lt;00:02, 152.00iter/s, Error=1.1935e-03]Least Squares Iteration:  62%|██████▏   | 624/1000 [00:02&lt;00:02, 152.00iter/s, Error=1.1924e-03]Least Squares Iteration:  62%|██████▎   | 625/1000 [00:02&lt;00:02, 152.00iter/s, Error=1.1914e-03]Least Squares Iteration:  63%|██████▎   | 626/1000 [00:02&lt;00:02, 152.00iter/s, Error=1.1904e-03]Least Squares Iteration:  63%|██████▎   | 627/1000 [00:02&lt;00:02, 147.35iter/s, Error=1.1904e-03]Least Squares Iteration:  63%|██████▎   | 627/1000 [00:02&lt;00:02, 147.35iter/s, Error=1.1893e-03]Least Squares Iteration:  63%|██████▎   | 628/1000 [00:02&lt;00:02, 147.35iter/s, Error=1.1883e-03]Least Squares Iteration:  63%|██████▎   | 629/1000 [00:02&lt;00:02, 147.35iter/s, Error=1.1873e-03]Least Squares Iteration:  63%|██████▎   | 630/1000 [00:02&lt;00:02, 147.35iter/s, Error=1.1863e-03]Least Squares Iteration:  63%|██████▎   | 631/1000 [00:02&lt;00:02, 147.35iter/s, Error=1.1853e-03]Least Squares Iteration:  63%|██████▎   | 632/1000 [00:02&lt;00:02, 147.35iter/s, Error=1.1843e-03]Least Squares Iteration:  63%|██████▎   | 633/1000 [00:02&lt;00:02, 147.35iter/s, Error=1.1832e-03]Least Squares Iteration:  63%|██████▎   | 634/1000 [00:02&lt;00:02, 147.35iter/s, Error=1.1822e-03]Least Squares Iteration:  64%|██████▎   | 635/1000 [00:02&lt;00:02, 147.35iter/s, Error=1.1812e-03]Least Squares Iteration:  64%|██████▎   | 636/1000 [00:02&lt;00:02, 147.35iter/s, Error=1.1802e-03]Least Squares Iteration:  64%|██████▎   | 637/1000 [00:02&lt;00:02, 147.35iter/s, Error=1.1792e-03]Least Squares Iteration:  64%|██████▍   | 638/1000 [00:02&lt;00:02, 147.35iter/s, Error=1.1782e-03]Least Squares Iteration:  64%|██████▍   | 639/1000 [00:02&lt;00:02, 147.35iter/s, Error=1.1772e-03]Least Squares Iteration:  64%|██████▍   | 640/1000 [00:02&lt;00:02, 147.35iter/s, Error=1.1762e-03]Least Squares Iteration:  64%|██████▍   | 641/1000 [00:02&lt;00:02, 147.35iter/s, Error=1.1752e-03]Least Squares Iteration:  64%|██████▍   | 642/1000 [00:02&lt;00:02, 145.93iter/s, Error=1.1752e-03]Least Squares Iteration:  64%|██████▍   | 642/1000 [00:02&lt;00:02, 145.93iter/s, Error=1.1742e-03]Least Squares Iteration:  64%|██████▍   | 643/1000 [00:02&lt;00:02, 145.93iter/s, Error=1.1733e-03]Least Squares Iteration:  64%|██████▍   | 644/1000 [00:02&lt;00:02, 145.93iter/s, Error=1.1723e-03]Least Squares Iteration:  64%|██████▍   | 645/1000 [00:02&lt;00:02, 145.93iter/s, Error=1.1713e-03]Least Squares Iteration:  65%|██████▍   | 646/1000 [00:02&lt;00:02, 145.93iter/s, Error=1.1703e-03]Least Squares Iteration:  65%|██████▍   | 647/1000 [00:02&lt;00:02, 145.93iter/s, Error=1.1693e-03]Least Squares Iteration:  65%|██████▍   | 648/1000 [00:02&lt;00:02, 145.93iter/s, Error=1.1683e-03]Least Squares Iteration:  65%|██████▍   | 649/1000 [00:02&lt;00:02, 145.93iter/s, Error=1.1674e-03]Least Squares Iteration:  65%|██████▌   | 650/1000 [00:02&lt;00:02, 145.93iter/s, Error=1.1664e-03]Least Squares Iteration:  65%|██████▌   | 651/1000 [00:02&lt;00:02, 145.93iter/s, Error=1.1654e-03]Least Squares Iteration:  65%|██████▌   | 652/1000 [00:02&lt;00:02, 145.93iter/s, Error=1.1645e-03]Least Squares Iteration:  65%|██████▌   | 653/1000 [00:02&lt;00:02, 145.93iter/s, Error=1.1635e-03]Least Squares Iteration:  65%|██████▌   | 654/1000 [00:02&lt;00:02, 145.93iter/s, Error=1.1625e-03]Least Squares Iteration:  66%|██████▌   | 655/1000 [00:02&lt;00:02, 145.93iter/s, Error=1.1616e-03]Least Squares Iteration:  66%|██████▌   | 656/1000 [00:02&lt;00:02, 145.93iter/s, Error=1.1606e-03]Least Squares Iteration:  66%|██████▌   | 657/1000 [00:02&lt;00:02, 143.17iter/s, Error=1.1606e-03]Least Squares Iteration:  66%|██████▌   | 657/1000 [00:02&lt;00:02, 143.17iter/s, Error=1.1596e-03]Least Squares Iteration:  66%|██████▌   | 658/1000 [00:02&lt;00:02, 143.17iter/s, Error=1.1587e-03]Least Squares Iteration:  66%|██████▌   | 659/1000 [00:02&lt;00:02, 143.17iter/s, Error=1.1577e-03]Least Squares Iteration:  66%|██████▌   | 660/1000 [00:02&lt;00:02, 143.17iter/s, Error=1.1568e-03]Least Squares Iteration:  66%|██████▌   | 661/1000 [00:02&lt;00:02, 143.17iter/s, Error=1.1558e-03]Least Squares Iteration:  66%|██████▌   | 662/1000 [00:02&lt;00:02, 143.17iter/s, Error=1.1549e-03]Least Squares Iteration:  66%|██████▋   | 663/1000 [00:02&lt;00:02, 143.17iter/s, Error=1.1539e-03]Least Squares Iteration:  66%|██████▋   | 664/1000 [00:02&lt;00:02, 143.17iter/s, Error=1.1530e-03]Least Squares Iteration:  66%|██████▋   | 665/1000 [00:02&lt;00:02, 143.17iter/s, Error=1.1520e-03]Least Squares Iteration:  67%|██████▋   | 666/1000 [00:02&lt;00:02, 143.17iter/s, Error=1.1511e-03]Least Squares Iteration:  67%|██████▋   | 667/1000 [00:02&lt;00:02, 143.17iter/s, Error=1.1502e-03]Least Squares Iteration:  67%|██████▋   | 668/1000 [00:02&lt;00:02, 143.17iter/s, Error=1.1492e-03]Least Squares Iteration:  67%|██████▋   | 669/1000 [00:02&lt;00:02, 143.17iter/s, Error=1.1483e-03]Least Squares Iteration:  67%|██████▋   | 670/1000 [00:02&lt;00:02, 143.17iter/s, Error=1.1474e-03]Least Squares Iteration:  67%|██████▋   | 671/1000 [00:02&lt;00:02, 143.17iter/s, Error=1.1464e-03]Least Squares Iteration:  67%|██████▋   | 672/1000 [00:02&lt;00:02, 136.27iter/s, Error=1.1464e-03]Least Squares Iteration:  67%|██████▋   | 672/1000 [00:02&lt;00:02, 136.27iter/s, Error=1.1455e-03]Least Squares Iteration:  67%|██████▋   | 673/1000 [00:02&lt;00:02, 136.27iter/s, Error=1.1446e-03]Least Squares Iteration:  67%|██████▋   | 674/1000 [00:02&lt;00:02, 136.27iter/s, Error=1.1436e-03]Least Squares Iteration:  68%|██████▊   | 675/1000 [00:02&lt;00:02, 136.27iter/s, Error=1.1427e-03]Least Squares Iteration:  68%|██████▊   | 676/1000 [00:02&lt;00:02, 136.27iter/s, Error=1.1418e-03]Least Squares Iteration:  68%|██████▊   | 677/1000 [00:02&lt;00:02, 136.27iter/s, Error=1.1409e-03]Least Squares Iteration:  68%|██████▊   | 678/1000 [00:02&lt;00:02, 136.27iter/s, Error=1.1400e-03]Least Squares Iteration:  68%|██████▊   | 679/1000 [00:02&lt;00:02, 136.27iter/s, Error=1.1390e-03]Least Squares Iteration:  68%|██████▊   | 680/1000 [00:02&lt;00:02, 136.27iter/s, Error=1.1381e-03]Least Squares Iteration:  68%|██████▊   | 681/1000 [00:02&lt;00:02, 136.27iter/s, Error=1.1372e-03]Least Squares Iteration:  68%|██████▊   | 682/1000 [00:02&lt;00:02, 136.27iter/s, Error=1.1363e-03]Least Squares Iteration:  68%|██████▊   | 683/1000 [00:02&lt;00:02, 136.27iter/s, Error=1.1354e-03]Least Squares Iteration:  68%|██████▊   | 684/1000 [00:02&lt;00:02, 136.27iter/s, Error=1.1345e-03]Least Squares Iteration:  68%|██████▊   | 685/1000 [00:02&lt;00:02, 136.27iter/s, Error=1.1336e-03]Least Squares Iteration:  69%|██████▊   | 686/1000 [00:02&lt;00:02, 132.58iter/s, Error=1.1336e-03]Least Squares Iteration:  69%|██████▊   | 686/1000 [00:02&lt;00:02, 132.58iter/s, Error=1.1327e-03]Least Squares Iteration:  69%|██████▊   | 687/1000 [00:03&lt;00:02, 132.58iter/s, Error=1.1318e-03]Least Squares Iteration:  69%|██████▉   | 688/1000 [00:03&lt;00:02, 132.58iter/s, Error=1.1309e-03]Least Squares Iteration:  69%|██████▉   | 689/1000 [00:03&lt;00:02, 132.58iter/s, Error=1.1300e-03]Least Squares Iteration:  69%|██████▉   | 690/1000 [00:03&lt;00:02, 132.58iter/s, Error=1.1291e-03]Least Squares Iteration:  69%|██████▉   | 691/1000 [00:03&lt;00:02, 132.58iter/s, Error=1.1282e-03]Least Squares Iteration:  69%|██████▉   | 692/1000 [00:03&lt;00:02, 132.58iter/s, Error=1.1273e-03]Least Squares Iteration:  69%|██████▉   | 693/1000 [00:03&lt;00:02, 132.58iter/s, Error=1.1264e-03]Least Squares Iteration:  69%|██████▉   | 694/1000 [00:03&lt;00:02, 132.58iter/s, Error=1.1255e-03]Least Squares Iteration:  70%|██████▉   | 695/1000 [00:03&lt;00:02, 132.58iter/s, Error=1.1246e-03]Least Squares Iteration:  70%|██████▉   | 696/1000 [00:03&lt;00:02, 132.58iter/s, Error=1.1238e-03]Least Squares Iteration:  70%|██████▉   | 697/1000 [00:03&lt;00:02, 132.58iter/s, Error=1.1229e-03]Least Squares Iteration:  70%|██████▉   | 698/1000 [00:03&lt;00:02, 132.58iter/s, Error=1.1220e-03]Least Squares Iteration:  70%|██████▉   | 699/1000 [00:03&lt;00:02, 132.58iter/s, Error=1.1211e-03]Least Squares Iteration:  70%|███████   | 700/1000 [00:03&lt;00:02, 131.91iter/s, Error=1.1211e-03]Least Squares Iteration:  70%|███████   | 700/1000 [00:03&lt;00:02, 131.91iter/s, Error=1.1202e-03]Least Squares Iteration:  70%|███████   | 701/1000 [00:03&lt;00:02, 131.91iter/s, Error=1.1194e-03]Least Squares Iteration:  70%|███████   | 702/1000 [00:03&lt;00:02, 131.91iter/s, Error=1.1185e-03]Least Squares Iteration:  70%|███████   | 703/1000 [00:03&lt;00:02, 131.91iter/s, Error=1.1176e-03]Least Squares Iteration:  70%|███████   | 704/1000 [00:03&lt;00:02, 131.91iter/s, Error=1.1167e-03]Least Squares Iteration:  70%|███████   | 705/1000 [00:03&lt;00:02, 131.91iter/s, Error=1.1159e-03]Least Squares Iteration:  71%|███████   | 706/1000 [00:03&lt;00:02, 131.91iter/s, Error=1.1150e-03]Least Squares Iteration:  71%|███████   | 707/1000 [00:03&lt;00:02, 131.91iter/s, Error=1.1141e-03]Least Squares Iteration:  71%|███████   | 708/1000 [00:03&lt;00:02, 131.91iter/s, Error=1.1133e-03]Least Squares Iteration:  71%|███████   | 709/1000 [00:03&lt;00:02, 131.91iter/s, Error=1.1124e-03]Least Squares Iteration:  71%|███████   | 710/1000 [00:03&lt;00:02, 131.91iter/s, Error=1.1116e-03]Least Squares Iteration:  71%|███████   | 711/1000 [00:03&lt;00:02, 131.91iter/s, Error=1.1107e-03]Least Squares Iteration:  71%|███████   | 712/1000 [00:03&lt;00:02, 131.91iter/s, Error=1.1098e-03]Least Squares Iteration:  71%|███████▏  | 713/1000 [00:03&lt;00:02, 131.91iter/s, Error=1.1090e-03]Least Squares Iteration:  71%|███████▏  | 714/1000 [00:03&lt;00:02, 125.34iter/s, Error=1.1090e-03]Least Squares Iteration:  71%|███████▏  | 714/1000 [00:03&lt;00:02, 125.34iter/s, Error=1.1081e-03]Least Squares Iteration:  72%|███████▏  | 715/1000 [00:03&lt;00:02, 125.34iter/s, Error=1.1073e-03]Least Squares Iteration:  72%|███████▏  | 716/1000 [00:03&lt;00:02, 125.34iter/s, Error=1.1064e-03]Least Squares Iteration:  72%|███████▏  | 717/1000 [00:03&lt;00:02, 125.34iter/s, Error=1.1056e-03]Least Squares Iteration:  72%|███████▏  | 718/1000 [00:03&lt;00:02, 125.34iter/s, Error=1.1047e-03]Least Squares Iteration:  72%|███████▏  | 719/1000 [00:03&lt;00:02, 125.34iter/s, Error=1.1039e-03]Least Squares Iteration:  72%|███████▏  | 720/1000 [00:03&lt;00:02, 125.34iter/s, Error=1.1030e-03]Least Squares Iteration:  72%|███████▏  | 721/1000 [00:03&lt;00:02, 125.34iter/s, Error=1.1022e-03]Least Squares Iteration:  72%|███████▏  | 722/1000 [00:03&lt;00:02, 125.34iter/s, Error=1.1014e-03]Least Squares Iteration:  72%|███████▏  | 723/1000 [00:03&lt;00:02, 125.34iter/s, Error=1.1005e-03]Least Squares Iteration:  72%|███████▏  | 724/1000 [00:03&lt;00:02, 125.34iter/s, Error=1.0997e-03]Least Squares Iteration:  72%|███████▎  | 725/1000 [00:03&lt;00:02, 125.34iter/s, Error=1.0988e-03]Least Squares Iteration:  73%|███████▎  | 726/1000 [00:03&lt;00:02, 125.34iter/s, Error=1.0980e-03]Least Squares Iteration:  73%|███████▎  | 727/1000 [00:03&lt;00:02, 107.91iter/s, Error=1.0980e-03]Least Squares Iteration:  73%|███████▎  | 727/1000 [00:03&lt;00:02, 107.91iter/s, Error=1.0972e-03]Least Squares Iteration:  73%|███████▎  | 728/1000 [00:03&lt;00:02, 107.91iter/s, Error=1.0963e-03]Least Squares Iteration:  73%|███████▎  | 729/1000 [00:03&lt;00:02, 107.91iter/s, Error=1.0955e-03]Least Squares Iteration:  73%|███████▎  | 730/1000 [00:03&lt;00:02, 107.91iter/s, Error=1.0947e-03]Least Squares Iteration:  73%|███████▎  | 731/1000 [00:03&lt;00:02, 107.91iter/s, Error=1.0938e-03]Least Squares Iteration:  73%|███████▎  | 732/1000 [00:03&lt;00:02, 107.91iter/s, Error=1.0930e-03]Least Squares Iteration:  73%|███████▎  | 733/1000 [00:03&lt;00:02, 107.91iter/s, Error=1.0922e-03]Least Squares Iteration:  73%|███████▎  | 734/1000 [00:03&lt;00:02, 107.91iter/s, Error=1.0914e-03]Least Squares Iteration:  74%|███████▎  | 735/1000 [00:03&lt;00:02, 107.91iter/s, Error=1.0906e-03]Least Squares Iteration:  74%|███████▎  | 736/1000 [00:03&lt;00:02, 107.91iter/s, Error=1.0897e-03]Least Squares Iteration:  74%|███████▎  | 737/1000 [00:03&lt;00:02, 107.91iter/s, Error=1.0889e-03]Least Squares Iteration:  74%|███████▍  | 738/1000 [00:03&lt;00:02, 107.91iter/s, Error=1.0881e-03]Least Squares Iteration:  74%|███████▍  | 739/1000 [00:03&lt;00:02, 97.02iter/s, Error=1.0881e-03] Least Squares Iteration:  74%|███████▍  | 739/1000 [00:03&lt;00:02, 97.02iter/s, Error=1.0873e-03]Least Squares Iteration:  74%|███████▍  | 740/1000 [00:03&lt;00:02, 97.02iter/s, Error=1.0865e-03]Least Squares Iteration:  74%|███████▍  | 741/1000 [00:03&lt;00:02, 97.02iter/s, Error=1.0857e-03]Least Squares Iteration:  74%|███████▍  | 742/1000 [00:03&lt;00:02, 97.02iter/s, Error=1.0849e-03]Least Squares Iteration:  74%|███████▍  | 743/1000 [00:03&lt;00:02, 97.02iter/s, Error=1.0840e-03]Least Squares Iteration:  74%|███████▍  | 744/1000 [00:03&lt;00:02, 97.02iter/s, Error=1.0832e-03]Least Squares Iteration:  74%|███████▍  | 745/1000 [00:03&lt;00:02, 97.02iter/s, Error=1.0824e-03]Least Squares Iteration:  75%|███████▍  | 746/1000 [00:03&lt;00:02, 97.02iter/s, Error=1.0816e-03]Least Squares Iteration:  75%|███████▍  | 747/1000 [00:03&lt;00:02, 97.02iter/s, Error=1.0808e-03]Least Squares Iteration:  75%|███████▍  | 748/1000 [00:03&lt;00:02, 97.02iter/s, Error=1.0800e-03]Least Squares Iteration:  75%|███████▍  | 749/1000 [00:03&lt;00:02, 97.02iter/s, Error=1.0792e-03]Least Squares Iteration:  75%|███████▌  | 750/1000 [00:03&lt;00:02, 97.02iter/s, Error=1.0784e-03]Least Squares Iteration:  75%|███████▌  | 751/1000 [00:03&lt;00:02, 100.56iter/s, Error=1.0784e-03]Least Squares Iteration:  75%|███████▌  | 751/1000 [00:03&lt;00:02, 100.56iter/s, Error=1.0776e-03]Least Squares Iteration:  75%|███████▌  | 752/1000 [00:03&lt;00:02, 100.56iter/s, Error=1.0768e-03]Least Squares Iteration:  75%|███████▌  | 753/1000 [00:03&lt;00:02, 100.56iter/s, Error=1.0760e-03]Least Squares Iteration:  75%|███████▌  | 754/1000 [00:03&lt;00:02, 100.56iter/s, Error=1.0752e-03]Least Squares Iteration:  76%|███████▌  | 755/1000 [00:03&lt;00:02, 100.56iter/s, Error=1.0745e-03]Least Squares Iteration:  76%|███████▌  | 756/1000 [00:03&lt;00:02, 100.56iter/s, Error=1.0737e-03]Least Squares Iteration:  76%|███████▌  | 757/1000 [00:03&lt;00:02, 100.56iter/s, Error=1.0729e-03]Least Squares Iteration:  76%|███████▌  | 758/1000 [00:03&lt;00:02, 100.56iter/s, Error=1.0721e-03]Least Squares Iteration:  76%|███████▌  | 759/1000 [00:03&lt;00:02, 100.56iter/s, Error=1.0713e-03]Least Squares Iteration:  76%|███████▌  | 760/1000 [00:03&lt;00:02, 100.56iter/s, Error=1.0705e-03]Least Squares Iteration:  76%|███████▌  | 761/1000 [00:03&lt;00:02, 100.56iter/s, Error=1.0697e-03]Least Squares Iteration:  76%|███████▌  | 762/1000 [00:03&lt;00:02, 99.30iter/s, Error=1.0697e-03] Least Squares Iteration:  76%|███████▌  | 762/1000 [00:03&lt;00:02, 99.30iter/s, Error=1.0690e-03]Least Squares Iteration:  76%|███████▋  | 763/1000 [00:03&lt;00:02, 99.30iter/s, Error=1.0682e-03]Least Squares Iteration:  76%|███████▋  | 764/1000 [00:03&lt;00:02, 99.30iter/s, Error=1.0674e-03]Least Squares Iteration:  76%|███████▋  | 765/1000 [00:03&lt;00:02, 99.30iter/s, Error=1.0666e-03]Least Squares Iteration:  77%|███████▋  | 766/1000 [00:03&lt;00:02, 99.30iter/s, Error=1.0658e-03]Least Squares Iteration:  77%|███████▋  | 767/1000 [00:03&lt;00:02, 99.30iter/s, Error=1.0651e-03]Least Squares Iteration:  77%|███████▋  | 768/1000 [00:03&lt;00:02, 99.30iter/s, Error=1.0643e-03]Least Squares Iteration:  77%|███████▋  | 769/1000 [00:03&lt;00:02, 99.30iter/s, Error=1.0635e-03]Least Squares Iteration:  77%|███████▋  | 770/1000 [00:03&lt;00:02, 99.30iter/s, Error=1.0627e-03]Least Squares Iteration:  77%|███████▋  | 771/1000 [00:03&lt;00:02, 99.30iter/s, Error=1.0620e-03]Least Squares Iteration:  77%|███████▋  | 772/1000 [00:03&lt;00:02, 99.30iter/s, Error=1.0612e-03]Least Squares Iteration:  77%|███████▋  | 773/1000 [00:03&lt;00:02, 100.87iter/s, Error=1.0612e-03]Least Squares Iteration:  77%|███████▋  | 773/1000 [00:03&lt;00:02, 100.87iter/s, Error=1.0604e-03]Least Squares Iteration:  77%|███████▋  | 774/1000 [00:03&lt;00:02, 100.87iter/s, Error=1.0597e-03]Least Squares Iteration:  78%|███████▊  | 775/1000 [00:03&lt;00:02, 100.87iter/s, Error=1.0589e-03]Least Squares Iteration:  78%|███████▊  | 776/1000 [00:03&lt;00:02, 100.87iter/s, Error=1.0581e-03]Least Squares Iteration:  78%|███████▊  | 777/1000 [00:03&lt;00:02, 100.87iter/s, Error=1.0574e-03]Least Squares Iteration:  78%|███████▊  | 778/1000 [00:03&lt;00:02, 100.87iter/s, Error=1.0566e-03]Least Squares Iteration:  78%|███████▊  | 779/1000 [00:03&lt;00:02, 100.87iter/s, Error=1.0559e-03]Least Squares Iteration:  78%|███████▊  | 780/1000 [00:03&lt;00:02, 100.87iter/s, Error=1.0551e-03]Least Squares Iteration:  78%|███████▊  | 781/1000 [00:03&lt;00:02, 100.87iter/s, Error=1.0544e-03]Least Squares Iteration:  78%|███████▊  | 782/1000 [00:03&lt;00:02, 100.87iter/s, Error=1.0536e-03]Least Squares Iteration:  78%|███████▊  | 783/1000 [00:03&lt;00:02, 100.87iter/s, Error=1.0528e-03]Least Squares Iteration:  78%|███████▊  | 784/1000 [00:03&lt;00:02, 103.00iter/s, Error=1.0528e-03]Least Squares Iteration:  78%|███████▊  | 784/1000 [00:03&lt;00:02, 103.00iter/s, Error=1.0521e-03]Least Squares Iteration:  78%|███████▊  | 785/1000 [00:03&lt;00:02, 103.00iter/s, Error=1.0513e-03]Least Squares Iteration:  79%|███████▊  | 786/1000 [00:04&lt;00:02, 103.00iter/s, Error=1.0506e-03]Least Squares Iteration:  79%|███████▊  | 787/1000 [00:04&lt;00:02, 103.00iter/s, Error=1.0498e-03]Least Squares Iteration:  79%|███████▉  | 788/1000 [00:04&lt;00:02, 103.00iter/s, Error=1.0491e-03]Least Squares Iteration:  79%|███████▉  | 789/1000 [00:04&lt;00:02, 103.00iter/s, Error=1.0484e-03]Least Squares Iteration:  79%|███████▉  | 790/1000 [00:04&lt;00:02, 103.00iter/s, Error=1.0476e-03]Least Squares Iteration:  79%|███████▉  | 791/1000 [00:04&lt;00:02, 103.00iter/s, Error=1.0469e-03]Least Squares Iteration:  79%|███████▉  | 792/1000 [00:04&lt;00:02, 103.00iter/s, Error=1.0461e-03]Least Squares Iteration:  79%|███████▉  | 793/1000 [00:04&lt;00:02, 103.00iter/s, Error=1.0454e-03]Least Squares Iteration:  79%|███████▉  | 794/1000 [00:04&lt;00:01, 103.00iter/s, Error=1.0446e-03]Least Squares Iteration:  80%|███████▉  | 795/1000 [00:04&lt;00:02, 100.48iter/s, Error=1.0446e-03]Least Squares Iteration:  80%|███████▉  | 795/1000 [00:04&lt;00:02, 100.48iter/s, Error=1.0439e-03]Least Squares Iteration:  80%|███████▉  | 796/1000 [00:04&lt;00:02, 100.48iter/s, Error=1.0432e-03]Least Squares Iteration:  80%|███████▉  | 797/1000 [00:04&lt;00:02, 100.48iter/s, Error=1.0424e-03]Least Squares Iteration:  80%|███████▉  | 798/1000 [00:04&lt;00:02, 100.48iter/s, Error=1.0417e-03]Least Squares Iteration:  80%|███████▉  | 799/1000 [00:04&lt;00:02, 100.48iter/s, Error=1.0410e-03]Least Squares Iteration:  80%|████████  | 800/1000 [00:04&lt;00:01, 100.48iter/s, Error=1.0402e-03]Least Squares Iteration:  80%|████████  | 801/1000 [00:04&lt;00:01, 100.48iter/s, Error=1.0395e-03]Least Squares Iteration:  80%|████████  | 802/1000 [00:04&lt;00:01, 100.48iter/s, Error=1.0388e-03]Least Squares Iteration:  80%|████████  | 803/1000 [00:04&lt;00:01, 100.48iter/s, Error=1.0380e-03]Least Squares Iteration:  80%|████████  | 804/1000 [00:04&lt;00:01, 100.48iter/s, Error=1.0373e-03]Least Squares Iteration:  80%|████████  | 805/1000 [00:04&lt;00:01, 100.48iter/s, Error=1.0366e-03]Least Squares Iteration:  81%|████████  | 806/1000 [00:04&lt;00:01, 101.96iter/s, Error=1.0366e-03]Least Squares Iteration:  81%|████████  | 806/1000 [00:04&lt;00:01, 101.96iter/s, Error=1.0359e-03]Least Squares Iteration:  81%|████████  | 807/1000 [00:04&lt;00:01, 101.96iter/s, Error=1.0351e-03]Least Squares Iteration:  81%|████████  | 808/1000 [00:04&lt;00:01, 101.96iter/s, Error=1.0344e-03]Least Squares Iteration:  81%|████████  | 809/1000 [00:04&lt;00:01, 101.96iter/s, Error=1.0337e-03]Least Squares Iteration:  81%|████████  | 810/1000 [00:04&lt;00:01, 101.96iter/s, Error=1.0330e-03]Least Squares Iteration:  81%|████████  | 811/1000 [00:04&lt;00:01, 101.96iter/s, Error=1.0323e-03]Least Squares Iteration:  81%|████████  | 812/1000 [00:04&lt;00:01, 101.96iter/s, Error=1.0316e-03]Least Squares Iteration:  81%|████████▏ | 813/1000 [00:04&lt;00:01, 101.96iter/s, Error=1.0308e-03]Least Squares Iteration:  81%|████████▏ | 814/1000 [00:04&lt;00:01, 101.96iter/s, Error=1.0301e-03]Least Squares Iteration:  82%|████████▏ | 815/1000 [00:04&lt;00:01, 101.96iter/s, Error=1.0294e-03]Least Squares Iteration:  82%|████████▏ | 816/1000 [00:04&lt;00:01, 101.96iter/s, Error=1.0287e-03]Least Squares Iteration:  82%|████████▏ | 817/1000 [00:04&lt;00:01, 102.06iter/s, Error=1.0287e-03]Least Squares Iteration:  82%|████████▏ | 817/1000 [00:04&lt;00:01, 102.06iter/s, Error=1.0280e-03]Least Squares Iteration:  82%|████████▏ | 818/1000 [00:04&lt;00:01, 102.06iter/s, Error=1.0273e-03]Least Squares Iteration:  82%|████████▏ | 819/1000 [00:04&lt;00:01, 102.06iter/s, Error=1.0266e-03]Least Squares Iteration:  82%|████████▏ | 820/1000 [00:04&lt;00:01, 102.06iter/s, Error=1.0259e-03]Least Squares Iteration:  82%|████████▏ | 821/1000 [00:04&lt;00:01, 102.06iter/s, Error=1.0251e-03]Least Squares Iteration:  82%|████████▏ | 822/1000 [00:04&lt;00:01, 102.06iter/s, Error=1.0244e-03]Least Squares Iteration:  82%|████████▏ | 823/1000 [00:04&lt;00:01, 102.06iter/s, Error=1.0237e-03]Least Squares Iteration:  82%|████████▏ | 824/1000 [00:04&lt;00:01, 102.06iter/s, Error=1.0230e-03]Least Squares Iteration:  82%|████████▎ | 825/1000 [00:04&lt;00:01, 102.06iter/s, Error=1.0223e-03]Least Squares Iteration:  83%|████████▎ | 826/1000 [00:04&lt;00:01, 102.06iter/s, Error=1.0216e-03]Least Squares Iteration:  83%|████████▎ | 827/1000 [00:04&lt;00:01, 102.06iter/s, Error=1.0209e-03]Least Squares Iteration:  83%|████████▎ | 828/1000 [00:04&lt;00:01, 100.89iter/s, Error=1.0209e-03]Least Squares Iteration:  83%|████████▎ | 828/1000 [00:04&lt;00:01, 100.89iter/s, Error=1.0202e-03]Least Squares Iteration:  83%|████████▎ | 829/1000 [00:04&lt;00:01, 100.89iter/s, Error=1.0195e-03]Least Squares Iteration:  83%|████████▎ | 830/1000 [00:04&lt;00:01, 100.89iter/s, Error=1.0188e-03]Least Squares Iteration:  83%|████████▎ | 831/1000 [00:04&lt;00:01, 100.89iter/s, Error=1.0181e-03]Least Squares Iteration:  83%|████████▎ | 832/1000 [00:04&lt;00:01, 100.89iter/s, Error=1.0174e-03]Least Squares Iteration:  83%|████████▎ | 833/1000 [00:04&lt;00:01, 100.89iter/s, Error=1.0168e-03]Least Squares Iteration:  83%|████████▎ | 834/1000 [00:04&lt;00:01, 100.89iter/s, Error=1.0161e-03]Least Squares Iteration:  84%|████████▎ | 835/1000 [00:04&lt;00:01, 100.89iter/s, Error=1.0154e-03]Least Squares Iteration:  84%|████████▎ | 836/1000 [00:04&lt;00:01, 100.89iter/s, Error=1.0147e-03]Least Squares Iteration:  84%|████████▎ | 837/1000 [00:04&lt;00:01, 100.89iter/s, Error=1.0140e-03]Least Squares Iteration:  84%|████████▍ | 838/1000 [00:04&lt;00:01, 100.89iter/s, Error=1.0133e-03]Least Squares Iteration:  84%|████████▍ | 839/1000 [00:04&lt;00:01, 99.66iter/s, Error=1.0133e-03] Least Squares Iteration:  84%|████████▍ | 839/1000 [00:04&lt;00:01, 99.66iter/s, Error=1.0126e-03]Least Squares Iteration:  84%|████████▍ | 840/1000 [00:04&lt;00:01, 99.66iter/s, Error=1.0119e-03]Least Squares Iteration:  84%|████████▍ | 841/1000 [00:04&lt;00:01, 99.66iter/s, Error=1.0113e-03]Least Squares Iteration:  84%|████████▍ | 842/1000 [00:04&lt;00:01, 99.66iter/s, Error=1.0106e-03]Least Squares Iteration:  84%|████████▍ | 843/1000 [00:04&lt;00:01, 99.66iter/s, Error=1.0099e-03]Least Squares Iteration:  84%|████████▍ | 844/1000 [00:04&lt;00:01, 99.66iter/s, Error=1.0092e-03]Least Squares Iteration:  84%|████████▍ | 845/1000 [00:04&lt;00:01, 99.66iter/s, Error=1.0085e-03]Least Squares Iteration:  85%|████████▍ | 846/1000 [00:04&lt;00:01, 99.66iter/s, Error=1.0078e-03]Least Squares Iteration:  85%|████████▍ | 847/1000 [00:04&lt;00:01, 99.66iter/s, Error=1.0072e-03]Least Squares Iteration:  85%|████████▍ | 848/1000 [00:04&lt;00:01, 99.66iter/s, Error=1.0065e-03]Least Squares Iteration:  85%|████████▍ | 849/1000 [00:04&lt;00:01, 99.66iter/s, Error=1.0058e-03]Least Squares Iteration:  85%|████████▌ | 850/1000 [00:04&lt;00:01, 101.26iter/s, Error=1.0058e-03]Least Squares Iteration:  85%|████████▌ | 850/1000 [00:04&lt;00:01, 101.26iter/s, Error=1.0051e-03]Least Squares Iteration:  85%|████████▌ | 851/1000 [00:04&lt;00:01, 101.26iter/s, Error=1.0045e-03]Least Squares Iteration:  85%|████████▌ | 852/1000 [00:04&lt;00:01, 101.26iter/s, Error=1.0038e-03]Least Squares Iteration:  85%|████████▌ | 853/1000 [00:04&lt;00:01, 101.26iter/s, Error=1.0031e-03]Least Squares Iteration:  85%|████████▌ | 854/1000 [00:04&lt;00:01, 101.26iter/s, Error=1.0025e-03]Least Squares Iteration:  86%|████████▌ | 855/1000 [00:04&lt;00:01, 101.26iter/s, Error=1.0018e-03]Least Squares Iteration:  86%|████████▌ | 856/1000 [00:04&lt;00:01, 101.26iter/s, Error=1.0011e-03]Least Squares Iteration:  86%|████████▌ | 857/1000 [00:04&lt;00:01, 101.26iter/s, Error=1.0005e-03]Least Squares Iteration:  86%|████████▌ | 858/1000 [00:04&lt;00:01, 101.26iter/s, Error=9.9979e-04]Least Squares Iteration:  86%|████████▌ | 859/1000 [00:04&lt;00:01, 101.26iter/s, Error=9.9913e-04]Least Squares Iteration:  86%|████████▌ | 860/1000 [00:04&lt;00:01, 101.26iter/s, Error=9.9846e-04]Least Squares Iteration:  86%|████████▌ | 861/1000 [00:04&lt;00:01, 94.90iter/s, Error=9.9846e-04] Least Squares Iteration:  86%|████████▌ | 861/1000 [00:04&lt;00:01, 94.90iter/s, Error=9.9780e-04]Least Squares Iteration:  86%|████████▌ | 862/1000 [00:04&lt;00:01, 94.90iter/s, Error=9.9714e-04]Least Squares Iteration:  86%|████████▋ | 863/1000 [00:04&lt;00:01, 94.90iter/s, Error=9.9647e-04]Least Squares Iteration:  86%|████████▋ | 864/1000 [00:04&lt;00:01, 94.90iter/s, Error=9.9582e-04]Least Squares Iteration:  86%|████████▋ | 865/1000 [00:04&lt;00:01, 94.90iter/s, Error=9.9516e-04]Least Squares Iteration:  87%|████████▋ | 866/1000 [00:04&lt;00:01, 94.90iter/s, Error=9.9450e-04]Least Squares Iteration:  87%|████████▋ | 867/1000 [00:04&lt;00:01, 94.90iter/s, Error=9.9385e-04]Least Squares Iteration:  87%|████████▋ | 868/1000 [00:04&lt;00:01, 94.90iter/s, Error=9.9319e-04]Least Squares Iteration:  87%|████████▋ | 869/1000 [00:04&lt;00:01, 94.90iter/s, Error=9.9254e-04]Least Squares Iteration:  87%|████████▋ | 870/1000 [00:04&lt;00:01, 94.90iter/s, Error=9.9189e-04]Least Squares Iteration:  87%|████████▋ | 871/1000 [00:04&lt;00:01, 94.62iter/s, Error=9.9189e-04]Least Squares Iteration:  87%|████████▋ | 871/1000 [00:04&lt;00:01, 94.62iter/s, Error=9.9123e-04]Least Squares Iteration:  87%|████████▋ | 872/1000 [00:04&lt;00:01, 94.62iter/s, Error=9.9058e-04]Least Squares Iteration:  87%|████████▋ | 873/1000 [00:04&lt;00:01, 94.62iter/s, Error=9.8993e-04]Least Squares Iteration:  87%|████████▋ | 874/1000 [00:04&lt;00:01, 94.62iter/s, Error=9.8927e-04]Least Squares Iteration:  88%|████████▊ | 875/1000 [00:04&lt;00:01, 94.62iter/s, Error=9.8863e-04]Least Squares Iteration:  88%|████████▊ | 876/1000 [00:04&lt;00:01, 94.62iter/s, Error=9.8799e-04]Least Squares Iteration:  88%|████████▊ | 877/1000 [00:04&lt;00:01, 94.62iter/s, Error=9.8734e-04]Least Squares Iteration:  88%|████████▊ | 878/1000 [00:04&lt;00:01, 94.62iter/s, Error=9.8669e-04]Least Squares Iteration:  88%|████████▊ | 879/1000 [00:04&lt;00:01, 94.62iter/s, Error=9.8605e-04]Least Squares Iteration:  88%|████████▊ | 880/1000 [00:04&lt;00:01, 94.62iter/s, Error=9.8541e-04]Least Squares Iteration:  88%|████████▊ | 881/1000 [00:04&lt;00:01, 92.16iter/s, Error=9.8541e-04]Least Squares Iteration:  88%|████████▊ | 881/1000 [00:04&lt;00:01, 92.16iter/s, Error=9.8476e-04]Least Squares Iteration:  88%|████████▊ | 882/1000 [00:05&lt;00:01, 92.16iter/s, Error=9.8412e-04]Least Squares Iteration:  88%|████████▊ | 883/1000 [00:05&lt;00:01, 92.16iter/s, Error=9.8348e-04]Least Squares Iteration:  88%|████████▊ | 884/1000 [00:05&lt;00:01, 92.16iter/s, Error=9.8284e-04]Least Squares Iteration:  88%|████████▊ | 885/1000 [00:05&lt;00:01, 92.16iter/s, Error=9.8220e-04]Least Squares Iteration:  89%|████████▊ | 886/1000 [00:05&lt;00:01, 92.16iter/s, Error=9.8157e-04]Least Squares Iteration:  89%|████████▊ | 887/1000 [00:05&lt;00:01, 92.16iter/s, Error=9.8093e-04]Least Squares Iteration:  89%|████████▉ | 888/1000 [00:05&lt;00:01, 92.16iter/s, Error=9.8030e-04]Least Squares Iteration:  89%|████████▉ | 889/1000 [00:05&lt;00:01, 92.16iter/s, Error=9.7966e-04]Least Squares Iteration:  89%|████████▉ | 890/1000 [00:05&lt;00:01, 92.16iter/s, Error=9.7903e-04]Least Squares Iteration:  89%|████████▉ | 891/1000 [00:05&lt;00:01, 93.66iter/s, Error=9.7903e-04]Least Squares Iteration:  89%|████████▉ | 891/1000 [00:05&lt;00:01, 93.66iter/s, Error=9.7840e-04]Least Squares Iteration:  89%|████████▉ | 892/1000 [00:05&lt;00:01, 93.66iter/s, Error=9.7777e-04]Least Squares Iteration:  89%|████████▉ | 893/1000 [00:05&lt;00:01, 93.66iter/s, Error=9.7714e-04]Least Squares Iteration:  89%|████████▉ | 894/1000 [00:05&lt;00:01, 93.66iter/s, Error=9.7650e-04]Least Squares Iteration:  90%|████████▉ | 895/1000 [00:05&lt;00:01, 93.66iter/s, Error=9.7588e-04]Least Squares Iteration:  90%|████████▉ | 896/1000 [00:05&lt;00:01, 93.66iter/s, Error=9.7524e-04]Least Squares Iteration:  90%|████████▉ | 897/1000 [00:05&lt;00:01, 93.66iter/s, Error=9.7462e-04]Least Squares Iteration:  90%|████████▉ | 898/1000 [00:05&lt;00:01, 93.66iter/s, Error=9.7399e-04]Least Squares Iteration:  90%|████████▉ | 899/1000 [00:05&lt;00:01, 93.66iter/s, Error=9.7337e-04]Least Squares Iteration:  90%|█████████ | 900/1000 [00:05&lt;00:01, 93.66iter/s, Error=9.7274e-04]Least Squares Iteration:  90%|█████████ | 901/1000 [00:05&lt;00:01, 91.83iter/s, Error=9.7274e-04]Least Squares Iteration:  90%|█████████ | 901/1000 [00:05&lt;00:01, 91.83iter/s, Error=9.7212e-04]Least Squares Iteration:  90%|█████████ | 902/1000 [00:05&lt;00:01, 91.83iter/s, Error=9.7150e-04]Least Squares Iteration:  90%|█████████ | 903/1000 [00:05&lt;00:01, 91.83iter/s, Error=9.7088e-04]Least Squares Iteration:  90%|█████████ | 904/1000 [00:05&lt;00:01, 91.83iter/s, Error=9.7026e-04]Least Squares Iteration:  90%|█████████ | 905/1000 [00:05&lt;00:01, 91.83iter/s, Error=9.6964e-04]Least Squares Iteration:  91%|█████████ | 906/1000 [00:05&lt;00:01, 91.83iter/s, Error=9.6903e-04]Least Squares Iteration:  91%|█████████ | 907/1000 [00:05&lt;00:01, 91.83iter/s, Error=9.6841e-04]Least Squares Iteration:  91%|█████████ | 908/1000 [00:05&lt;00:01, 91.83iter/s, Error=9.6779e-04]Least Squares Iteration:  91%|█████████ | 909/1000 [00:05&lt;00:00, 91.83iter/s, Error=9.6717e-04]Least Squares Iteration:  91%|█████████ | 910/1000 [00:05&lt;00:00, 91.83iter/s, Error=9.6656e-04]Least Squares Iteration:  91%|█████████ | 911/1000 [00:05&lt;00:00, 90.45iter/s, Error=9.6656e-04]Least Squares Iteration:  91%|█████████ | 911/1000 [00:05&lt;00:00, 90.45iter/s, Error=9.6594e-04]Least Squares Iteration:  91%|█████████ | 912/1000 [00:05&lt;00:00, 90.45iter/s, Error=9.6534e-04]Least Squares Iteration:  91%|█████████▏| 913/1000 [00:05&lt;00:00, 90.45iter/s, Error=9.6472e-04]Least Squares Iteration:  91%|█████████▏| 914/1000 [00:05&lt;00:00, 90.45iter/s, Error=9.6411e-04]Least Squares Iteration:  92%|█████████▏| 915/1000 [00:05&lt;00:00, 90.45iter/s, Error=9.6350e-04]Least Squares Iteration:  92%|█████████▏| 916/1000 [00:05&lt;00:00, 90.45iter/s, Error=9.6289e-04]Least Squares Iteration:  92%|█████████▏| 917/1000 [00:05&lt;00:00, 90.45iter/s, Error=9.6228e-04]Least Squares Iteration:  92%|█████████▏| 918/1000 [00:05&lt;00:00, 90.45iter/s, Error=9.6168e-04]Least Squares Iteration:  92%|█████████▏| 919/1000 [00:05&lt;00:00, 90.45iter/s, Error=9.6107e-04]Least Squares Iteration:  92%|█████████▏| 920/1000 [00:05&lt;00:00, 90.45iter/s, Error=9.6047e-04]Least Squares Iteration:  92%|█████████▏| 921/1000 [00:05&lt;00:00, 88.33iter/s, Error=9.6047e-04]Least Squares Iteration:  92%|█████████▏| 921/1000 [00:05&lt;00:00, 88.33iter/s, Error=9.5987e-04]Least Squares Iteration:  92%|█████████▏| 922/1000 [00:05&lt;00:00, 88.33iter/s, Error=9.5926e-04]Least Squares Iteration:  92%|█████████▏| 923/1000 [00:05&lt;00:00, 88.33iter/s, Error=9.5866e-04]Least Squares Iteration:  92%|█████████▏| 924/1000 [00:05&lt;00:00, 88.33iter/s, Error=9.5805e-04]Least Squares Iteration:  92%|█████████▎| 925/1000 [00:05&lt;00:00, 88.33iter/s, Error=9.5745e-04]Least Squares Iteration:  93%|█████████▎| 926/1000 [00:05&lt;00:00, 88.33iter/s, Error=9.5686e-04]Least Squares Iteration:  93%|█████████▎| 927/1000 [00:05&lt;00:00, 88.33iter/s, Error=9.5625e-04]Least Squares Iteration:  93%|█████████▎| 928/1000 [00:05&lt;00:00, 88.33iter/s, Error=9.5565e-04]Least Squares Iteration:  93%|█████████▎| 929/1000 [00:05&lt;00:00, 88.33iter/s, Error=9.5506e-04]Least Squares Iteration:  93%|█████████▎| 930/1000 [00:05&lt;00:00, 87.82iter/s, Error=9.5506e-04]Least Squares Iteration:  93%|█████████▎| 930/1000 [00:05&lt;00:00, 87.82iter/s, Error=9.5446e-04]Least Squares Iteration:  93%|█████████▎| 931/1000 [00:05&lt;00:00, 87.82iter/s, Error=9.5386e-04]Least Squares Iteration:  93%|█████████▎| 932/1000 [00:05&lt;00:00, 87.82iter/s, Error=9.5327e-04]Least Squares Iteration:  93%|█████████▎| 933/1000 [00:05&lt;00:00, 87.82iter/s, Error=9.5268e-04]Least Squares Iteration:  93%|█████████▎| 934/1000 [00:05&lt;00:00, 87.82iter/s, Error=9.5209e-04]Least Squares Iteration:  94%|█████████▎| 935/1000 [00:05&lt;00:00, 87.82iter/s, Error=9.5149e-04]Least Squares Iteration:  94%|█████████▎| 936/1000 [00:05&lt;00:00, 87.82iter/s, Error=9.5091e-04]Least Squares Iteration:  94%|█████████▎| 937/1000 [00:05&lt;00:00, 87.82iter/s, Error=9.5032e-04]Least Squares Iteration:  94%|█████████▍| 938/1000 [00:05&lt;00:00, 87.82iter/s, Error=9.4972e-04]Least Squares Iteration:  94%|█████████▍| 939/1000 [00:05&lt;00:00, 85.13iter/s, Error=9.4972e-04]Least Squares Iteration:  94%|█████████▍| 939/1000 [00:05&lt;00:00, 85.13iter/s, Error=9.4913e-04]Least Squares Iteration:  94%|█████████▍| 940/1000 [00:05&lt;00:00, 85.13iter/s, Error=9.4855e-04]Least Squares Iteration:  94%|█████████▍| 941/1000 [00:05&lt;00:00, 85.13iter/s, Error=9.4796e-04]Least Squares Iteration:  94%|█████████▍| 942/1000 [00:05&lt;00:00, 85.13iter/s, Error=9.4737e-04]Least Squares Iteration:  94%|█████████▍| 943/1000 [00:05&lt;00:00, 85.13iter/s, Error=9.4679e-04]Least Squares Iteration:  94%|█████████▍| 944/1000 [00:05&lt;00:00, 85.13iter/s, Error=9.4620e-04]Least Squares Iteration:  94%|█████████▍| 945/1000 [00:05&lt;00:00, 85.13iter/s, Error=9.4561e-04]Least Squares Iteration:  95%|█████████▍| 946/1000 [00:05&lt;00:00, 85.13iter/s, Error=9.4504e-04]Least Squares Iteration:  95%|█████████▍| 947/1000 [00:05&lt;00:00, 85.13iter/s, Error=9.4445e-04]Least Squares Iteration:  95%|█████████▍| 948/1000 [00:05&lt;00:00, 83.66iter/s, Error=9.4445e-04]Least Squares Iteration:  95%|█████████▍| 948/1000 [00:05&lt;00:00, 83.66iter/s, Error=9.4388e-04]Least Squares Iteration:  95%|█████████▍| 949/1000 [00:05&lt;00:00, 83.66iter/s, Error=9.4329e-04]Least Squares Iteration:  95%|█████████▌| 950/1000 [00:05&lt;00:00, 83.66iter/s, Error=9.4271e-04]Least Squares Iteration:  95%|█████████▌| 951/1000 [00:05&lt;00:00, 83.66iter/s, Error=9.4214e-04]Least Squares Iteration:  95%|█████████▌| 952/1000 [00:05&lt;00:00, 83.66iter/s, Error=9.4156e-04]Least Squares Iteration:  95%|█████████▌| 953/1000 [00:05&lt;00:00, 83.66iter/s, Error=9.4098e-04]Least Squares Iteration:  95%|█████████▌| 954/1000 [00:05&lt;00:00, 83.66iter/s, Error=9.4041e-04]Least Squares Iteration:  96%|█████████▌| 955/1000 [00:05&lt;00:00, 83.66iter/s, Error=9.3983e-04]Least Squares Iteration:  96%|█████████▌| 956/1000 [00:05&lt;00:00, 83.66iter/s, Error=9.3926e-04]Least Squares Iteration:  96%|█████████▌| 957/1000 [00:05&lt;00:00, 82.56iter/s, Error=9.3926e-04]Least Squares Iteration:  96%|█████████▌| 957/1000 [00:05&lt;00:00, 82.56iter/s, Error=9.3869e-04]Least Squares Iteration:  96%|█████████▌| 958/1000 [00:05&lt;00:00, 82.56iter/s, Error=9.3811e-04]Least Squares Iteration:  96%|█████████▌| 959/1000 [00:05&lt;00:00, 82.56iter/s, Error=9.3754e-04]Least Squares Iteration:  96%|█████████▌| 960/1000 [00:05&lt;00:00, 82.56iter/s, Error=9.3698e-04]Least Squares Iteration:  96%|█████████▌| 961/1000 [00:05&lt;00:00, 82.56iter/s, Error=9.3640e-04]Least Squares Iteration:  96%|█████████▌| 962/1000 [00:05&lt;00:00, 82.56iter/s, Error=9.3583e-04]Least Squares Iteration:  96%|█████████▋| 963/1000 [00:05&lt;00:00, 82.56iter/s, Error=9.3526e-04]Least Squares Iteration:  96%|█████████▋| 964/1000 [00:05&lt;00:00, 82.56iter/s, Error=9.3469e-04]Least Squares Iteration:  96%|█████████▋| 965/1000 [00:06&lt;00:00, 82.56iter/s, Error=9.3413e-04]Least Squares Iteration:  97%|█████████▋| 966/1000 [00:06&lt;00:00, 79.10iter/s, Error=9.3413e-04]Least Squares Iteration:  97%|█████████▋| 966/1000 [00:06&lt;00:00, 79.10iter/s, Error=9.3356e-04]Least Squares Iteration:  97%|█████████▋| 967/1000 [00:06&lt;00:00, 79.10iter/s, Error=9.3300e-04]Least Squares Iteration:  97%|█████████▋| 968/1000 [00:06&lt;00:00, 79.10iter/s, Error=9.3243e-04]Least Squares Iteration:  97%|█████████▋| 969/1000 [00:06&lt;00:00, 79.10iter/s, Error=9.3187e-04]Least Squares Iteration:  97%|█████████▋| 970/1000 [00:06&lt;00:00, 79.10iter/s, Error=9.3130e-04]Least Squares Iteration:  97%|█████████▋| 971/1000 [00:06&lt;00:00, 79.10iter/s, Error=9.3075e-04]Least Squares Iteration:  97%|█████████▋| 972/1000 [00:06&lt;00:00, 79.10iter/s, Error=9.3018e-04]Least Squares Iteration:  97%|█████████▋| 973/1000 [00:06&lt;00:00, 79.10iter/s, Error=9.2962e-04]Least Squares Iteration:  97%|█████████▋| 974/1000 [00:06&lt;00:00, 71.62iter/s, Error=9.2962e-04]Least Squares Iteration:  97%|█████████▋| 974/1000 [00:06&lt;00:00, 71.62iter/s, Error=9.2907e-04]Least Squares Iteration:  98%|█████████▊| 975/1000 [00:06&lt;00:00, 71.62iter/s, Error=9.2851e-04]Least Squares Iteration:  98%|█████████▊| 976/1000 [00:06&lt;00:00, 71.62iter/s, Error=9.2795e-04]Least Squares Iteration:  98%|█████████▊| 977/1000 [00:06&lt;00:00, 71.62iter/s, Error=9.2739e-04]Least Squares Iteration:  98%|█████████▊| 978/1000 [00:06&lt;00:00, 71.62iter/s, Error=9.2683e-04]Least Squares Iteration:  98%|█████████▊| 979/1000 [00:06&lt;00:00, 71.62iter/s, Error=9.2628e-04]Least Squares Iteration:  98%|█████████▊| 980/1000 [00:06&lt;00:00, 71.62iter/s, Error=9.2573e-04]Least Squares Iteration:  98%|█████████▊| 981/1000 [00:06&lt;00:00, 71.62iter/s, Error=9.2517e-04]Least Squares Iteration:  98%|█████████▊| 982/1000 [00:06&lt;00:00, 71.98iter/s, Error=9.2517e-04]Least Squares Iteration:  98%|█████████▊| 982/1000 [00:06&lt;00:00, 71.98iter/s, Error=9.2462e-04]Least Squares Iteration:  98%|█████████▊| 983/1000 [00:06&lt;00:00, 71.98iter/s, Error=9.2406e-04]Least Squares Iteration:  98%|█████████▊| 984/1000 [00:06&lt;00:00, 71.98iter/s, Error=9.2352e-04]Least Squares Iteration:  98%|█████████▊| 985/1000 [00:06&lt;00:00, 71.98iter/s, Error=9.2297e-04]Least Squares Iteration:  99%|█████████▊| 986/1000 [00:06&lt;00:00, 71.98iter/s, Error=9.2241e-04]Least Squares Iteration:  99%|█████████▊| 987/1000 [00:06&lt;00:00, 71.98iter/s, Error=9.2186e-04]Least Squares Iteration:  99%|█████████▉| 988/1000 [00:06&lt;00:00, 71.98iter/s, Error=9.2131e-04]Least Squares Iteration:  99%|█████████▉| 989/1000 [00:06&lt;00:00, 71.98iter/s, Error=9.2077e-04]Least Squares Iteration:  99%|█████████▉| 990/1000 [00:06&lt;00:00, 72.66iter/s, Error=9.2077e-04]Least Squares Iteration:  99%|█████████▉| 990/1000 [00:06&lt;00:00, 72.66iter/s, Error=9.2022e-04]Least Squares Iteration:  99%|█████████▉| 991/1000 [00:06&lt;00:00, 72.66iter/s, Error=9.1967e-04]Least Squares Iteration:  99%|█████████▉| 992/1000 [00:06&lt;00:00, 72.66iter/s, Error=9.1914e-04]Least Squares Iteration:  99%|█████████▉| 993/1000 [00:06&lt;00:00, 72.66iter/s, Error=9.1859e-04]Least Squares Iteration:  99%|█████████▉| 994/1000 [00:06&lt;00:00, 72.66iter/s, Error=9.1804e-04]Least Squares Iteration: 100%|█████████▉| 995/1000 [00:06&lt;00:00, 72.66iter/s, Error=9.1750e-04]Least Squares Iteration: 100%|█████████▉| 996/1000 [00:06&lt;00:00, 72.66iter/s, Error=9.1695e-04]Least Squares Iteration: 100%|█████████▉| 997/1000 [00:06&lt;00:00, 72.66iter/s, Error=9.1642e-04]Least Squares Iteration: 100%|█████████▉| 998/1000 [00:06&lt;00:00, 72.86iter/s, Error=9.1642e-04]Least Squares Iteration: 100%|█████████▉| 998/1000 [00:06&lt;00:00, 72.86iter/s, Error=9.1587e-04]Least Squares Iteration: 100%|█████████▉| 999/1000 [00:06&lt;00:00, 72.86iter/s, Error=9.1534e-04]Least Squares Iteration: 100%|██████████| 1000/1000 [00:06&lt;00:00, 153.78iter/s, Error=9.1534e-04]\n\n\n\n\n\n\n\n\n\nNote that torch does have the framework to run autograd on the least squares objective itself, but for this general method we are using the adjoint to compute the gradient (and indirectly invoking autograd). This framework is the most general for when there might not be explicit analytic solutions to the least squares problem, but we have the forward operator and its adjoint."
  },
  {
    "objectID": "content/eosc555/lectures/lecture1-2/index.html",
    "href": "content/eosc555/lectures/lecture1-2/index.html",
    "title": "Lecture 1: Introduction to Inverse Theory",
    "section": "",
    "text": "Inverse theory is a set of mathematical techniques used to infer the properties of a physical system from observations of its output. It is a fundamental tool in many scientific disciplines, including geophysics, seismology, and medical imaging. Inverse theory is used to solve a wide range of problems, such as:\n\nParameter Estimation: Determining the values of unknown parameters in a model that best fit the observed data.\nSystem Identification: Identifying the structure and dynamics of a system from input-output data.\nImage Reconstruction: Reconstructing an image or object from noisy or incomplete measurements.\n\nWhat many of these tasks have in common is that we are working with incomplete information. There is a forward problem that has generated the data that we observe \\(\\vec{b}\\) from a set of input data \\(\\vec{x}\\), and we want to infer the inverse problem that generated the data. However the inverse problem is often ill-posed, meaning that there are multiple solutions that can fit the data equally well. Inverse theory provides a framework for finding the best solution to these problems.\nThe forward problem can be described for example as a differetial equation or operator \\(L\\) that takes in some measured parameters \\(u\\) with model parameters \\(x\\) :\n\\[ L(x)[u] = q \\iff u = L^{-1}(x)[q] \\]\nFor example making measurements of an electromagnetic field in correspondence to conductivity values that are underground we have:\n\\[ \\nabla \\sigma \\nabla u = q + \\text{BC}\\]\nWe measure the \\(u\\) at some points and use that to try and form an estimate of the conductivity \\(\\sigma\\). The forward problem is to solve for \\(u\\) given \\(\\sigma\\) and the inverse problem is to solve for \\(\\sigma\\) given \\(u\\). The forward problem is often well-posed and the inverse problem is often ill-posed.\nFor a computational framework we can discretize the the equation so that the operator is a matrix \\(A\\) and the data is a vector \\(\\vec{b}\\):\n\\[ \\underbrace{A}_{\\text{Forward Map}} \\underbrace{\\vec{x}}_{\\text{Model Parameters}} + \\epsilon = \\underbrace{\\vec{b}}_{\\text{Observed Data}} \\]\nIn this case we may have a sparse set of measurements \\(b\\) and a large set of \\(x\\) making the problem underdetermined. The goal of inverse theory is to find the best estimate of \\(x\\) given \\(b\\).\n\n\nTo illustrate the concept of inverse theory, consider the following example:\n\nSuppose that you have agreed to meet a friend to watch them during a triathlon race but you showed up late and missed the start. They are expecting for you to have been there at some point during the time at which they were changing from a running phase to a cycle phase. They expect you to know the time at which they made the transition. However you only know the overall start time and finish time of the race.\nIf the race starts at time \\(t=0\\) and then ends at time \\(t=b\\) how do you use this information to deduce the actual time \\(t_r \\in [0,b]\\) at which they crossed the transition zone of the race?\n\nThe first restriction on feasible solutions is the domain \\([0,b]\\) so that we know that \\(0&lt;t_r&lt;b\\).\nAfter this there are some other techniquest that we could use to better inform the probability of the occurence at different times. For example, we might have a good idea of their fitness level or average running speed from previous experience. Or in the abscence of this information there might be average times for the competitors that are available to further inform the problem and reduce the amount of error in the estimate.\n\n\n\nFor cases where the matrix \\(A\\) is not full rank, the singular value decomposition (SVD) provides a more general framework for solving the least squares problem. The SVD decomposes the matrix \\(A\\) into three matrices \\(U\\), \\(\\Sigma\\), and \\(V\\)\n\\[ A = U \\Sigma V^T \\]\nThe matrices have the following special properties:\n\nOrthogonal Subspaces: \\(U\\) and \\(V\\) are orthogonal matrices, meaning that \\(U^TU = I\\) and \\(V^TV = I\\), that is \\(U^T = U^{-1}\\) and $V^T = V^{-1}.\nOrdered Singular Values: \\(\\Sigma\\) is a diagonal matrix with non-negative values on the diagonal, known as the singular values of \\(A\\). The singular values are ordered such that \\(\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r\\). The number of non-zero singular values is equal to the rank of \\(A\\).\n\nSupposed that we have a \\(\\text{rank}(A) = r\\) matrix \\(A\\) which maps from \\(\\mathbb{R}^m\\rightarrow \\mathbb{R}^n\\). A fundamental way to view this mapping is as a composition of three linear transformations: a rotation \\(V\\), a scaling \\(\\Sigma\\), and another rotation \\(U\\). The orthogonal matrix \\(V\\) has the property that all of its rows and columns are orthogonal to each other, and the vectors themselves are normalized to \\(1\\). To see this property of the orthogonal matrix consider that \\(V^T V = I\\) and \\(V V^T = I\\):\n\\[ \\begin{align}\nZ = V^T V &= I \\\\\nz_{ij} = \\langle v_i, v_j \\rangle &= \\delta_{ij} \\end{align} \\]\nEach of the elements of the matrix \\(V^T\\) is the dot product of the \\(i\\)th and \\(j\\)th columns of \\(V\\). The dotproduct of all vectors against themselves is \\(1\\) and the dotproduct of any two different vectors is \\(0\\). So from this we can see that all of the columns of \\(V\\) are orthogonal to each other. The same property holds for \\(U\\).\n\\(V^T\\) by our definition of \\(A\\) must accept a vector from \\(\\mathbb{R}^m\\) and the matrix is square, indicating an \\(m \\times m\\) matrix. The matrix \\(U\\) must output a vector in \\(\\mathbb{R}^n\\) and the matrix is square, indicating an \\(n \\times n\\) matrix. The matrix \\(\\Sigma\\) must be \\(n \\times m\\) to map from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^n\\).\nIn all its glory:\n\\[\n\\begin{aligned}\nA_{n \\times m} &= U_{n \\times n} \\, \\Sigma_{n \\times m} \\, V^T_{m \\times m} \\\\\n&= \\left[ \\begin{array}{ccc|ccc}\n\\mathbf{u}_1 & \\cdots & \\mathbf{u}_r & \\mathbf{u}_{r+1} & \\cdots & \\mathbf{u}_n\n\\end{array} \\right]_{n \\times n}\n\\left[ \\begin{array}{ccc}\n\\sigma_1 &  &  \\\\\n& \\ddots &  \\\\\n&  & \\sigma_r \\\\\n0 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & 0\n\\end{array} \\right]_{n \\times m}\n\\left[ \\begin{array}{ccc|ccc}\n\\mathbf{v}^T_1 \\\\\n\\vdots \\\\\n\\mathbf{v}^T_r \\\\\n\\mathbf{v}^T_{r+1} \\\\\n\\vdots \\\\\n  \\mathbf{v}^T_m\n\\end{array} \\right]_{m \\times m}\n\\end{aligned}\n\\]\nIn this case the first \\(r\\) columns of \\(U\\) are the range of \\(A\\), the rest of \\(U\\) is filled with its orthogonal complement. The first \\(r\\) columns of \\(V\\) are the domain of \\(A\\), the rest of \\(V\\) is filled with its orthogonal complement. These are the four fundamental subspaces of the matrix \\(A\\), more information on this can be found at: Wikipedia: SVD\nThe matrices as shown above are for a rectangular \\(A\\) where \\(n&gt;m\\) but the same properties hold for all \\(n,m\\). Some of the singular values \\(\\sigma_i\\) may be zero, in which case the matrix \\(A\\) is not full rank.\nAnother way to decompose the SVD is to write it as a sum of outer products that are scaled by the diagonal matrix of singular values:\n\\[ A = \\sum_{i=1}^r \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T \\]\nIf \\(\\sigma_i&gt;0\\) then \\(v_i\\) is not in the null space of \\(A\\) because \\(A v_i = \\sigma_i u_i\\). If \\(\\sigma_i = 0\\) then \\(v_i\\) is in the null space of \\(A\\) because \\(A v_i = 0\\).\n\n\nBack to the task of inverting \\(Ax + \\epsilon = b\\) we can apply the SVD decomposition:\n\\[\\begin{align}\nU \\Sigma V^T x + \\epsilon &= b \\\\\n\\Sigma V^T x +&= U^T (b-\\epsilon) \\\\\nV \\Sigma^{-1} U^T (b-\\epsilon) &= x\\\\\nA^+ (b-\\epsilon) &= \\hat{x}\n\\end{align}\\]\nWhere \\(A^+ = V \\Sigma^{-1} U^T\\) is the pseudoinverse of \\(A\\). The pseudoinverse is a generalization of the matrix inverse for non-square matrices. We recover a square matrix by removing all of the absent or zero singular values from \\(\\Sigma\\) and inverting the rest, giving an \\(r \\times r\\) diagonal matrix whose inverse is simply the inverse of each element.\n\\[ \\left[ \\begin{array}{ccc}\n\\sigma_1 &  &  \\\\\n& \\ddots &  \\\\\n&  & \\sigma_r \\\\\n0 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & 0\n\\end{array} \\right]_{n \\times m}\n\\rightarrow \\left[ \\begin{array}{ccc}\n\\sigma_1^{-1} &  &  \\\\\n  & \\ddots &  \\\\\n  &  & \\sigma_r^{-1} \\\\\n  \\end{array} \\right]_{r \\times r}\\]\nThen \\[\\hat{x} = \\sum_i^N \\sigma_i^{-1} \\mathbf{u}_i^T (b-\\epsilon) \\mathbf{v}_i\\] is the solution to the least squares problem. This can be solved also as a truncated sum since \\(0&lt;N&lt;r\\). In actual practice with real world measurement we end up with many singular values that may be effectively \\(0\\) by nature of being very small relative to the noise in the data and the largest single value. We have that the solution \\(\\hat{x}\\) is a sum of \\(v_i\\) components that form an orthogonal basis \\(\\hat{x} = \\sum_i \\beta_i v_i\\) where \\(\\beta_i = \\frac{u_i^T (b-\\epsilon)}{\\sigma_i}\\). These small singular values blow up in size when inverted and so extra truncation is often necessary to avoid numerical instability and excessive amplification of noise \\(\\epsilon\\).\n\n\n\n\nLeast squares and matrix inversion is a classic starting point for understanding inverse theory. Suppose that we have input data \\(\\vec{x}\\) and output data \\(\\vec{b}\\) that are related by a linear system of equations: \\[Ax = b\\] where \\(A\\) is a matrix of coefficients. In many cases, the system is overdetermined, meaning that there are more equations than unknowns. In this case, there is no exact solution to the system, and we must find the best solution that minimizes the error between the observed data \\(\\vec{b}\\) and the predicted data \\(A\\vec{x}\\). In the simplest form of inversion that we can attempt, we can solve the least squares solution. In this case we reject all of the observed data that is from the null space of \\(A\\) assuming a zero value for each of those parameters.\n\n\nLet \\(A\\) be a \\(3 \\times 2\\) matrix and \\(\\vec{b}\\) be a \\(3 \\times 1\\) vector. The \\(\\vec{x}\\) that we are trying to solve for is a \\(2 \\times 1\\) vector. The system of equations is given by:\n\\[ A = \\begin{bmatrix}  \\vec{a}_1 & \\vec{a}_2 \\end{bmatrix} \\quad \\vec{x} = \\begin{bmatrix} x_1 \\\\ x_2  \\end{bmatrix}  \\quad \\vec{b} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix} \\]\nIn this case we have an overdetermined system with three equations, two unknowns, and three data samples. If the system of equations is full rank then we are trying to map from a 2D space to a 3D space: \\(A: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3\\). In this case there is no exact solution to the system for any \\(b\\) that is not in the column space of \\(A\\).\nInstead we can solve for the least squares solution \\(\\vec{x}_{LS}\\) by minimizing the error between the observed data \\(\\vec{b}\\) and the predicted data \\(A\\vec{x}\\) from the forward model.\n\\[ \\vec{x}_{LS} = \\arg \\min_{\\vec{x}} ||A\\vec{x} - \\vec{b}||_2^2 \\]\nWe want to find the argument that minimizes the function \\(f(\\vec{x}) = ||A\\vec{x} - \\vec{b}||_2^2\\). By first order optimality conditions, the gradient of the function must be zero at the minimum.\n\\[ \\begin{align}\n\\nabla f(\\vec{x}) &= 0 \\\\\n\\nabla ||A\\vec{x} - \\vec{b}||_2^2 &= 0 \\\\\n\\nabla (A\\vec{x} - \\vec{b})^T (A\\vec{x} - \\vec{b}) &= 0 \\\\\n\\nabla \\left( \\vec{x}^T A^T A \\vec{x} - 2 \\vec{b}^T A \\vec{x} + \\vec{b}^T \\vec{b} \\right) &= 0 \\\\\n2 A^T A \\vec{x} - 2 A^T \\vec{b} &= 0 \\\\\nA^T A \\vec{x} &= A^T \\vec{b} \\\\\n\\vec{x}_{LS} &= (A^T A)^{-1} A^T \\vec{b}\n\\end{align} \\]\nThis is known as the normal equations for the least squares solution. We take a note of caution here that \\(A^T A\\) must be invertible for this solution to exist. If \\(A\\) is not full rank then the matrix \\(A^T A\\) will not be invertible and other methods must be used.\nWe call the difference between the observed data and the predicted data the residual.\n\\(r = \\vec{b} - A\\vec{x}_{LS}\\)\nUsing this information, what we really want to minimize is the sum of the squares of the residuals: \\(||r||_2^2\\). This is the same as the sum of the squares of the errors in the data.\nThere is an altogether informative way to think about the minimization problem purely in terms of linear algebra and subspaces to derive the same normal equations.\n\n\n\nLeast Squares Visual\n\n\nWe have the range of \\(A\\) or image of \\(A\\) as the subspace of \\(\\mathbb{R}^3\\) that is spanned by the columns of \\(A\\). This subspace is rank \\(2\\) because there are only two columns in \\(A\\), \\(R(A) \\subset \\mathbb{R}^3\\). The inaccessible parts of \\(\\mathbb{R}^3\\) are in the orthogonal complement of \\(R(A)\\), \\(R(A)^\\perp\\). Recalling that \\(R(A)^\\perp = N(A^T)\\) we can diagram the solution to least squares as a minimization of the error vector \\(r\\) in the orthogonal complement of \\(R(A)\\).\nAs seen the \\(r\\) vector is perpendicular to the \\(x_{LS}\\) solution, the projection of \\(r\\) onto \\(R(A)\\) is zero. Since it is in a null space of \\(A^T\\) then \\(A^T r = 0\\).\n\\[ \\begin{align} A^T \\left ( Ax_{LS} - b \\right ) &= 0\\\\\nA^T A x_{LS} &= A^T b \\\\\n\\end {align} \\]\nSo we recover the normal equations without using any of the machinery of calculus.\nFor a review on the four fundamental subspaces of a matrix see the UBC Math 307 notes on the topic: Math 307"
  },
  {
    "objectID": "content/eosc555/lectures/lecture1-2/index.html#the-singular-value-decomposition",
    "href": "content/eosc555/lectures/lecture1-2/index.html#the-singular-value-decomposition",
    "title": "Lecture 1: Introduction to Inverse Theory",
    "section": "",
    "text": "For cases where the matrix \\(A\\) is not full rank, the singular value decomposition (SVD) provides a more general framework for solving the least squares problem. The SVD decomposes the matrix \\(A\\) into three matrices \\(U\\), \\(\\Sigma\\), and \\(V\\)\n\\[ A = U \\Sigma V^T \\]\nThe matrices have the following special properties:\n\nOrthogonal Subspaces: \\(U\\) and \\(V\\) are orthogonal matrices, meaning that \\(U^TU = I\\) and \\(V^TV = I\\), that is \\(U^T = U^{-1}\\) and $V^T = V^{-1}.\nOrdered Singular Values: \\(\\Sigma\\) is a diagonal matrix with non-negative values on the diagonal, known as the singular values of \\(A\\). The singular values are ordered such that \\(\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r\\). The number of non-zero singular values is equal to the rank of \\(A\\).\n\nSupposed that we have a \\(\\text{rank}(A) = r\\) matrix \\(A\\) which maps from \\(\\mathbb{R}^m\\rightarrow \\mathbb{R}^n\\). A fundamental way to view this mapping is as a composition of three linear transformations: a rotation \\(V\\), a scaling \\(\\Sigma\\), and another rotation \\(U\\). The orthogonal matrix \\(V\\) has the property that all of its rows and columns are orthogonal to each other, and the vectors themselves are normalized to \\(1\\). To see this property of the orthogonal matrix consider that \\(V^T V = I\\) and \\(V V^T = I\\):\n\\[ \\begin{align}\nZ = V^T V &= I \\\\\nz_{ij} = \\langle v_i, v_j \\rangle &= \\delta_{ij} \\end{align} \\]\nEach of the elements of the matrix \\(V^T\\) is the dot product of the \\(i\\)th and \\(j\\)th columns of \\(V\\). The dotproduct of all vectors against themselves is \\(1\\) and the dotproduct of any two different vectors is \\(0\\). So from this we can see that all of the columns of \\(V\\) are orthogonal to each other. The same property holds for \\(U\\).\n\\(V^T\\) by our definition of \\(A\\) must accept a vector from \\(\\mathbb{R}^m\\) and the matrix is square, indicating an \\(m \\times m\\) matrix. The matrix \\(U\\) must output a vector in \\(\\mathbb{R}^n\\) and the matrix is square, indicating an \\(n \\times n\\) matrix. The matrix \\(\\Sigma\\) must be \\(n \\times m\\) to map from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^n\\).\nIn all its glory:\n\\[\n\\begin{aligned}\nA_{n \\times m} &= U_{n \\times n} \\, \\Sigma_{n \\times m} \\, V^T_{m \\times m} \\\\\n&= \\left[ \\begin{array}{ccc|ccc}\n\\mathbf{u}_1 & \\cdots & \\mathbf{u}_r & \\mathbf{u}_{r+1} & \\cdots & \\mathbf{u}_n\n\\end{array} \\right]_{n \\times n}\n\\left[ \\begin{array}{ccc}\n\\sigma_1 &  &  \\\\\n& \\ddots &  \\\\\n&  & \\sigma_r \\\\\n0 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & 0\n\\end{array} \\right]_{n \\times m}\n\\left[ \\begin{array}{ccc|ccc}\n\\mathbf{v}^T_1 \\\\\n\\vdots \\\\\n\\mathbf{v}^T_r \\\\\n\\mathbf{v}^T_{r+1} \\\\\n\\vdots \\\\\n  \\mathbf{v}^T_m\n\\end{array} \\right]_{m \\times m}\n\\end{aligned}\n\\]\nIn this case the first \\(r\\) columns of \\(U\\) are the range of \\(A\\), the rest of \\(U\\) is filled with its orthogonal complement. The first \\(r\\) columns of \\(V\\) are the domain of \\(A\\), the rest of \\(V\\) is filled with its orthogonal complement. These are the four fundamental subspaces of the matrix \\(A\\), more information on this can be found at: Wikipedia: SVD\nThe matrices as shown above are for a rectangular \\(A\\) where \\(n&gt;m\\) but the same properties hold for all \\(n,m\\). Some of the singular values \\(\\sigma_i\\) may be zero, in which case the matrix \\(A\\) is not full rank.\nAnother way to decompose the SVD is to write it as a sum of outer products that are scaled by the diagonal matrix of singular values:\n\\[ A = \\sum_{i=1}^r \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T \\]\nIf \\(\\sigma_i&gt;0\\) then \\(v_i\\) is not in the null space of \\(A\\) because \\(A v_i = \\sigma_i u_i\\). If \\(\\sigma_i = 0\\) then \\(v_i\\) is in the null space of \\(A\\) because \\(A v_i = 0\\).\n\n\nBack to the task of inverting \\(Ax + \\epsilon = b\\) we can apply the SVD decomposition:\n\\[\\begin{align}\nU \\Sigma V^T x + \\epsilon &= b \\\\\n\\Sigma V^T x +&= U^T (b-\\epsilon) \\\\\nV \\Sigma^{-1} U^T (b-\\epsilon) &= x\\\\\nA^+ (b-\\epsilon) &= \\hat{x}\n\\end{align}\\]\nWhere \\(A^+ = V \\Sigma^{-1} U^T\\) is the pseudoinverse of \\(A\\). The pseudoinverse is a generalization of the matrix inverse for non-square matrices. We recover a square matrix by removing all of the absent or zero singular values from \\(\\Sigma\\) and inverting the rest, giving an \\(r \\times r\\) diagonal matrix whose inverse is simply the inverse of each element.\n\\[ \\left[ \\begin{array}{ccc}\n\\sigma_1 &  &  \\\\\n& \\ddots &  \\\\\n&  & \\sigma_r \\\\\n0 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & 0\n\\end{array} \\right]_{n \\times m}\n\\rightarrow \\left[ \\begin{array}{ccc}\n\\sigma_1^{-1} &  &  \\\\\n  & \\ddots &  \\\\\n  &  & \\sigma_r^{-1} \\\\\n  \\end{array} \\right]_{r \\times r}\\]\nThen \\[\\hat{x} = \\sum_i^N \\sigma_i^{-1} \\mathbf{u}_i^T (b-\\epsilon) \\mathbf{v}_i\\] is the solution to the least squares problem. This can be solved also as a truncated sum since \\(0&lt;N&lt;r\\). In actual practice with real world measurement we end up with many singular values that may be effectively \\(0\\) by nature of being very small relative to the noise in the data and the largest single value. We have that the solution \\(\\hat{x}\\) is a sum of \\(v_i\\) components that form an orthogonal basis \\(\\hat{x} = \\sum_i \\beta_i v_i\\) where \\(\\beta_i = \\frac{u_i^T (b-\\epsilon)}{\\sigma_i}\\). These small singular values blow up in size when inverted and so extra truncation is often necessary to avoid numerical instability and excessive amplification of noise \\(\\epsilon\\)."
  },
  {
    "objectID": "content/eosc555/lectures/lecture1-2/index.html#least-squares",
    "href": "content/eosc555/lectures/lecture1-2/index.html#least-squares",
    "title": "Lecture 1: Introduction to Inverse Theory",
    "section": "",
    "text": "Least squares and matrix inversion is a classic starting point for understanding inverse theory. Suppose that we have input data \\(\\vec{x}\\) and output data \\(\\vec{b}\\) that are related by a linear system of equations: \\[Ax = b\\] where \\(A\\) is a matrix of coefficients. In many cases, the system is overdetermined, meaning that there are more equations than unknowns. In this case, there is no exact solution to the system, and we must find the best solution that minimizes the error between the observed data \\(\\vec{b}\\) and the predicted data \\(A\\vec{x}\\). In the simplest form of inversion that we can attempt, we can solve the least squares solution. In this case we reject all of the observed data that is from the null space of \\(A\\) assuming a zero value for each of those parameters.\n\n\nLet \\(A\\) be a \\(3 \\times 2\\) matrix and \\(\\vec{b}\\) be a \\(3 \\times 1\\) vector. The \\(\\vec{x}\\) that we are trying to solve for is a \\(2 \\times 1\\) vector. The system of equations is given by:\n\\[ A = \\begin{bmatrix}  \\vec{a}_1 & \\vec{a}_2 \\end{bmatrix} \\quad \\vec{x} = \\begin{bmatrix} x_1 \\\\ x_2  \\end{bmatrix}  \\quad \\vec{b} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix} \\]\nIn this case we have an overdetermined system with three equations, two unknowns, and three data samples. If the system of equations is full rank then we are trying to map from a 2D space to a 3D space: \\(A: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3\\). In this case there is no exact solution to the system for any \\(b\\) that is not in the column space of \\(A\\).\nInstead we can solve for the least squares solution \\(\\vec{x}_{LS}\\) by minimizing the error between the observed data \\(\\vec{b}\\) and the predicted data \\(A\\vec{x}\\) from the forward model.\n\\[ \\vec{x}_{LS} = \\arg \\min_{\\vec{x}} ||A\\vec{x} - \\vec{b}||_2^2 \\]\nWe want to find the argument that minimizes the function \\(f(\\vec{x}) = ||A\\vec{x} - \\vec{b}||_2^2\\). By first order optimality conditions, the gradient of the function must be zero at the minimum.\n\\[ \\begin{align}\n\\nabla f(\\vec{x}) &= 0 \\\\\n\\nabla ||A\\vec{x} - \\vec{b}||_2^2 &= 0 \\\\\n\\nabla (A\\vec{x} - \\vec{b})^T (A\\vec{x} - \\vec{b}) &= 0 \\\\\n\\nabla \\left( \\vec{x}^T A^T A \\vec{x} - 2 \\vec{b}^T A \\vec{x} + \\vec{b}^T \\vec{b} \\right) &= 0 \\\\\n2 A^T A \\vec{x} - 2 A^T \\vec{b} &= 0 \\\\\nA^T A \\vec{x} &= A^T \\vec{b} \\\\\n\\vec{x}_{LS} &= (A^T A)^{-1} A^T \\vec{b}\n\\end{align} \\]\nThis is known as the normal equations for the least squares solution. We take a note of caution here that \\(A^T A\\) must be invertible for this solution to exist. If \\(A\\) is not full rank then the matrix \\(A^T A\\) will not be invertible and other methods must be used.\nWe call the difference between the observed data and the predicted data the residual.\n\\(r = \\vec{b} - A\\vec{x}_{LS}\\)\nUsing this information, what we really want to minimize is the sum of the squares of the residuals: \\(||r||_2^2\\). This is the same as the sum of the squares of the errors in the data.\nThere is an altogether informative way to think about the minimization problem purely in terms of linear algebra and subspaces to derive the same normal equations.\n\n\n\nLeast Squares Visual\n\n\nWe have the range of \\(A\\) or image of \\(A\\) as the subspace of \\(\\mathbb{R}^3\\) that is spanned by the columns of \\(A\\). This subspace is rank \\(2\\) because there are only two columns in \\(A\\), \\(R(A) \\subset \\mathbb{R}^3\\). The inaccessible parts of \\(\\mathbb{R}^3\\) are in the orthogonal complement of \\(R(A)\\), \\(R(A)^\\perp\\). Recalling that \\(R(A)^\\perp = N(A^T)\\) we can diagram the solution to least squares as a minimization of the error vector \\(r\\) in the orthogonal complement of \\(R(A)\\).\nAs seen the \\(r\\) vector is perpendicular to the \\(x_{LS}\\) solution, the projection of \\(r\\) onto \\(R(A)\\) is zero. Since it is in a null space of \\(A^T\\) then \\(A^T r = 0\\).\n\\[ \\begin{align} A^T \\left ( Ax_{LS} - b \\right ) &= 0\\\\\nA^T A x_{LS} &= A^T b \\\\\n\\end {align} \\]\nSo we recover the normal equations without using any of the machinery of calculus.\nFor a review on the four fundamental subspaces of a matrix see the UBC Math 307 notes on the topic: Math 307"
  },
  {
    "objectID": "content/CV.html",
    "href": "content/CV.html",
    "title": "CV",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: &lt;a href=\"www/cv/CV.pdf\"&gt;Download PDF&lt;/a&gt;."
  },
  {
    "objectID": "content/about/biography.html",
    "href": "content/about/biography.html",
    "title": "Bio",
    "section": "",
    "text": "I wasn’t always so academically focused. In fact, I had a ten year career in forestry where I planted over 2 million trees along with a variety of other projects. I also spent some years living in the Peruvian Amazon working as a travel guide and translator.\nI came back to study at UBC in 2020 to revisit my interest in science and technology, after an injury required me to change lifestyles. It has been a challenging but rewarding journey, and I am excited to see where it takes me next."
  },
  {
    "objectID": "content/about/biography.html#some-past-adventures",
    "href": "content/about/biography.html#some-past-adventures",
    "title": "Bio",
    "section": "Some Past Adventures",
    "text": "Some Past Adventures\n\n\n\n\nSta. Clautilde, Rio Napo, Peru\n\n\n\n\n\nTarapoto, San Martin, Peru\n\n\n\n\n\nRemote Helicopter Forestry Work\n\n\n\n\n\nEverest Base Camp\n\n\n\n\n\nDirtbiking in Myanmar\n\n\n\n\n\nSailing to Mexico from Victoria"
  },
  {
    "objectID": "blog/posts/scorematching/index.html",
    "href": "blog/posts/scorematching/index.html",
    "title": "Score Matching for Density Estimation",
    "section": "",
    "text": "The Problem of Density Estimation\nWhen working with a set of data, one of the tasks that we often want to do is to estimate the underlying probability density function (PDF) of the data. Knowing the probability distribution is a powerful tool that allows to make predictions, generate new samples, and understand the data better. For example, we may have a coin and want to know the probability of getting heads or tails. We can flip the coin many times and count the number of heads and tails to estimate the probability of each outcome. However, when it comes to higher dimensional spaces that are continuous in distribution, the problem of estimating the PDF in this way becomes intractable.\nFor example, with a set of small images such as the CIFAR-10 dataset, the images are 32x32 pixels with 3 color channels. The number of dimensions in the data space is 32x32x3 = 3072. With 8-bit images the number of all possible unique images is \\(255^{3072}\\), which is an incomprehensibly large number. The 60,000 images that are included in CIFAR-10 represent but a tiny fraction of samples in the space of all possible images.\n Figure 1: Sample images from the CIFAR-10 dataset (Krizhevsky 2009)\nTo demonstrate the issue with random image generation in such a sparsely populated space, we can generate a random 32x32 image with 3 color channels and display it.\n\n\nShow the code\nusing Plots, Images\n\n# Generate 6 random images and display them in a grid\nplots = []\nfor i in 1:6\n    # Generate a random 32x32 3-channel image\n    im = rand(3, 32, 32)\n    im = colorview(RGB, im)\n    p = plot(im, showaxis=false, showgrid=false, title=\"Random Image $i\")\n    push!(plots, p)\nend\n\n# Create a plot with a 2x3 grid layout\nplot_grid = plot(plots..., layout=(2, 3), size=(800, 400))\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYes we have successfuly generated random 32x32 color images, but they are not very useful or interesting.\nIf there were some way to learn the underlying distribution of the data, we could generate new samples that are realistic (probable) but that have never been seen before by sampling from higher probability regions of the learned distribution. So how do recent developments in machine learning manage to generate new and plausible samples from high dimensional data sets?\nOne of the techniques that has been developed is called generative modeling. Generative models are a class of machine learning models that are trained to learn the underlying distribution of the data. Once the model has learned the distribution, it can generate new samples that are similar to the training data.\nOne of the powerful techniques that allows for learning a probability distribution is score matching.\n\n\nParameter Estimation\nLet us take a moment to consider the problem of fitting a model to data in the most simple sense. Suppose that we have a set of data points and want to fit a linear model by drawing a line through it. One of the techniques that can be used is to minimize the sum of the squared errors between the data points and the line. This is known as the method of least squares.\nWe have model \\(f(x) = \\hat y = mx + b\\) with free parameters \\(\\theta = {m, b}\\) and data points \\((x_i, y_i)\\). The objective is to find the parameters \\(\\theta\\) that minimize the sum of the squared errors \\(J\\) between the predicted values \\(\\hat y_i\\) and the true values \\(y_i\\).: \\[  \\text{arg}\\min_{m,b} J(m,b) = \\text{arg}\\min_{m,b} \\sum_{i=1}^{n} (\\hat y_i - y_i)^2 \\]\nWe could use some calculus at this point to solve the minimization problem but more general matrix methods can be used to solve the problem.\n\\[\\begin{align*}\n    X &= \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix} , \\quad  \\vec{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} , \\quad \\vec{\\theta} &= \\begin{bmatrix} b \\\\ m \\end{bmatrix} \\\\\n    \\text{arg}\\min_{\\vec{\\theta}} J(\\vec{\\theta}) &= \\text{arg}\\min_{\\vec{\\theta}} ||\\vec{\\hat y} - \\vec{y}||^2\\\\\n    \\text{arg}\\min_{\\vec{\\theta}} J(\\vec{\\theta}) &= \\text{arg}\\min_{\\vec{\\theta}} ||X\\vec{\\theta} - \\vec{y}||^2\n\\end{align*}\\]\nThe solution to this problem is well known and can be found by solving the normal equations: \\[ X^T X \\vec{\\theta} = X^T \\vec{y} \\]\nAn example of this optimization problem is shown below where we generate some random data points and fit a line to them.\n\n\nShow the code\nusing Plots, Random\n\n# Generate some random data points with correlation along a line\nRandom.seed!(1234)\nn_points = 10\nx = rand(n_points)\nm_true = 0.8; b_true = -1\ny = .8* x .- 1 + 0.3 * randn(n_points)\n\n# Create the matrix X\nX = hcat(ones(n_points), x)\n\n# Solve the normal equations to find the parameters theta\ntheta = X \\ y\n\n# Generate x values for the complete line\nx_line = range(minimum(x) - 0.1, maximum(x) + 0.1, length=100)\nX_line = hcat(ones(length(x_line)), x_line)\n\n# Compute the y values for the line\ny_hat_line = X_line * theta\n\n# Compute the fitted values for the original data points\ny_hat = X * theta\n\n# Unpack learned parameters\nb, m = theta\n\n# Plot the data points and the fitted line\ntitle_text = \"Fitted Line: y = $(round(m, digits=2))x + $(round(b, digits=2)) vs. \\n True Line: y = $(m_true)x + $(b_true)\"\np1 = scatter(x, y, label=\"Data Points\", color=:red, ms=5, aspect_ratio=:equal, xlabel=\"x\", ylabel=\"y\", title=title_text)\nplot!(p1, x_line, y_hat_line, label=\"Fitted Line\", color=:blue)\n# Add dashed lines for residuals\nfor i in 1:n_points\n    plot!(p1, [x[i], x[i]], [y[i], y_hat[i]], color=:black, linestyle=:dash, label=\"\")\nend\n\ndisplay(p1)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a simple example of parameter estimation but it shows some of the important concepts that are used in more complex models. There is an underlying distribution which is a line with some error or noise added to it. We collected some random points from the distribution and then used them in an optimization problem where we minimized the squared error between the predicted values and the true values. The best solution is the parameters \\(\\theta\\) that minimize the error. In doing so we recovered a line that is close to the one that was used to generate the data.\n\n\nEstimating a Density Function\nWhen it comes to estimating denstity we are constrained by the fact that any model must sum to 1 of the entire sample space.\n\n\nDenoising Autoencoders\nDenoising Autoencoders (DAE) are a type of machine learning model that is trained to reconstruct the input data from a noisy or corrupted version of the input. The DAE is trained to take an sample such as an image with unwanted noise and restore it to the original sample.\nIn the process of learning the denoising parameters, the DAE also can learn the score function the underlying distribution of noisy samples, which is a kernel density estimate of the true distribution.\nThe score function is an operator defined as: \\[ s(f(x)) = \\nabla_x \\log f(x) \\]\nWhere \\(f(x)\\) is the density function or PDF of the distribution.\nBy learning a score function for a model, we can reverse the score operation to obtain the original density function it was derived from. This is the idea behind score matching, where we indirectly find the the pdf of a distribution by matching the score of a proposed model \\(p(x;\\theta)\\) to the score of the true distribution \\(q(x)\\).\nAnother benefit of learning the score function of a distribution is that it can be used to move from less probable regions of the distribution to more probable regions using gradient ascent. This is useful when it comes to generative models, where we want to generate new samples from the distribution that are more probable.\nHowever one of the challenges is that the score function is not always well-defined, especially in regions of low probability where there are sparse samples. This can make it difficult to learn the score function accurately in these regions.\nThis post explores some of those limitations and how increasing the bandwidth of the noise kernel in the DAE can help to stabilize the score function in regions of low probability.\n\n\nSample of Score Matching\nSuppose we have a distribution in 2D space that consists of three Gaussians as our ground truth. We can plot this pdf and its gradient field.\n\n\nShow the code\nusing Plots, Distributions\n\n# Define the ground truth distribution\nfunction p(x, y)\n    mu1, mu2, mu3 = [-1, -1], [1, 1], [1, -1]\n    sigma1, sigma2, sigma3 = [0.5 0.3; 0.3 0.5], [0.5 0.3; 0.3 0.5], [0.5 0; 0 0.5]\n\n    return 0.2 * pdf(MvNormal(mu1, sigma1), [x, y]) + 0.2 * pdf(MvNormal(mu2, sigma2), [x, y]) + 0.6 * pdf(MvNormal(mu3, sigma3), [x, y])\nend\n\n# Plot the distribution using a heatmap\nheatmap(\n    -3:0.01:3, -3:0.01:3, p,\n    c=cgrad(:davos, rev=true),\n    aspect_ratio=:equal,\n    xlabel=\"x\", ylabel=\"y\", title=\"Ground Truth PDF q(x)\",\n    xlims=(-3, 3), ylims=(-3, 3),\n    xticks=[-3, 3], yticks=[-3, 3]\n)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\nSampling from the distribution can be done by generating 100 random points\n\n\nShow the code\nusing Random, Plots, Distributions\n\n# Define the ground truth distribution\nfunction p(x, y)\n    mu1, mu2, mu3 = [-1, -1], [1, 1], [1, -1]\n    sigma1, sigma2, sigma3 = [0.5 0.3; 0.3 0.5], [0.5 0.3; 0.3 0.5], [0.5 0; 0 0.5]\n\n    return 0.2 * pdf(MvNormal(mu1, sigma1), [x, y]) + 0.2 * pdf(MvNormal(mu2, sigma2), [x, y]) + 0.6 * pdf(MvNormal(mu3, sigma3), [x, y])\nend\n\n# Sample 200 points from the ground truth distribution\nn_points = 200\npoints = []\n\n# Set random seed for reproducibility\nRandom.seed!(1234)\n\nwhile length(points) &lt; n_points\n    x = rand() * 6 - 3\n    y = rand() * 6 - 3\n    if rand() &lt; p(x, y)\n        push!(points, (x, y))\n    end\nend\n\n# Plot the distribution using a heatmap\n# heatmap(\n#     -3:0.01:3, -3:0.01:3, p,\n#     c=cgrad(:davos, rev=true),\n#     aspect_ratio=:equal,\n#     xlabel=\"x\", ylabel=\"y\", title=\"Ground Truth PDF q(θ)\",\n\n# )\n\n# Scatter plot of the sampled points\nscatter([x for (x, y) in points], [y for (x, y) in points], label=\"Sampled Points\", color=:red, ms=2,\n     xlims=(-3, 3), ylims=(-3, 3),\n     xticks=[-3, 3], yticks=[-3, 3])\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom this sampling of points we can visualize the effect of the choice of noise bandwidth on the kernel density estimate.\n\n\nShow the code\nusing Plots, Distributions, ForwardDiff\n\n# Define the ground truth distribution\nfunction p(x, y)\n    mu1, mu2, mu3 = [-1, -1], [1, 1], [1, -1]\n    sigma1, sigma2, sigma3 = [0.5 0.3; 0.3 0.5], [0.5 0.3; 0.3 0.5], [0.5 0; 0 0.5]\n\n    return 0.2 * pdf(MvNormal(mu1, sigma1), [x, y]) + 0.2 * pdf(MvNormal(mu2, sigma2), [x, y]) + 0.6 * pdf(MvNormal(mu3, sigma3), [x, y])\nend\n\n# Define the log of the distribution\nfunction log_p(x, y)\n    val = p(x, y)\n    return val &gt; 0 ? log(val) : -Inf\nend\n\n# Function to compute the gradient using ForwardDiff\nfunction gradient_log_p(u, v)\n    grad = ForwardDiff.gradient(x -&gt; log_p(x[1], x[2]), [u, v])\n    return grad[1], grad[2]\nend\n\n# Generate a grid of points\nxs = -3:0.5:3\nys = -3:0.5:3\n\n# Create meshgrid manually\nxxs = [x for x in xs, y in ys]\nyys = [y for x in xs, y in ys]\n\n# Compute the gradients at each point\nU = []\nV = []\nfor x in xs\n    for y in ys\n        u, v = gradient_log_p(x, y)\n\n        push!(U, u)\n        push!(V, v)\n    end\nend\n\n# Convert U and V to arrays\nU = reshape(U, length(xs), length(ys))\nV = reshape(V, length(xs), length(ys))\n\n# Plot the distribution using a heatmap\nheatmap(\n    -3:0.01:3, -3:0.01:3, p,\n    c=cgrad(:davos, rev=true),\n    aspect_ratio=:equal,\n    xlabel=\"x\", ylabel=\"y\", title=\"Ground Truth PDF q(x) with score\",\n    xlims=(-3, 3), ylims=(-3, 3),\n    xticks=[-3, 3], yticks=[-3, 3]\n)\n\n# Flatten the gradients and positions for quiver plot\nxxs_flat = [x for x in xs for y in ys]\nyys_flat = [y for x in xs for y in ys]\n\n# Plot the vector field\nquiver!(xxs_flat, yys_flat, quiver=(vec(U)/20, vec(V)/20), color=:green, quiverkeyscale=0.5)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\nNow we apply a Gaussian kernel to the sample points to create the kernel density estimate:\n\n\nShow the code\nusing Plots, Distributions, KernelDensity\n\n# Convert points to x and y vectors\nx_points = [x for (x, y) in points]\ny_points = [y for (x, y) in points]\n\n# Perform kernel density estimation using KernelDensity.jl\nparzen = kde((y_points, x_points); boundary=((-3,3),(-3,3)), bandwidth = (.3,.3))\n\n# Plot the ground truth PDF\np1 = heatmap(\n    -3:0.01:3, -3:0.01:3, p,\n    c=cgrad(:davos, rev=true),\n    aspect_ratio=:equal,\n    xlabel=\"x\", ylabel=\"y\", title=\"Ground Truth PDF q(x)\",\n    xlims=(-3, 3), ylims=(-3, 3),\n    xticks=[-3, 3], yticks=[-3, 3]\n)\n\n# Scatter plot of the sampled points on top of the ground truth PDF\nscatter!(p1, x_points, y_points, label=\"Sampled Points\", color=:red, ms=2)\n\n\n# Plot the kernel density estimate\np2 = heatmap(\n    parzen.x, parzen.y, parzen.density,\n    c=cgrad(:davos, rev=true),\n    aspect_ratio=:equal,\n    xlabel=\"x\", ylabel=\"y\", title=\"Kernel Density Estimate\",\n    xlims=(-3, 3), ylims=(-3, 3),\n    xticks=[-3, 3], yticks=[-3, 3]\n)\n\n# Scatter plot of the sampled points on top of the kernel density estimate\nscatter!(p2, x_points,  y_points, label=\"Sampled Points\", color=:red, ms=2)\n\n# Arrange the plots side by side\nplot(p1, p2, layout = @layout([a b]), size=(800, 400))\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\nNow looking at the density estimate across many bandwidths, we can see the effect on adding more and more noise to the original sampled points and our density estimate that we are learning. At very large bandwidths the estimate becomes a uniform distribution.\n\n\nShow the code\nusing Plots, Distributions, KernelDensity\n# Define the range of bandwidths for the animation\nbandwidths = [(0.01 + 0.05 * i, 0.01 + 0.05 * i) for i in 0:40]\n\n# Create the animation\nanim = @animate for bw in bandwidths\n    kde_result = kde((x_points,y_points); boundary=((-6, 6), (-6, 6)), bandwidth=bw)\n\n    p2 = heatmap(\n        kde_result.x, kde_result.y, kde_result.density',\n        c=cgrad(:davos, rev=true),\n        aspect_ratio=:equal,\n        xlabel=\"x\", ylabel=\"y\", title=\"Kernel Density Estimate,Bandwidth = $(round(bw[1],digits=2))\",\n        xlims=(-6, 6), ylims=(-6, 6),\n        xticks=[-6, 6], yticks=[-6, 6]\n    )\n\n    scatter!(p2, x_points, y_points, label=\"Sampled Points\", color=:red, ms=2)\nend\n\n# Save the animation as a GIF\ngif(anim, \"parzen_density_animation_with_gradients.gif\", fps=2,show_msg = false)\n\n\n\n\n\nNow we can compute the score of the kernel density estimate to see how it changes with the bandwidth. The score function of the distribution is numerically unstable at regions of sparse data. Recalling that the score is the gradient of the log-density funtion, when the density is very low the function approaches negative infinity. Within the limits of numerical precision, taking the log of the density function will result in a negative infinity in sparse and low probability regions. Higher bandwidths of KDE using the Gaussian kernel for example, spread out both the discrete sampling and the true distribution over space. This extends the region of numerical stability for a higher bandwidth.\nThe regions with poor numerical stability can be seen as noise artifacts and missing data in the partial derivatives of the log-density function. Some of these artifacts may also propogate from the fourier transform calculations that the kernel density estimate uses.\n\n\nShow the code\nusing Plots, Distributions, KernelDensity, ForwardDiff\n\n# Define the range of bandwidths for the animation\nbandwidths = [(0.01 + 0.05 * i, 0.01 + 0.05 * i) for i in 0:30]\n\nboundary = (-10, 10)\n# Create the animation\nanim = @animate for bw in bandwidths\n    kde_result = kde((x_points, y_points); boundary=(boundary, boundary), bandwidth=bw)\n\n        # Compute log-density\n    log_density = log.(kde_result.density)\n\n    # Compute gradients of log-density\n    grad_x = zeros(size(log_density))\n    grad_y = zeros(size(log_density))\n\n    # Compute gradients using finite difference centered difference\n    for i in 2:size(log_density, 1)-1\n        for j in 2:size(log_density, 2)-1\n            grad_x[i, j] = (log_density[i+1, j] - log_density[i-1, j]) / (kde_result.x[i+1] - kde_result.x[i-1])\n            grad_y[i, j] = (log_density[i, j+1] - log_density[i, j-1]) / (kde_result.y[j+1] - kde_result.y[j-1])\n        end\n    end\n    # Downsample the gradients and coordinates by selecting every 10th point\n    downsample_indices_x = 1:10:size(grad_x, 1)\n    downsample_indices_y = 1:10:size(grad_y, 2)\n\n    grad_x_downsampled = grad_x[downsample_indices_x, downsample_indices_y]\n    grad_y_downsampled = grad_y[downsample_indices_x, downsample_indices_y]\n\n    x_downsampled = kde_result.x[downsample_indices_x]\n    y_downsampled = kde_result.y[downsample_indices_y]\n\n    xxs_flat = repeat(x_downsampled, inner=[length(y_downsampled)])\n    yys_flat = repeat(y_downsampled, outer=[length(x_downsampled)])\n\n    grad_x_flat = grad_x_downsampled[:]\n    grad_y_flat = grad_y_downsampled[:]\n\n    # Plot heatmaps of the gradients\n    p1 = heatmap(\n        kde_result.x, kde_result.y, grad_x',\n        c=cgrad(:davos, rev=true),\n        aspect_ratio=:equal,\n        xlabel=\"x\", ylabel=\"y\", title=\"Partial Derivative of Log-Density wrt x \\n Bandwidth = $(round(bw[1],digits=2))\",\n        xlims=boundary, ylims=boundary\n    )\n\n    # Overlay the scatter plot of the sampled points\n    scatter!(p1, x_points, y_points, label=\"Sampled Points\", color=:red, ms=2)\n\n    p2 = heatmap(\n        kde_result.x, kde_result.y, grad_y',\n        c=cgrad(:davos, rev=true),\n        aspect_ratio=:equal,\n        xlabel=\"x\", ylabel=\"y\", title=\"Partial Derivative of Log-Density wrt y \\n Bandwidth = $(round(bw[1],digits=2))\",\n        xlims=boundary, ylims=boundary\n    )\n\n    # Overlay the scatter plot of the sampled points\n    scatter!(p2, x_points, y_points, label=\"Sampled Points\", color=:red, ms=2)\n\n    plot(p1, p2, layout = @layout([a b]), size=(800, 400))\nend\n# Save the animation as a GIF\ngif(anim, \"parzen_density_partials.gif\", fps=2, show_msg=false)\n\n\n\n\n\nAnd combining the gradient overtop of the ground truth distribution that is modeled with the kernel density estimate, starting with the larger bandwidths and moving to the smaller bandwidths, we can see that the region of numerical stability is extended with the larger bandwidths. The larger bandwidths also remove some of the precision in the model, with larger bandwidths the model approaches a single gaussian distribution.\n\n\nShow the code\n# Define the range of bandwidths for the animation\nbandwidths = [(0.01 + 0.2 * i, 0.01 + 0.2 * i) for i in 0:10]\nbandwidths = reverse(bandwidths)\n\nboundary = (-10, 10)\n# Create the animation\nanim = @animate for bw in bandwidths\n    kde_result = kde((x_points, y_points); boundary=(boundary, boundary), bandwidth=bw)\n\n    # Compute log-density\n    log_density = log.(kde_result.density)\n\n    # Compute gradients of log-density\n    grad_x = zeros(size(log_density))\n    grad_y = zeros(size(log_density))\n\n    # Compute gradients using finite difference centered difference\n    for i in 2:size(log_density, 1)-1\n        for j in 2:size(log_density, 2)-1\n            grad_x[i, j] = (log_density[i+1, j] - log_density[i-1, j]) / (kde_result.x[i+1] - kde_result.x[i-1])\n            grad_y[i, j] = (log_density[i, j+1] - log_density[i, j-1]) / (kde_result.y[j+1] - kde_result.y[j-1])\n        end\n    end\n    # Downsample the gradients and coordinates by selecting every 10th point\n    downsample_indices_x = 1:20:size(grad_x, 1)\n    downsample_indices_y = 1:20:size(grad_y, 2)\n\n    grad_x_downsampled = grad_x[downsample_indices_x, downsample_indices_y]\n    grad_y_downsampled = grad_y[downsample_indices_x, downsample_indices_y]\n\n    x_downsampled = kde_result.x[downsample_indices_x]\n    y_downsampled = kde_result.y[downsample_indices_y]\n\n    xxs_flat = repeat(x_downsampled, inner=[length(y_downsampled)])\n    yys_flat = repeat(y_downsampled, outer=[length(x_downsampled)])\n\n    grad_x_flat = grad_x_downsampled[:]\n    grad_y_flat = grad_y_downsampled[:]\n\n     # Plot the actual distribution\n    x_range = boundary[1]:0.01:boundary[2]\n    y_range = boundary[1]:0.01:boundary[2]\n    p1 = heatmap(\n        x_range, y_range, p,\n        c=cgrad(:davos, rev=true),\n        aspect_ratio=:equal,\n        xlabel=\"x\", ylabel=\"y\", title=\"Ground Truth PDF q(x)\\n with score of Kernel Density Estimate, \\n Bandwidth = $(round(bw[1],digits=2))\",\n        xlims=boundary, ylims=boundary,\n        size=(800, 800)\n    )\n\n    # Plot a quiver plot of the downsampled gradients\n    quiver!(yys_flat, xxs_flat, quiver=(grad_x_flat/10, grad_y_flat/10), \n    color=:green, quiverkeyscale=0.5, aspect_ratio=:equal)\nend\n# Save the animation as a GIF\ngif(anim, \"parzen_density_gradient_animation_with_gradients.gif\", fps=2, show_msg=false)\n\n\n\n\n\n\n\n\n\n\nReferences\n\nKrizhevsky, Alex. 2009. “Learning Multiple Layers of Features from Tiny Images.” https://www.cs.toronto.edu/~kriz/cifar.html."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Posts",
    "section": "",
    "text": "Score Matching for Density Estimation\n\n\nEstimation of the probability density function using score matching\n\n\nScore matching is a method for indirectly estimating the probability density function of a distribution. In this post, I will explain the score matching method as well as some of its limitations.\n\n\n\n\n\nJun 22, 2024\n\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome\n\n\nA first post using Quarto\n\n\nFind out more about the tools I’m using to create this blog.\n\n\n\n\n\nApr 22, 2024\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/welcome/index.html",
    "href": "blog/posts/welcome/index.html",
    "title": "Welcome",
    "section": "",
    "text": "This year, I have been expanding my knowledge of publishing and coding techniques that are designed to make sharing technical work easier, more visual, and more interactive. Michael Friedlander, who teaches CPSC 406 Computational Optimization at UBC, is an advocate for using Julia and Quarto in teaching and research. Drawing inspiration from his work along with Patrick Altmeyer’s website, I have decided to start my own blog using Quarto.\nQuarto is a scientific and technical publishing system built on Pandoc. It is designed to make it easy to write and publish technical content, such as research papers, books, and reports. One of its main features is that it allows for writing content in markdown along with code chunks in Julia, Python, R, and other languages. In addition, Quarto supports a wide range of output formats, including HTML, PDF, and Word. It has the great convenience of being able to port writing from Obsidian or in Latex to a blog post or html with minimal effort.\nI’m excited to use this new tool to share my work and ideas, especially as I continue to learn more about data science, machine learning, and optimization. I hope you find the content here useful and/or interesting."
  },
  {
    "objectID": "blog/posts/welcome/index.html#examples-of-julia-code-and-plots",
    "href": "blog/posts/welcome/index.html#examples-of-julia-code-and-plots",
    "title": "Welcome",
    "section": "Examples of Julia Code and Plots",
    "text": "Examples of Julia Code and Plots\nHere’s a parametrically defined, snail-like surface. Although it exists in 3D space, the surface is two-dimensional in that any location on it can be specified using just two coordinates—similar to how we navigate the surface of the Earth. You van see this incorporated as the two parameters \\(u\\) and \\(v\\) in the code below. These two coordinates map into 3D space that is defined by the functions \\(s1\\), \\(s2\\), and \\(s3\\) giving a vector \\[\\mathbf{s}(u,v) = \\begin{bmatrix}s1(u,v) \\\\ s2(u,v) \\\\ s3(u,v)\\end{bmatrix}\\]\nThe surface is then plotted using the surface function from the Julia Plots package.\nNote the usage of the vectorized operation of the functions \\(s1\\), \\(s2\\), and \\(s3\\) to create the vectors xs, ys, and zs. The passing of the input vectors u and v' creates the required meshgrid for the surface plot.\n\n\nShow the code\nusing Plots\n\n# Your plotting code here\nu = range(0, stop=6π, length=100)\nv = range(0, stop=2π, length=30)\ns1(u, v) = 2 * (1 - exp(u / (6 * π))) * cos(u) * cos(v / 2)^2\ns2(u, v) = 2 * (-1 + exp(u / (6 * π))) * sin(u) * cos(v / 2)^2\ns3(u, v) = 1 - 0.71 * exp(u / (3 * π)) - sin(v) + exp(u / (6 * π)) * sin(v)\n\nxs, ys, zs = s1.(u, v'), s2.(u, v'), s3.(u, v')\nsurface(xs, ys, zs, color=cgrad(:acton), alpha=0.5, legend=false)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Surface Plot Example\n\n\n\n\nThis code is an example of the animation features included in the Julia Plot library found at Julia Plots Package that can be used to create a gif. The gif below shows a parametric plot of a heart. Note just how compact the code is for creating this gif and the natural expression that the code has. This is the power of Julia.\n\n\nShow the code\nusing Plots\n\n@userplot CirclePlot\n@recipe function f(cp::CirclePlot)\n    x, y, i = cp.args\n    n = length(x)\n    inds = circshift(1:n, 1 - i)\n    linewidth --&gt; range(0, 10, length = n)\n    seriesalpha --&gt; range(0, 1, length = n)\n    aspect_ratio --&gt; 1\n    label --&gt; false\n    x[inds], y[inds]\nend\n\nn = 400\nt = range(0, 2π, length = n)\nx = 16sin.(t).^3\ny = 13cos.(t) .- 5cos.(2t) .- 2cos.(3t) .- cos.(4t)\n\nanim = @animate for i ∈ 1:n\n    circleplot(x, y, i, line_z = 1:n, cbar = false, c = :reds, framestyle = :none)\nend every 5\ngif(anim, \"anim_fps15.gif\", fps = 15, show_msg = false)\n\n\n\n\n\n\n\nFigure 2: Heart Animation Example"
  },
  {
    "objectID": "content/about/overview.html",
    "href": "content/about/overview.html",
    "title": "Simon Ghyselincks",
    "section": "",
    "text": "Welcome to my personal site! I’m Simon Ghyselincks, currently a 5th-year Engineering Physics student at the University of British Columbia (UBC), with a minor in Computer Science. I am studying a cross-disciplinary blend of engineering, computer science, and applied mathematics. What I really love is coding to solve tough problems in robotics, machine learning, signal processing, and more."
  },
  {
    "objectID": "content/about/overview.html#welcome",
    "href": "content/about/overview.html#welcome",
    "title": "Simon Ghyselincks",
    "section": "",
    "text": "Welcome to my personal site! I’m Simon Ghyselincks, currently a 5th-year Engineering Physics student at the University of British Columbia (UBC), with a minor in Computer Science. I am studying a cross-disciplinary blend of engineering, computer science, and applied mathematics. What I really love is coding to solve tough problems in robotics, machine learning, signal processing, and more."
  },
  {
    "objectID": "content/about/overview.html#academics-and-projects",
    "href": "content/about/overview.html#academics-and-projects",
    "title": "Simon Ghyselincks",
    "section": "Academics and Projects",
    "text": "Academics and Projects\nI am currently working with Eldad Haber at UBC Earth and Ocean Sciences on generative AI for geophysical applications. Our work explores the application of recent advances in normalizing flows with stochastic interpolants to generate 3d models of the earth’s crust. I am also continuing to develop our Engineering Physics capstone project “Learning to Balance” which explores the application of reinforcement learning to a reaction wheel robot with complex dynamics. Read more about my projects here.\n\nFeel free to connect with me on LinkedIn or check out my GitHub."
  },
  {
    "objectID": "content/about/overview.html#my-journey",
    "href": "content/about/overview.html#my-journey",
    "title": "Simon Ghyselincks",
    "section": "My Journey",
    "text": "My Journey\nRead more about my journey and past pursuits here."
  },
  {
    "objectID": "content/eosc555/index.html",
    "href": "content/eosc555/index.html",
    "title": "EOSC 555B: Nonlinear Inverse Theory",
    "section": "",
    "text": "Lecture notes for EOSC 555B: Nonlinear Inverse Theory taken at the University of British Columbia. Portions of template code originate from the course instrcutor Prof. Eldad Haber, while I’ve added some of my own numerical experiments and examples explored as part of self-study on the topics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 5: Autodiff and Gauss Newton Optimization\n\n\nA look at some of the foundations of automatic differentiation and the Gauss-Newton optimization method.\n\n\nAutomatic differentiation is a powerful tool for solving optimization problems and for recovering the jacobian. It can be used to automate the process of Gauss-Newton optimization.\n\n\n\n\n\nSep 25, 2024\n\n\n18 min\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 4: Regularization and the Conjugate Gradient Methods\n\n\nA derivation of regularization techniques for least squares\n\n\nTikhonov regularization is a common technique used in inverse theory to stabilize ill-posed problems. In this lecture, we derive the Tikhonov regularization technique, we also have a look at a least squares solution that does not require the computation of the full SVD of the matrix \\(A\\), using the conjugate gradient method.\n\n\n\n\n\nSep 20, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 3: Image Denoising with Gradient Descent and Early Stopping\n\n\nA derivation of least squares gradient descent and ODE analysis\n\n\nIn continuation of Lecture 2, we now look at an alternative approach to image denoising using gradient descent and early stopping. We will derive the least squares gradient descent algorithm and analyze it as an ordinary differential equation.\n\n\n\n\n\nSep 17, 2024\n\n\n22 min\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 2: Image Denoising with SVD\n\n\nApplications of Least Squares and SVD\n\n\nImage denoising and deblurring are important techniques in signal processing and recovery. I this coding exercise, we will explore the application of least squares, SVD, and the pseudoinverse to denoise and deblur images.\n\n\n\n\n\nSep 15, 2024\n\n\n22 min\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 1: Introduction to Inverse Theory\n\n\nLeast Squares and the SVD\n\n\nInverse theory has broad applications across many scientific disciplines. This lecture introduces the concept of least squares and the singular value decomposition (SVD) as a foundation for understanding inverse theory. We then use these properties to analyse the stability and conditioning of linear systems for solving inverse problems using the pseudoinverse and ML techniques.\n\n\n\n\n\nSep 14, 2024\n\n\n11 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/eosc555/lectures/lecture2/index.html",
    "href": "content/eosc555/lectures/lecture2/index.html",
    "title": "Lecture 2: Image Denoising with SVD",
    "section": "",
    "text": "The motivation for the exercise comes from a real world problem. The Hubble space telescope when launched had a defect in its mirror. This defect caused the images to be blurred. The problem was initially addressed by using signal processing techniques to remove the aberrations from the images.\n\n\nFor such an image processing problem, we can consider the continuous incoming light as striking a 2D mirror that distorts the light, followed by a 2D sensor that captures the light. In this context we suppose that we have a noise kernel or a point spread function (PSF) that describes the distortion of the light at the mirror. The point spread function, being a convolution kernel, behaves as a Green’s function for the system in the continuous case:\n\\[ \\vec{b}(x,y) = \\int_{\\mathcal{X}} \\int_{\\mathcal{Y}} \\vec{G}(x - x', y - y') \\vec{u}(x',y') \\, dx' dy' \\]\nwhere \\(\\vec{b}(x,y)\\) is the blurred image data that is recovered at the sensor, \\(\\vec{u}(x',y')\\) is the true image data, and \\(\\vec{G}(x,y)\\) is the point spread function.\nIn the special case that the point spread function is \\(\\delta(x-x',y-y')\\), then the image data is not distorted and the sensor captures the true image data. However our experiment is to consider cases where there could be even severe distortions and see how this impacts the proposition of recovering the true image data, \\(\\vec{u}(x',y')\\) from our sensor data, \\(\\vec{b}(x,y)\\).\n\n\nThe discrete analog of the continuous PSF can be more conveniently treated with we essentially flatten the the 2D mesh into a 1D vector, a common operation for signal processing. The unflattened case we have:\n\\[ b_{ij} = \\sum_{k=1}^{n} \\sum_{l=1}^{m} \\Delta x \\Delta y G(x_i - x_k, y_j - y_l) u_{kl} \\]\nwhere \\(b\\) is the blurred image data at the sensor, \\(u\\) is the true image data, and \\(G\\) is the discrete point spread function. If we flatten the 2D mesh into a 1D vector we can represent this as a 1D convolution operation: \\[ \\vec{b} = \\vec{G} * \\vec{u} \\]\nSince this is a convolution operation, we can process it much more quickly by leveraging the convolution theorem.\n\\[\\begin{align}\n\\mathcal{F}(\\vec{b}) &= \\mathcal{F}(\\vec{G} * \\vec{u}) \\\\\n\\mathcal{F}(\\vec{b}) &= \\mathcal{F}(\\vec{G}) \\mathcal{F}(\\vec{u}) \\\\\n\\vec{b} &= \\mathcal{F}^{-1}(\\mathcal{F}(\\vec{G}) \\odot \\mathcal{F}(\\vec{u}))\n\\end{align}\n\\]\nThe \\(\\odot\\) hadamard product is element-wise multiplication, the discrete analog of multiplication of two functions except over an array.\n\n\n\n\nIf we flatten the data down into a 1D vector then it is possible to construct a matrix operator that performs the convolution. This is a Toeplitz matrix, a matrix where each descending diagonal from left to right is constant, so that the row vectors represent a sliding window of the convolution kernel. We can flatten out the PSF and construct the matrix using it as the first row entry and then shifting the PSF to the right to fill out the rest of the rows."
  },
  {
    "objectID": "content/eosc555/lectures/lecture2/index.html#least-squares-recovery-with-svd-and-pseudoinverse",
    "href": "content/eosc555/lectures/lecture2/index.html#least-squares-recovery-with-svd-and-pseudoinverse",
    "title": "Lecture 2: Image Denoising with SVD",
    "section": "Least Squares Recovery with SVD and Pseudoinverse",
    "text": "Least Squares Recovery with SVD and Pseudoinverse\nNow that we have a matrix operator recovered we can formulate the forward problem as \\(A\\mathbf{x} = \\mathbf{b}\\) with our known \\(A\\) and \\(\\mathbf{b}\\), and we want to recover \\(\\mathbf{x}\\). To do this we use the SVD decomposition to gather the pseudo inverse. We can decide to filter out some of the singular values that are very small to improve the conditioning on the matrix as well, using a cutoff value for example.\n\nSVD Decomposition\n\n\nShow the code\nU, S, V = torch.svd(Amat.to(torch.float64))\nb = Amv(x)\n\n\nNow we make a log plot of the singular values to see how they decay, noting that we lose numerical precision around the \\(10^{-6}\\) mark. We can also asses what the frobenius norm of the difference between the original matrix and the reconstructed matrix is to get a sense of the error in the decomposition and reconstruction.\n\n\nShow the code\nplt.semilogy(S)\nplt.xlabel('Singular Value Index')\nplt.ylabel('Singular Value')\n\nloss = F.mse_loss(Amat, U @ torch.diag(S) @ V.T)\nprint(f\"The loss is {loss}\")\n\n\nThe loss is 1.812403923995022e-34\n\n\n\n\n\nSVD Decomposition of the Convolution Matrix.\n\n\n\n\nThe loss is quite small which is a good sign that the decomposition is working well within the numerical precision of the machine.\n\n\nInitial Attempt at Pseudoinverse\nTo recover the original image data we first naively try to invert the matrix to see what happens.\n\n\nShow the code\nxhat = torch.linalg.solve(Amat,b.reshape(dim**2))\nplt.subplot(1,2,1)\nplt.imshow(xhat.reshape(x.shape[-2:]))\nplt.title('Naive Inverse')\nplt.subplot(1,2,2)\nplt.imshow(x.reshape(x.shape[-2:]))\nplt.title('Original Image');\n\n\n\n\n\nNaive Pseudoinverse Recovery of the Original Image.\n\n\n\n\nWow, not even close! This is because the matrix is so ill conditioned that it is effectively low rank and not invertible. We can improve the situation by filtering out the singular values that are very small.\n\n\nPseudoinverse with Filtering\nWe can filter out the poor conditioning singular values and exclude those values from the inversion. To get an idea of what the values are doing, we can plot the first few singular values and the corresponding singular vector that they project onto. In the case of the SVD the most important information about the matrix is captured in the left-most vectors of the matrix \\(U\\).\n\n\nShow the code\nn= 5\nfor i in range(n):\n  plt.subplot(1,n,i+1)\n  plt.imshow(U[:,i+1].reshape(x.shape[-2:]))\n  plt.title(f'Mode {i}')\n\n\n\n\n\n\n\n\n\nFor the inverse problem, the most import singular values are conversely found in the left-most vectors of the matrix \\(V\\). We can also check what the right-most vectors are doing, as they will blow up in value when inverting small singular values. They are high frequency modes of the image, creating the reconstruction issues when they are subjected to error in numerical precision.\n\n\nShow the code\nn= 5\nfor i in range(n):\n  plt.subplot(1,n,i+1)\n  plt.imshow(V[:,i+1].reshape(x.shape[-2:]))\n  plt.title(f'Mode {i}')\nplt.show()\n\nfor i in range(n):\n  plt.subplot(1,n,i+1)\n  plt.imshow(V[:,-(i+1)].reshape(x.shape[-2:]))\n  plt.title(f'Mode {V.shape[1]-i}')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese modes are the most important ones, as they contain the big-picture detail without the high frequency noise. We can now filter out the singular values that are very small and invert the matrix to recover the original image.\n\n\nShow the code\nb_flat = b.flatten().to(torch.float64)\nx_flat = x.flatten().to(torch.float64)\nthresholds = [1e-1, 1e-3, 1e-6, 1e-7, 1e-8, 1e-10]\n\nplt.figure(figsize=(7,5))  # Adjust the figure size as needed\n\nfor idx, threshold in enumerate(thresholds):\n    # Filter the singular values\n    S_filtered = S.clone()\n    S_filtered[S_filtered &lt; threshold] = 0\n\n    # Compute the reciprocal of the filtered singular values\n    S_inv = torch.zeros_like(S_filtered)\n    non_zero_mask = S_filtered &gt; 0\n    S_inv[non_zero_mask] = 1 / S_filtered[non_zero_mask]\n\n    # Construct the pseudoinverse of Amat\n    A_pinv = V @ torch.diag(S_inv) @ U.T\n\n    # Reconstruct the original image\n    xhat = A_pinv @ b_flat\n\n    # Compute the reconstruction error\n    error = torch.norm(xhat - x_flat, p='fro').item()\n\n    # Plot the reconstructed image in the appropriate subplot\n    plt.subplot(2, 3, idx + 1)  # idx + 1 because subplot indices start at 1\n    plt.imshow(xhat.reshape(x.shape[-2:]))\n    plt.title(f'Threshold {threshold}\\nError: {error:.4f}')\n    plt.colorbar()\n    plt.axis('off')  # Optionally turn off axis ticks and labels\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nPseudoinverse Recovery of the Original Image with Filtering.\n\n\n\n\nLooking at the results, around the \\(10^{-7}\\) mark we start to a peak level of recovery, as measured by the error in the Frobenius norm of the reconstruction. But what happens when we add noise to the data signal?\n\n\nAdding Noise to the Signal\nNow we add some noise to the signal and try least squares again for the direct solution\n\n\nShow the code\nb_flat = b.flatten().to(torch.float64)\nx_flat = x.flatten().to(torch.float64)\nAmat = Amat.to(torch.float64)\n\nalpha = .01\nnoise = torch.randn_like(b_flat) * alpha\n\nH = Amat.T @ Amat + alpha**2 * torch.eye(Amat.shape[0])\nxhat = torch.linalg.solve(H, Amat.T @ (b_flat + noise))\n\nplt.subplot(1,2,1)\nplt.imshow(x[0,0])\nplt.title('Original Image')\nplt.subplot(1,2,2)\nplt.imshow(xhat.reshape(x.shape[-2:]))\nplt.title('Reconstructed Image');\n\n\n\n\n\nPseudoinverse Recovery of the Original Image with Noise.\n\n\n\n\nThe reconstruction is not very good, the noise has been amplifed all over the image. We can try the pseudoinverse method again with the noise added to the signal.\n\n\nShow the code\nAmat_noisy = Amat + alpha * torch.eye(Amat.shape[0])\nUn, Sn, Vn = torch.svd(Amat_noisy)\n\nthresholds = [.5, .1, .05, .03, .005, .001]\n\nplt.figure(figsize=(7,5))  # Adjust the figure size as needed\n\nfor idx, threshold in enumerate(thresholds):\n    # Filter the singular values\n    S_filtered = Sn.clone()\n    S_filtered[S_filtered &lt; threshold] = 0\n\n    # Compute the reciprocal of the filtered singular values\n    S_inv = torch.zeros_like(S_filtered)\n    non_zero_mask = S_filtered &gt; 0\n    S_inv[non_zero_mask] = 1 / S_filtered[non_zero_mask]\n\n    # Construct the pseudoinverse of Amat\n    A_pinv = Vn @ torch.diag(S_inv) @ Un.T\n\n    # Reconstruct the original image\n    xhat = A_pinv @ (b_flat + noise)\n\n    # Compute the reconstruction error\n    error = torch.norm(xhat - x_flat, p='fro').item()\n\n    # Plot the reconstructed image in the appropriate subplot\n    plt.subplot(2, 3, idx + 1)  # idx + 1 because subplot indices start at 1\n    plt.imshow(xhat.reshape(x.shape[-2:]))\n    plt.title(f'Threshold {threshold}\\nError: {error:.4f}')\n    plt.colorbar()\n    plt.axis('off')  # Optionally turn off axis ticks and labels\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nPseudoinverse Recovery of the Original Image with Noise.\n\n\n\n\nThe small addition of noise is quite significant in the recovery threshold for reconstruction. Using a higher threshold for the singular values becomes important when dealing with noise in the signal. Previously numerical precision was the main issue, but now the measurement noise is the main issue."
  },
  {
    "objectID": "content/eosc555/lectures/lecture4/index.html",
    "href": "content/eosc555/lectures/lecture4/index.html",
    "title": "Lecture 4: Regularization and the Conjugate Gradient Methods",
    "section": "",
    "text": "$$ \n$$"
  },
  {
    "objectID": "content/eosc555/lectures/lecture4/index.html#tikhnov-regularization",
    "href": "content/eosc555/lectures/lecture4/index.html#tikhnov-regularization",
    "title": "Lecture 4: Regularization and the Conjugate Gradient Methods",
    "section": "Tikhnov Regularization",
    "text": "Tikhnov Regularization\nWe have looked at the least squares formulation for solving inverse problems:\n\\[ \\min \\frac{1}{2} \\|A x - b\\|^2 \\]\nwhere \\(A \\in \\mathbb R^{m \\times n}\\) is a linear operator, \\(x \\in \\mathbb R^n\\) is the unknown model, and \\(b \\in \\mathbb R^m\\) is the data.\nThe least squares problem is often ill-posed, meaning that the solution is not unique or stable. If there are more unknowns than equations, such as the case when \\(n &gt; m\\), then the problem is underdetermined and there are infinitely many solutions.\nWe can return to unique solutions by adding a regularization term to the selection of the \\(x\\) that we want to minimize. The Tikhonov regularization technique adds a penalty term to the least squares problem:\n\\[ \\min \\frac{1}{2} \\|A x - b\\|^2 + \\frac{1}{2}  \\lambda \\|Lx\\|^2 \\]\nwhere \\(L \\in \\mathbb R^{n \\times n}\\) is a regularization matrix. The regularization matrix \\(L\\) is often chosen to be the identity matrix, but other choices are possible.\n\nUniqueness\nTo check the uniqueness of the solution, we can rewrite the problem as a quadratic form:\n\\[ \\min \\frac{1}{2} x^T A^T A x - b^T A x + \\frac{1}{2} \\lambda x^T L^T L x \\] \\[ = \\min \\frac{1}{2} x^T H x - b^T A x + \\frac{1}{2}\\|b\\|^2\\]\nwhere \\(H = A^T A + \\lambda L^T L\\) is the Hessian matrix which is symmetric and positive semi-definite by spectral theorem. If we choose an appropriate \\(\\lambda\\), then the Hessian matrix is positive definite and the problem is well-posed. In the case where \\(L=I\\), the Hessian becomes full rank for \\(\\lambda &gt; 0\\) and the problem is well-posed. The quality that \\(H \\succ 0\\) means that the matrix is invertible.\n\n\nSolution\nThe unique solution is given by by the first order optimatility condition:\n\\[ \\begin{align}\n(A^T A + \\lambda L^T L) \\mathbf{x}_{\\text{RLS}} - A^T b&= 0 \\\\\n\\mathbf{x}_{\\text{RLS}} &= (A^T A + \\lambda L^T L)^{-1} A^T b\n\\end{align}\n\\]\n\n\nSVD Decomposition\nThe solution can be written in terms of the singular value decomposition of \\(A\\), and with the assumption that \\(L=I\\):\n\\[ \\begin{align}\nA &= U \\Sigma V^T \\\\\nA^T A &= V \\Sigma^T \\Sigma V^T \\\\\n\\mathbf{x}_{\\text{RLS}} &= \\left( V \\Sigma^2 V^T + \\lambda I \\right)^{-1} V \\Sigma^T U^T b \\\\\n&= \\left( V \\Sigma^2 V^T + \\lambda I V V^T \\right)^{-1} V \\Sigma^T U^T b\\\\\n&= V \\left( \\Sigma^2 + \\lambda I \\right)^{-1} \\Sigma^T U^T b\\\\\n&= V \\mathbf{Diag}\\left( \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda} \\right) U^T b\\\\\n&= \\sum _i ^ n \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda} \\langle u_i, b \\rangle v_i\n\\end{align}\n\\]\nThis form is more readily comparable to some of the other methods that we have see so far, which are presented in the table below:"
  },
  {
    "objectID": "content/eosc555/lectures/lecture4/index.html#comparison-of-least-squares-methods",
    "href": "content/eosc555/lectures/lecture4/index.html#comparison-of-least-squares-methods",
    "title": "Lecture 4: Regularization and the Conjugate Gradient Methods",
    "section": "Comparison of Least Squares Methods",
    "text": "Comparison of Least Squares Methods\n\n\n\n\n\n\n\nMethod\nSolution\n\n\n\n\nTikhonov\n\\(x_{\\text{RLS}} = \\sum _i ^ n \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda} \\langle u_i, b \\rangle v_i\\)\n\n\nThresholded SVD\n\\(x_{\\text{TSVD}} = \\sum _i ^ n h(\\sigma_i) \\langle u_i, b \\rangle v_i\\)\n\n\nGradient Flow\n\\(x_{\\text{SDF}} = \\sum _i ^ n \\frac{\\exp(-\\sigma_i^2 t) - 1}{\\sigma_i} \\langle u_i, b \\rangle v_i\\)\n\n\n\nAs we can see all three methods have a similar form and offer some mechanism for controlling the noise induced by the small singular values of \\(A\\).\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef generate_ill_conditioned_matrix(m, n, condition_number):   \n    # Generate random orthogonal matrices U and V\n    U, _ = np.linalg.qr(np.random.randn(m, m))\n    V, _ = np.linalg.qr(np.random.randn(n, n))\n    \n    sigma = np.linspace(1, 1/condition_number, min(m, n))    \n    Sigma = np.diag(sigma)    \n    A = U @ Sigma @ V[:min(m, n), :]\n    \n    return A, sigma\n\n# Seed for reproducibility\nnp.random.seed(4)\nA, S = generate_ill_conditioned_matrix(8, 24, 1e3)\n\n# Create a vector b of size 5 with random values\nb = np.random.randn(8)\n\n# Compute the SVD of A\nU, S, Vt = np.linalg.svd(A, full_matrices=False)\nV = Vt.T\nU = U  # Already in proper shape\n\n# Number of singular values\nn = len(S)\n\n# Define parameters for each method\n# Gradient Flow\nt_values = np.linspace(0, 0.6, 100)\n\n# Tikhonov Regularization\nlambda_values = np.linspace(1e-4, 1, 100)\n\n# Thresholded SVD\nthreshold_values = np.linspace(0, max(S), 100)\n\n# Compute scaling factors for each method\n# Gradient Flow Scaling\ndef gradient_flow_scaling(sigma, t):\n    return (1 - np.exp(-sigma**2 * t)) / sigma\n\ngradient_scalings = np.array([gradient_flow_scaling(s, t_values) for s in S])\n\n# Tikhonov Scaling\ndef tikhonov_scaling(sigma, lambd):\n    return sigma / (sigma**2 + lambd)\n\ntikhonov_scalings = np.array([tikhonov_scaling(s, lambda_values) for s in S])\n\n# Thresholded SVD Scaling\ndef tsvd_scaling(sigma, threshold):\n    return np.where(sigma &gt;= threshold, 1/sigma, 0)\n\ntsvd_scalings = np.array([tsvd_scaling(s, threshold_values) for s in S])\n\n# Initialize the plot with 3 subplots\nfig, axes = plt.subplots(3, 1, figsize=(5, 15))\n\n# Define a color palette\npalette = sns.color_palette(\"husl\", n)\n\n# Plot Gradient Flow\nax = axes[0]\nfor i in range(n):\n    ax.plot(t_values, gradient_scalings[i], color=palette[i], linewidth=2, label=f'$1/\\sigma_{i}$' )\n    ax.axhline(y=1/S[i], color=palette[i], linestyle='--', linewidth=1)\nax.set_yscale('log')\nax.set_xlabel('Time (t)', fontsize=14)\nax.set_ylabel('Scaling Factor', fontsize=14)\nax.set_title('Gradient Flow', fontsize=16)\nax.legend()\nax.grid(True)\n\n# Plot Tikhonov Regularization\nax = axes[1]\nfor i in range(n):\n    ax.plot(lambda_values, tikhonov_scalings[i], color=palette[i], linewidth=2, label=f'$1/\\sigma_{i}$' )\n    ax.axhline(y=1/S[i], color=palette[i], linestyle='--', linewidth=1)\nax.set_yscale('log')\nax.set_xlabel('Regularization Parameter (λ)', fontsize=14)\nax.set_ylabel('Scaling Factor', fontsize=14)\nax.set_title('Tikhonov Regularization', fontsize=16)\nax.legend()\nax.grid(True)\n\n# Plot Thresholded SVD\nax = axes[2]\nfor i in range(n):\n    ax.plot(threshold_values, tsvd_scalings[i], color=palette[i], linewidth=2, label=f'$1/\\sigma_{i}$')\n    ax.axhline(y=1/S[i], color=palette[i], linestyle='--', linewidth=1)\nax.set_yscale('log')\nax.set_xlabel('Threshold (τ)', fontsize=14)\nax.set_ylabel('Scaling Factor', fontsize=14)\nax.set_title('Thresholded SVD', fontsize=16)\nax.legend()\nax.grid(True)\n\n# Adjust layout and add a legend\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nEvolution of scaling factors for three different methods"
  },
  {
    "objectID": "content/eosc555/lectures/lecture4/index.html#solving-least-squares-using-conjugate-gradient",
    "href": "content/eosc555/lectures/lecture4/index.html#solving-least-squares-using-conjugate-gradient",
    "title": "Lecture 4: Regularization and the Conjugate Gradient Methods",
    "section": "Solving Least Squares Using Conjugate Gradient",
    "text": "Solving Least Squares Using Conjugate Gradient\nA detailed explanation of this method can be found at Wikipedia\n\nConjugate Vectors Definition\nA set of vectors \\(\\{ p_1, p_2, \\ldots, p_n \\}\\) is said to be conjugate with respect to a matrix \\(A\\) if:\n\\[\n\\langle p_i, A p_j \\rangle = 0 \\quad \\text{for all } i \\neq j\n\\]\nThis is a generalization of the concept of orthoganality to non-symmetric matrices.\nStandard Orthogonality: When $ A = I $ (the identity matrix), the definition reduces to the standard concept of orthogonality. For a symmetric \\(A\\) we also have an orthogonal decomposition of eigenvectors by spectral theorem.\n\nBack to the problem of least squares, we can express the solution $ x $ as a linear combination of conjugate vectors:\n\\[\nx = x_0 + \\sum_{i=1}^n \\alpha_i p_i\n\\]\nwhere:\n\n\\(x_0\\) is an initial guess (can be zero).\n\\(\\alpha_i\\) are scalar coefficients.\n\\(p_i\\) are conjugate vectors with respect to \\(A\\).\n\nTo recover the coefficients of \\(\\alpha_i\\) we can use a projection in the weighted space of \\(A\\):\n\\[ \\begin{align}\nA x_0 + \\sum_{i=1}^n \\alpha_i A p_i &= b\\\\\nr &= b - A x_0\\\\\n\\sum_{i=1}^n \\alpha_i A p_i &= r\\\\\n\\langle p_i, \\sum_{i=1}^n \\alpha_i A p_i \\rangle &= \\langle p_i, r \\rangle\\\\\n\\alpha_i \\langle p_i, A p_i \\rangle &= \\langle p_i, r \\rangle\\\\\n\\alpha_i &= \\frac{\\langle p_i, r \\rangle}{\\langle p_i, A p_i \\rangle}\n\\end{align}\n\\] In the case where \\(x_0\\) is zero, then this reduces to \\[ \\alpha_i = \\frac{\\langle p_i, b \\rangle}{\\langle p_i, A p_i \\rangle} \\]\n\n\nAlgorithm Steps\nInitialize:\n\n\\(x = x_0\\)\n\\(r_0 = b - A x_0\\)\n\\(p_0 = r_0\\)\n\nFor \\(i = 0,1, 2, \\ldots\\):\n\nCompute \\(\\alpha_i\\):\n\\[\n\\alpha_i = \\frac{\\langle r_i, r_i \\rangle}{\\langle p_i, A p_i \\rangle}\n\\]\nUpdate Solution \\(x\\):\n\\[\nx_{i+1} = x_{i} + \\alpha_i p_i\n\\]\nUpdate Residual \\(r\\):\n\\[\nr_{i+1} = r_{i} - \\alpha_i A p_i\n\\]\nCheck for Convergence:\n\nIf \\(\\| r_{i+1} \\|\\) is small enough, stop.\n\nCompute \\(\\beta_i\\):\n\\[\n\\beta_i = \\frac{\\langle r_{i+1}, r_{i+1}\\rangle}{\\langle r_i,r_i \\rangle}\n\\]\nUpdate Conjugate Direction \\(p_{i+1}\\):\n\\[\np_{i+1} = r_{i+1} + \\beta_i p_i\n\\]\n\n\nThe method can be seen better if we trace through the minimization problem for fixed \\(x\\) and with variable \\(\\alpha\\):\n\\[\n\\begin{align}\n& \\min \\frac{1}{2} \\|A (x+\\alpha p) - b\\|^2  \\\\\n&= \\frac{1}{2}r^T r + \\alpha \\langle r, A p \\rangle + \\frac{1}{2} \\alpha^2 \\langle p, A^T A p \\rangle \\\\\n0 &= \\langle r, A p \\rangle + \\alpha \\langle p, A^T A p \\rangle \\\\\n\\alpha &= -\\frac{\\langle r, A p \\rangle}{\\|A p\\|^2}\n\\end{align}\n\\]\nBut we can also trace this through using the expansion of lest squares and removing the \\(\\|b\\|^2\\) term:\n\\[\n\\begin{align}\n& \\min \\frac{1}{2} \\tilde x^T A x - \\tilde x^T b  \\\\\n&= \\frac{1}{2} \\left( x^T A x + 2 \\alpha x^T A p + \\alpha^2 p^T A p \\right) - x^T b - \\alpha p^T b\\\\\n0&= x^TAp + \\alpha p^T A p - p^T b \\\\\n\\alpha &= \\frac{p^T (Ax-b)}{p^T A p}\n\\end {align}\n\\]"
  },
  {
    "objectID": "content/projects/projects.html",
    "href": "content/projects/projects.html",
    "title": "Project Home",
    "section": "",
    "text": "This project focuses on the development and challenges of building and controlling a reaction wheel unicycle. Performed as part of a 2-year long capstone project in the UBC Engineering Physics program, sponsored by the Engineering Physics project lab.\nExplore the Learning to Balance Project",
    "crumbs": [
      "Home",
      "Projects",
      "Project Home"
    ]
  },
  {
    "objectID": "content/projects/projects.html#learning-to-balance-a-reaction-wheel-unicycle",
    "href": "content/projects/projects.html#learning-to-balance-a-reaction-wheel-unicycle",
    "title": "Project Home",
    "section": "",
    "text": "This project focuses on the development and challenges of building and controlling a reaction wheel unicycle. Performed as part of a 2-year long capstone project in the UBC Engineering Physics program, sponsored by the Engineering Physics project lab.\nExplore the Learning to Balance Project",
    "crumbs": [
      "Home",
      "Projects",
      "Project Home"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/introduction.html",
    "href": "content/projects/RLUnicycle/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Welcome to our self-balancing unicycle robot capstone project! We are a team of undergraduate UBC Engineering Physics students working on our final academic checkpoint as engineering students before being released into the wild. This project is directly sponsored by the UBC Engineering Physics Project Lab.\n\n\n\nTristan Lee, Julian Lapenna, Kyle Mackenzie, Jackson Fraser, and Simon Ghyselincks\n\n\n\n\nOur goal is to design and develop a self-balancing reaction wheel robot that can navigate autonomously and be used as a platform to compare traditional control methods with reinforcement learning. The spirit of the project is to explore some of the challenges in implementing advanced control strategies on a real-world system. This includes bridging the gap between simulated models and real applications, coordinating peripherals with low latency, and designing hardware for controllability. It also presents a great opportunity to apply some fundamental physics and engineering concepts in a hands-on challenge.\n\n\n\nOur work draws on previous advances made in robotics. Notably, the Max Planck Institute’s Wheelbot project has served as a significant source of inspiration, many of our design choices and control strategies are influenced by their work. We aim to build on their development with a more advanced control and motor system that can navigate autonomously and adapt to dynamic disturbances using reinforcement learning.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Introduction"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/introduction.html#project-overview",
    "href": "content/projects/RLUnicycle/introduction.html#project-overview",
    "title": "Introduction",
    "section": "",
    "text": "Welcome to our self-balancing unicycle robot capstone project! We are a team of undergraduate UBC Engineering Physics students working on our final academic checkpoint as engineering students before being released into the wild. This project is directly sponsored by the UBC Engineering Physics Project Lab.\n\n\n\nTristan Lee, Julian Lapenna, Kyle Mackenzie, Jackson Fraser, and Simon Ghyselincks\n\n\n\n\nOur goal is to design and develop a self-balancing reaction wheel robot that can navigate autonomously and be used as a platform to compare traditional control methods with reinforcement learning. The spirit of the project is to explore some of the challenges in implementing advanced control strategies on a real-world system. This includes bridging the gap between simulated models and real applications, coordinating peripherals with low latency, and designing hardware for controllability. It also presents a great opportunity to apply some fundamental physics and engineering concepts in a hands-on challenge.\n\n\n\nOur work draws on previous advances made in robotics. Notably, the Max Planck Institute’s Wheelbot project has served as a significant source of inspiration, many of our design choices and control strategies are influenced by their work. We aim to build on their development with a more advanced control and motor system that can navigate autonomously and adapt to dynamic disturbances using reinforcement learning.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Introduction"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/introduction.html#the-robot",
    "href": "content/projects/RLUnicycle/introduction.html#the-robot",
    "title": "Introduction",
    "section": "The Robot",
    "text": "The Robot\n\n\nThe robot is composed of two reaction wheels, a single drive wheel, a controller, and a battery, all mounted on a 3D printed PLA frame. It has a total height of 30cm and a weight of 1.25kg, incorporating a compact and efficient design intended to allow self-erection from a position resting on its resetting legs. The Jetson Nano acts as an autonomous controller that reads the sensors and reacts to the environment using the motors.\n\nMuch like a unicycle, it balances on one wheel, with side-to-side stability provided by the roll wheel and direction controlled by a yaw wheel. The mechanism of balancing and steering relies on a reaction torque produced by spinning the reaction wheels. When a motor applies torque to one of the flywheels, an equal and opposite torque acts on the robot’s body, with the net effect altering the angular motion of both the wheel and the robot. The unstable axes to control are roll and pitch where the robot will fall to the ground without any intervention.\n\n\n\n\nSide View Pitch Axis\n\n\n\n\n\nSide View Roll Axis",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Introduction"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/introduction.html#the-challenge",
    "href": "content/projects/RLUnicycle/introduction.html#the-challenge",
    "title": "Introduction",
    "section": "The Challenge",
    "text": "The Challenge\nRobotics often confronts complex dynamics that are difficult to model precisely. Traditional control systems, while reliable under predictable conditions, may falter with unexpected disturbances. This project explores how Reinforcement Learning can enable our unicycle robot to adapt through trial and error, improving its decision-making capabilities in a dynamic environment.\n\nPrototyping and Progress\nWe have initiated our project with a Reaction Wheel Inverted Pendulum (RWIP) model to understand and tackle the unstable roll axis dynamics. Our efforts so far have included the application of both a traditional PID controller and an RL controller, with the latter showing promising results in handling dynamic disturbances aggressively yet effectively. With the completion of a function 2-DOF underactuated model, we are now moving towards the development of a full-scale 3-axis robot prototype.\n\n\n\n\n3-Axis Partial Build\n\n\n\n\n\nWith Prototype\n\n\n\n\n\n\n\nRWIP Model",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Introduction"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/introduction.html#looking-ahead",
    "href": "content/projects/RLUnicycle/introduction.html#looking-ahead",
    "title": "Introduction",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nThe insights gained from the RWIP will guide the development of the full-scale robot, with the eventual integration of state-space models for sophisticated control strategies and enhanced point-to-point navigation.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Introduction"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/introduction.html#development-pages",
    "href": "content/projects/RLUnicycle/introduction.html#development-pages",
    "title": "Introduction",
    "section": "Development Pages",
    "text": "Development Pages\nExplore the detailed development of specific components of our project:\n\n\n\nComponent\nDescription\n\n\n\n\nReal-Time Kernel\nDive into how we handle real-time constraints on the Jetson Nano.\n\n\nTelemetry\nDiscover how our system communicates and processes real-time data.\n\n\nDynamics and Control\nLearn about the dynamic modeling and control of our robot prototype.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Introduction"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/telemetry/telemetry.html",
    "href": "content/projects/RLUnicycle/telemetry/telemetry.html",
    "title": "Telemetry and Database Systems for Capstone",
    "section": "",
    "text": "This guide is designed to provide an overview of the telemetry and databasing system that is in use for our Learning to Balance, reinforcement learning unicycle project. First I will provide an overview of some hardware recommendations, then a look at the networked services that are in use, and finally a look at some of the software code and implementation through example code.\n\n\n\nTelemetry Overview\n\n\nThe telemetry and databasing can be seen as a two way pipeline that is controlling the flow of data from sensors, motors, and control deceisions. Data from the robot is offloaded to a server in an efficient manner that takes into consideration limited processing power and the desire for a stable control loop, where it can be further processed. The server in turn manages control signals being sent to the robot which can be used to adjust parameters or send commands to the robot. By using a central server and internet connectivity to all client devices, the database, live telemetry, and control panel can be accessed from any networked computer through a browser or software API, from anywhere in the world.\n\n\nFor this guide, it is assumed that the reader has some familiarity with running commands on a Linux command line and access to a terminal on their client device (e.g. personal laptop). This could be through WSL or VSCode on Windows, or a terminal on a Mac or Linux machine.\nThe guide will also use Python for some of the examples, so a basic understanding of Python is recommended. However, the same libraries used are available in other languages such as C++.\n\n\n\n\nThe Lenovo M900 series of refurbished tiny PCs are recommended as an affordable option that meets the compute needs for a server. The SSD of the device was set to dual boot into Linux Ubuntu 22.04 for the purposes of running a server. This type of device is capable of handling the computational loads of running multiple services at once in the context of managing, database, messaging, and control services. It can also be used as a workstation for the team.\nThe Raspberry Pi 4B 8GB with a an external SSD was tested as a configuration but the requirements are at the limits of the processing power of the devic, and it is not recommended for a server, especially when considering the cost of a refurbished Lenovo.\n\n\n\n\n\n\n\n\n\nLenovo Server\n\n\n\n\n\n\n\nWifi Dongle\n\n\n\n\n\n\n\n\nOur robot is using an Nvidia Jetson Nano 4GB which does not have wi-fi by default. To complicate matters further, a real time (PREEMPT_RT) patch has been applied to our the Linux Kernel. Many wifi dongle drivers are not compatible with the low level kernel changes made by the patch, for example the rtl8188EUS driver no longer was working after the patch.\nA recommended slower but reliable USB Wi-fi dongle for use with outdated and/or patched Linux kernels is the MT7601U Chipset, which was found to work without installation of any additional drivers on Ubuntu 22.04, Ubuntu 16.04 PREEMPT-RT, and Raspbian. The dongle is also very cheap and can be found on Amazon or Aliexpress.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Telemetry and Database Systems for Capstone"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/telemetry/telemetry.html#hardware-recommendation-and-requirements",
    "href": "content/projects/RLUnicycle/telemetry/telemetry.html#hardware-recommendation-and-requirements",
    "title": "Telemetry and Database Systems for Capstone",
    "section": "",
    "text": "The Lenovo M900 series of refurbished tiny PCs are recommended as an affordable option that meets the compute needs for a server. The SSD of the device was set to dual boot into Linux Ubuntu 22.04 for the purposes of running a server. This type of device is capable of handling the computational loads of running multiple services at once in the context of managing, database, messaging, and control services. It can also be used as a workstation for the team.\n\n\n\n\n\n\n\n\n\nLenovo Server\n\n\n\n\n\n\n\nWifi Dongle\n\n\n\n\n\nThe Raspberry Pi 4B 8GB with a an external SSD was tested as a configuration but the requirements are at the limits of the processing power of the devic, and it is not recommended for a server, especially when considering the cost of a refurbished Lenovo.\n\n\n\nOur robot is using an Nvidia Jetson Nano 4GB which does not have wi-fi by default. To complicate matters further, a real time (PREEMPT_RT) patch has been applied to our the Linux Kernel. Many wifi dongle drivers are not compatible with the low level kernel changes made by the patch, for example the rtl8188EUS driver no longer was working after the patch.\nA recommended slower but reliable USB Wi-fi dongle for use with outdated and/or patched Linux kernels is the MT7601U Chipset, which was found to work without installation of any additional drivers on Ubuntu 22.04, Ubuntu 16.04 PREEMPT-RT, and Raspbian. The dongle is also very cheap and can be found on Amazon or Aliexpress.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Telemetry and Database Systems for Capstone"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/telemetry/telemetry.html#zerotier-virtual-network",
    "href": "content/projects/RLUnicycle/telemetry/telemetry.html#zerotier-virtual-network",
    "title": "Telemetry and Database Systems for Capstone",
    "section": "ZeroTier Virtual Network",
    "text": "ZeroTier Virtual Network\nZerotier allows all authorized devices on the network to communicate directly with eachother using assigned virtual IP address, similar to running a local network over a wifi router for example.\nIP Addresses: When a website address is typed into a browser, the request is sent to a Domain Name Server (DNS) which translates the address into an IP address. The IP address is a unique identifier that functions similar to a postal address, marking out the exact location where that server is located on the internet. For example, open a terminal and type ping google.com to see the IP address of Google’s server.\n❯ ping google.com\nPING google.com (142.251.33.78) 56(84) bytes of data.\n64 bytes from sea09s28-in-f14.1e100.net (142.251.33.78): icmp_seq=1 ttl=114 time=21.6 ms\nThe Google webpage can be accessed by typing in the IP address directly into the browser. The number is a unique identifier for that server on the internet.\nPublic and Private IP Addresses: The IP protocol reserves certain ranges of IP addresses for pivate networks. For example, the entire block of addresses 192.168.0.0 – 192.168.255.255 do not point to the wider internet, but are reserved for local devices. This is why home routers can all have the same common IP address of 192.168.0.1 without creating any conflicts. It acts as a local addressing system, like apartment numbers in a building.\nNetwork Ports: Ports are used to differentiate between different services running on the same IP address. For example, a web server might be running on port 80, while an email server might be running on port 25. When typing in a website address, the browser automatically connects to the server on port 80. To connect to a different service, you can specify the port using a colon, for example http://172.22.1.1:8086/ connects to port 8086 which is commonly used for InfluxDB.\nLocal Host: The IP address http://localhost is a special address that points to the local machine. It is used to access services running on the same machine, for example http://localhost:3000 would connect to a service running on port 3000 on the local machine. It is a way to query services running on the same machine without needing to know the IP address.\nThe clients on the ZeroTier network connect to the robot and server using their assigned virtual IP addresses managed by the ZeroTier service.\n\nSetting up ZeroTier Network\nTo setup a network you should first create a free account at https://my.zerotier.com/. Bonus points if you set up a team email so that anyone on the team can login to manage the network as needed. Once you have an account you can create a network and add devices to it. The network ID is a 16 digit number that is used to identify the network.\n\n\nZeroTier Client Setup\nEvery device intended to be part of the network, including laptops, the server, and the Jetson, should have the ZeroTier client installed. Once installed, enter the network ID from the ZeroTier website into the client, and approve the device to join the network. You may also want to assign static IP addresses, especially for critical devices like the server. This can all be managed via the ZeroTier website.\nInstallation Instructions: Download the ZeroTier client here.\n\nSteps:\n\nDownload and Install the ZeroTier client for your operating system:\nStart the ZeroTier service:\n\nOn Windows:\n\nOpen the ZeroTier client, which will add an icon to the system tray.\nRight-click on the icon and select Join Network, then enter the network ID.\nSet the client UI to launch on startup.\n\nOn Linux: Run the following commands:\nsudo systemctl enable zerotier-one\nsudo systemctl start zerotier-one\nsudo zerotier-cli join YOUR_NETWORK_ID\n\nApprove the device:\nTake note of the client ID and either log into the ZeroTier website or use the command-line interface to approve the device for network access.\nVerify the connection:\nAfter approval, you can verify the connection by pinging another connected device on the network using its assigned virtual IP address.\nAssign a static IP (optional):\nFor important devices like the server or the Jetson, assign static IP addresses through the ZeroTier web console under the Members tab. This ensures consistent IP allocation across reboots.\n\n\n\n\n\nManaging IP Addresses\nBelow is an example of a ZeroTier network where the server has been assigned the static IP address 172.22.1.1 on the network:\n\n\n\nZeroTier Network and Access",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Telemetry and Database Systems for Capstone"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/telemetry/telemetry.html#mqtt-overview",
    "href": "content/projects/RLUnicycle/telemetry/telemetry.html#mqtt-overview",
    "title": "Telemetry and Database Systems for Capstone",
    "section": "MQTT Overview",
    "text": "MQTT Overview\n\n\n\nMQTT is a lightweight messaging protocol that provides an efficient and cost-effective method of carrying out telemetry-based communication between devices. MQTT messages are routed through the Lenovo server that is acting as the broker using Mosquitto. The robot can publish data to a topic, which can be picked up by various subscribers such as Grafana or other laptops, phones, etc that are connected to the broker and subscribed to the topic. Likewise, return commands to the robot can be sent via a command topic to the robot to turn it on or off, or adjust parameters. The default port for MQTT is 1883.\nFor setup, installation, and maintenance of the broker, I recommend installing MQTT Explorer on any device connected to the network. This allows for montitoring of all messaging and client connections across the system.\nDownload from MQTT Explorer\n\nKey Features of MQTT\n\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\nLightweight Protocol\nIdeal for constrained devices and networks with limited bandwidth.\n\n\nPublish-Subscribe Model\nAllows devices to publish messages to a topic and any client subscribed to that topic will receive the messages.\n\n\nReliable Message Delivery\nOffers various levels of Quality of Service (QoS) to guarantee message delivery.\n\n\nMinimal Overhead\nAdds only a small overhead to each message, ensuring efficient use of network resources.\n\n\n\n\n\n\nInstalling Mosquitto MQTT Broker\nFrom the server open a terminal and run the following commands to install the MQTT broker:\nsudo apt update\nsudo apt install mosquitto mosquitto-clients\nsudo systemctl enable mosquitto\nsudo systemctl start mosquitto\nAfter installation, open up the MQTT Explorer and connect to the broker using the IP address of the server and the default port. You should see the server as a client connected to the broker.\n\n\n\nConnecting to MQTT Broker\n\n\nOnce connected, some test messages can be sent through the GUI and verified that they are being received by the server.\nFor more detailed instructions or help, consider using ChatGPT for some test commands to verify the installation is working as intended.\n\n\nInterfacing with Software\nMQTT interfaces with Python, Node-Red, and Grafana to provide a network of communication topics. The broker can be accessed by any device on the ZeroTier network, where messages can be sent to a topic, or action can be taken based on a message received from a topic.\nAn example Python script for publishing messages to the MQTT broker is shown below, which publishes a test message to the topic “jetson/telemetery” every 1 second. Notice that the two key components to a successful message are the topic and the message payload. The topic is the address that the message is sent to, and the payload is the data that is being sent. The payload can be a string, a number, or a JSON object. For our project, we are using JSON objects to send data.\nThe package is installed using pip: pip install paho-mqtt\n\nimport json\nimport time\nimport paho.mqtt.client as mqtt\nimport random\n\n# Define the MQTT settings\nbroker_address = \"172.22.1.1\"  # Lenovo's IP address (replace with your broker IP)\nport = 1883\ntopic = \"jeston/telemetry\"\n\n# Create an MQTT client instance\nclient = mqtt.Client()\n\n# Define the callback for receiving messages\ndef on_message(client, userdata, message):\n    print(f\"Message received on topic {message.topic}: {message.payload.decode()}\")\n\n# Define the callback for connecting to the broker\ndef on_connect(client, userdata, flags, rc):\n    print(\"Connected to broker with result code \" + str(rc))\n    # Subscribe to the topic when connected\n    client.subscribe(topic)\n\n# Assign the callbacks\nclient.on_message = on_message\nclient.on_connect = on_connect\n\n# Connect to the broker\nclient.connect(broker_address, port)\n\n# Start the loop to process messages\nclient.loop_start()\n\n# Publish some test messages to the topic every second\ntry:\n  range(3)\n  for i in range(3):\n        message = {\"sensor\": \"temperature\", \"value\": 20 + random.random() * 5}\n        client.publish(topic, json.dumps(message))\n        print(f\"Published message: {message}\")\n        time.sleep(1)\nexcept KeyboardInterrupt:\n    print(\"Exiting...\")\n\n# Stop the loop and disconnect\nclient.loop_stop()\nclient.disconnect()\n\nPublished message: {'sensor': 'temperature', 'value': 24.716931987828893}\n\n\nC:\\Users\\sghys\\AppData\\Local\\Temp\\ipykernel_35516\\406674647.py:12: DeprecationWarning: Callback API version 1 is deprecated, update to latest version\n  client = mqtt.Client()\n\n\nConnected to broker with result code 0\nPublished message: {'sensor': 'temperature', 'value': 20.272412564292544}\nMessage received on topic jeston/telemetry: {\"sensor\": \"temperature\", \"value\": 20.272412564292544}\nPublished message: {'sensor': 'temperature', 'value': 23.740786448298167}\nMessage received on topic jeston/telemetry: {\"sensor\": \"temperature\", \"value\": 23.740786448298167}\n\n\n&lt;MQTTErrorCode.MQTT_ERR_SUCCESS: 0&gt;\n\n\nNote that in this demo script, the client is both publishing and subscribing to the same topic. In practice for the robot we are using multiple topics for different data streams and commands. A more advanced implementation to assign topics and manage data can be found at: RLUnicycle\nThis test script is useful for publishing test data when it comes to verifying the installation of InfluxDB and Grafana ahead.\nWe are now at this stage in the setup:\n\n\n\n\n\nflowchart TD\n  %% Customizing colors for subgraphs and nodes\n  style Robot fill:#E3F2FD,stroke:#90CAF9,stroke-width:2px\n  style Server fill:#FFF8E1,stroke:#FFCC80,stroke-width:2px\n  style Clients fill:#F1F8E9,stroke:#A5D6A7,stroke-width:2px\n\n  Y[Robot]\n  Z[Server]\n\n  %% Define a class for black text\n  classDef blackText fill:none,color:#000,stroke:none;\n\n  subgraph Robot\n    E(Jetson)\n  end\n  \n  subgraph Server\n    E &lt;--&gt; F[MQTT Broker]\n  end\n\n  %% Clients section coloring applied to individual floating nodes\n    K[Python Script] --&gt;|Commands| F\n    L[Laptop] --&gt;|SSH| E\n    L --&gt;|MQTT Explorer| F\n    L --&gt; K\n  \n  %% Styling for the floating client items\n  style K fill:#F1F8E9,stroke:#A5D6A7,stroke-width:2px\n  style L fill:#F1F8E9,stroke:#A5D6A7,stroke-width:2px\n  style Y fill:#E3F2FD,stroke:#90CAF9,stroke-width:2px\n  style Z fill:#FFF8E1,stroke:#FFCC80,stroke-width:2px",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Telemetry and Database Systems for Capstone"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/telemetry/telemetry.html#grafana-live-telemetry-and-database-dashboards",
    "href": "content/projects/RLUnicycle/telemetry/telemetry.html#grafana-live-telemetry-and-database-dashboards",
    "title": "Telemetry and Database Systems for Capstone",
    "section": "Grafana Live Telemetry and Database Dashboards",
    "text": "Grafana Live Telemetry and Database Dashboards\nGrafana is a powerful open-source platform for creating dashboards and visualizing time-series data. It is particularly well-suited for monitoring and analyzing real-time data. Grafana supports a wide range of data sources and can be used to display both live and historical data in a variety of formats, including graphs, tables, and gauges. Think of it as graph nirvana.\nWhen it comes to viewing the telemetry data, a plugin can be installed to function as a bridge between the MQTT broker and the Grafana server. https://grafana.com/grafana/plugins/grafana-mqtt-datasource/\nTo setup Grafana, install the software on the Lenovo server first. The default port for Grafana is 3000. The program operates through a web browser and can be accessed by navigating to the IP address of the Lenovo server on port 3000.\nOnce you have logged in, you can add a data source by selecting MQTT from the list of available data sources if you have correctly installed the plugin. The panel will listen to all messages on a particular topic and display them in a graph or table format.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Telemetry and Database Systems for Capstone"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/telemetry/telemetry.html#influxdb-databasing",
    "href": "content/projects/RLUnicycle/telemetry/telemetry.html#influxdb-databasing",
    "title": "Telemetry and Database Systems for Capstone",
    "section": "InfluxDB Databasing",
    "text": "InfluxDB Databasing\nThe database can be accessed through Python API, through Grafana, or even a direct viewer. The database is also to be installed on the Lenovo server. The default port for InfluxDB is 8086. The database can be accessed through a web browser by navigating to the IP address of the Lenovo server on port 8086.\nMQTT gives livestream data but if we want data storage and permanence between runs it needs to be databased. InfluxDB offers this service along with data manipulation services and a special query language. It also includes a data explorer through the web interface.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Telemetry and Database Systems for Capstone"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/telemetry/telemetry.html#node-red",
    "href": "content/projects/RLUnicycle/telemetry/telemetry.html#node-red",
    "title": "Telemetry and Database Systems for Capstone",
    "section": "Node Red",
    "text": "Node Red\nNode Red is a flow-based open source development tool for visual programming developed by IBM. It is used for wiring together hardware devices, APIs, and online services in new and interesting ways. It provides a browser-based editor that makes it easy to wire together flows using the wide range of nodes in the palette that can be deployed to its runtime in a single-click.\nFor this application Node Red is used to bridge the MQTT broker to the InfluxDB database. This allows for the data to be stored in a database for later access. The data can be manipulated and stored in a more permanent format.\nThe Node Red server is installed on the Lenovo server. The default port for Node Red is 1880. The program operates through a web browser and can be accessed by navigating to the IP address of the Lenovo server on port 1880.\n\nNode Red Dashboard\n\n\n\nNode Red Dashboard\n\n\nThe Node Red dashboard is an additional feature that is accessed through the Node Red server. It allows for the creation of custom dashboards that can be used to provide a GUI for a robotics project. The GUI can be used to connect directly to incoming signals and also produce outgoing command signals that can be sent to the robot.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Telemetry and Database Systems for Capstone"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/telemetry/telemetry.html#prerequisites",
    "href": "content/projects/RLUnicycle/telemetry/telemetry.html#prerequisites",
    "title": "Telemetry and Database Systems for Capstone",
    "section": "",
    "text": "For this guide, it is assumed that the reader has some familiarity with running commands on a Linux command line and access to a terminal on their client device (e.g. personal laptop). This could be through WSL or VSCode on Windows, or a terminal on a Mac or Linux machine.\nThe guide will also use Python for some of the examples, so a basic understanding of Python is recommended. However, the same libraries used are available in other languages such as C++.\n\n\n\n\nThe Lenovo M900 series of refurbished tiny PCs are recommended as an affordable option that meets the compute needs for a server. The SSD of the device was set to dual boot into Linux Ubuntu 22.04 for the purposes of running a server. This type of device is capable of handling the computational loads of running multiple services at once in the context of managing, database, messaging, and control services. It can also be used as a workstation for the team.\nThe Raspberry Pi 4B 8GB with a an external SSD was tested as a configuration but the requirements are at the limits of the processing power of the devic, and it is not recommended for a server, especially when considering the cost of a refurbished Lenovo.\n\n\n\n\n\n\n\n\n\nLenovo Server\n\n\n\n\n\n\n\nWifi Dongle\n\n\n\n\n\n\n\n\nOur robot is using an Nvidia Jetson Nano 4GB which does not have wi-fi by default. To complicate matters further, a real time (PREEMPT_RT) patch has been applied to our the Linux Kernel. Many wifi dongle drivers are not compatible with the low level kernel changes made by the patch, for example the rtl8188EUS driver no longer was working after the patch.\nA recommended slower but reliable USB Wi-fi dongle for use with outdated and/or patched Linux kernels is the MT7601U Chipset, which was found to work without installation of any additional drivers on Ubuntu 22.04, Ubuntu 16.04 PREEMPT-RT, and Raspbian. The dongle is also very cheap and can be found on Amazon or Aliexpress.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Telemetry and Database Systems for Capstone"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/telemetry/telemetry.html#connecting-to-robot-controller-using-ssh",
    "href": "content/projects/RLUnicycle/telemetry/telemetry.html#connecting-to-robot-controller-using-ssh",
    "title": "Telemetry and Database Systems for Capstone",
    "section": "Connecting to Robot Controller using SSH",
    "text": "Connecting to Robot Controller using SSH\nNow that there is a virtual network created, it is a good time to enable remote access to the robot controller via SSH. SSH is a secure shell protocol that allows for running command line commands on a remote device. This can be very useful for managing the robot, running scripts, and updating software code. To access the controller, use the virtual IP address assigned to the robot on the ZeroTier network. It is recommended to assign a static IP address using ZeroTier UI or CLI, so that it the address does not change between reboots.\nFor faster access to ssh devices, consider setting up an alias and or an ssh key once ssh is verified working for a device. More information on SSH can be found here\nExample: Our robot has the static ip address 172.22.0.5. To connect from a linux terminal on a computer connected to the private network with zerotier client installed, run the following command:\nssh jetson@172.22.0.5\nThis will attempt to login to the username jetson on the robot controller, which will then prompt for a user password, the same one as if logging in directly on the robot. For a controller that is running Linux, we recommend setting up different users on the system so that each team member is able to login to their own account and manage their own files and credentials. The software between user accounts can be shared using symbolic links to a central repository, or using GitHub to manage individual software branches.\n\nVSCode Server\nThe Jetson is also able to handle VSCode Server (https://code.visualstudio.com/docs/remote/ssh), although the outdated Ubuntu 16.04 requires running an older version of VSCode IDE to ssh in. It is important to be aware that VSCode Server is active on the Jetson when sshed in, which takes up some system resources, although in practice it has not been a performance issue. For best performance, running the robot through a simple terminal is recommended. The benefits of sshing through VSCode is that it provides a GUI interface for software development and file management on the robot, as if the user was using VSCode on their own computer.",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Telemetry and Database Systems for Capstone"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/telemetry/telemetry.html#telegraf-and-influxdb",
    "href": "content/projects/RLUnicycle/telemetry/telemetry.html#telegraf-and-influxdb",
    "title": "Telemetry and Database Systems for Capstone",
    "section": "Telegraf and InfluxDB",
    "text": "Telegraf and InfluxDB\n\n \n\nTelegraf and InfluxDB are free an open-source products that are made by the same company, InfluxData. Telegraf is driven by a configuration file that organizes data coming in from multiple sources for processing and forwarding into the InfluxDB database. InfluxDB is a time-series database that is used to store the telemetry data that is being streamed from the robot. It has a web browser interface that can be used to explore the data, along with APIs in Python and other languages that can make queries to the database.\nFollow the instructions from in the links above to install both services on the server.\n\nInfluxDB Configuration\nThe InfluxDB database can be accessed through a web browser by navigating to the IP address of the Lenovo server on port 8086. For example, http://172.22.1.1:8086/. A login process will establish a username, password, and organization. The organization is simply a way to group data together across users. A bucket is a way to group data together within an organization. For our database we have assigned the organization name as Capstone and the bucket name as telegraf but these are free to choose. Once the organization and bucket have been created, the database is ready to receive data.\n\n\nTelegraf Configuration\nThe Telegraf configuration file is located at /etc/telegraf/telegraf.conf by default. The default configuration is ~10,000 lines which are mostly commented out and pertain to services that we are not using. One way to deal with this is to make a backup of the original file and then delete all the commented out lines.\nTelegraf can be seen as a central messaging switchboard. To make use of it we need to connect the MQTT topics into the switchboard, and connect the output of the processing to the InfluxDB database for storage. The main changes that are suggested from the default configuration are to:\n\nRemove the logging of server stats from the pool of inputs.\nAdd the MQTT input plugin to the configuration file.\nAdd the InfluxDB output plugin to the configuration file.\n\nA stripped down header is shown below:\n# Default Header\n[global_tags]\n\n# Configuration for telegraf agent\n[agent]\n  ## Default data collection interval for all inputs\n  interval = \"10s\"\n  round_interval = true\n  metric_batch_size = 1000\n  metric_buffer_limit = 10000\n  collection_jitter = \"0s\"\n  flush_interval = \"10s\"\n  flush_jitter = \"0s\"\n  precision = \"\"\n\n  ## Override default hostname, if empty use os.Hostname()\n  hostname = \"\"\n  ## If set to true, do no set the \"host\" tag in the telegraf agent.\n  omit_hostname = true\n\n\n\n\n\nNow we add InfluxDB as an output plugin and MQTT as an input plugin. The MQTT plugin is used to listen to messages on a particular topic and the InfluxDB plugin is used to write the data to the database. The configuration for the MQTT plugin is shown below.\nNote the token above can be generated through the InfluxDB web interface.\n\n\n[[outputs.influxdb_v2]]\n  # localhost assumes telegraf and influxdb are on the same server\n  urls = [\"http://localhost:8086\"]\n  token = \"api token from InfluxDB\"\n  organization = \"Capstone\"\n  bucket = \"telegraf\"\n\n[[outputs.prometheus_client]]\n    listen = \":9273\"\n    metric_version = 2\nFinally the incoming messages from MQTT are processed. A very important consideration here is to fully automate the process of message conversion from JSON to adopt a robot-driven database. The core principle is that changes in the robot software and telemetry should not change either this configuration file or the database schema. In our case, all messages that are to be databased start with robot/ and the topic indicates the data category. For example robot/imu1 is the MQTT topic that recieves information on the imu sensor {ax: 0.1, ay: 0.2, az: 0.3, gx: 0.4, gy: 0.5, gz: 0.6}. Telegraph identifies that this is to be databased, removes the robot/ prepend, records the json message _measurement as imu1 and the _field as ax, ay, az, gx, gy, gz.\n[[processors.starlark]]\n  source = '''\ndef apply(metric):\n    # Get the topic tag value (e.g., \"robot/motor\")\n    topic = metric.tags.get(\"topic\")\n    \n    # Extract the part after \"robot/\"\n    if topic.startswith(\"robot/\"):\n        measurement = topic.split(\"robot/\", 1)[1]\n        # Set the new measurement based on the tail of the topic\n        metric.name = measurement\n    \n    return metric\n'''\n\n# MQTT Consumer Input Plugin\n[[inputs.mqtt_consumer]]\n  servers = [\"tcp://localhost:1883\"]\n\n  topics = [\n    \"robot/#\"  # Subscribe to all subtopics under robot/\n  ]\n  qos = 0\n  client_id = \"telegraf_mqtt_consumer\"\n  data_format = \"json\"\n  ## Use a part of the topic or JSON structure as the measurement name.\n  json_name_key = \"measurement\"\nThat is it for the configuration file, the other components that are recording server metrics can be eliminated to clear up the database.\n\n\nTesting the Configuration with Data Explorer\nNow go back to the mqtt python script and send some messages to a robot/ topic for testing. They should now be automatically processed into the database under the scheme described. Verify the messages are passing through the MQTT broker, then verify they are reaching the InfluxDB database using the data explorer.\n\nIf all goes well, the data explorer will show the bucket has new data. The measurement filter will show the topic that was passed to MQTT, the field will show the keys from the passed JSON, and the data will show the values. Important to note is that InfluxDB automatically applies data operations such as aggregration to reduce the number of sample points. This can be managed using the window period on the right hand side.\nInflux also has its own query language, which can be previewed clicking the Script Editor button. This will give direct insight into how the data is being processed when a query is sent and can be edited to further finetune the settings or to be used as an API call from elsewhere (for example from a Python script to make plots).\nfrom(bucket: \"telegraf\")\n  |&gt; range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |&gt; filter(fn: (r) =&gt; r[\"_measurement\"] == \"sensors/imu\")\n  |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"accel_x\")\n  |&gt; aggregateWindow(every: 100ms, fn: mean, createEmpty: false)\n  |&gt; yield(name: \"mean\")\nThe accel_x is being aggregated into 100ms sample periods using the mean of all values in that window. This can be removed to get the raw data, or changed to a different aggregation function. The range values can also be set to relative times to get the last 10 minutes of data for example.\nfrom(bucket: \"telegraf\")\n  |&gt; range(start: -10m)\n  |&gt; filter(fn: (r) =&gt; r[\"_measurement\"] == \"sensors/imu\")\n  |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"accel_x\")\nBuilding these queries through the script editor is a good way to get the correct string to use in a Python script to make a query to the database.\n\n\nExample Python Query\nTo get a feel for how this can be integrate into Python for data analysis, a simple example is shown below. This script will query the last 1 minute of data from the database and plot the acceleration data over time.\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom influxdb_client import InfluxDBClient\n\n# Server IP address with InfluxDB port\nurl = \"http://172.22.1.1:8086\"\ntoken = \"gGu-3t4Avltf6-yHamGXItRfOKBQIDLgWEfhdURE7wURQazK_yvIa8O9k0O-_doXX8Q0Acy82vVavb5AcM2Lhw==\"\norg = \"Capstone\"\nbucket = \"telegraf\"\n\nclient = InfluxDBClient(url=url, token=token, org=org)\n\n# Query for the last 10 minutes of data\nlast_mins = 1\nquery = f'''\nfrom(bucket: \"{bucket}\")\n  |&gt; range(start: -{last_mins}m)\n  |&gt; filter(fn: (r) =&gt; r[\"_measurement\"] == \"sensors/imu\")\n  |&gt; filter(fn: (r) =&gt; r[\"_field\"] == \"accel_x\")\n  |&gt; aggregateWindow(every: 1s, fn: mean, createEmpty: false)\n'''\n\n# Query the data\nquery_api = client.query_api()\ntables = query_api.query(org=org, query=query)\n\n# Extract values (accel_x) from query response\nvalues = [record.get_value() for table in tables for record in table.records]\n\n# Plot using Seaborn\nplt.figure(figsize=(5, 3))\nsns.lineplot(data=values, linewidth=2.5)\n\n# Customize plot\nplt.title('Acceleration Data (accel_x) Over Time', fontsize=16)\nplt.xlabel('Steps', fontsize=14)\nplt.ylabel('Acceleration (accel_x)', fontsize=14)\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\nThis concludes the setup of the MQTT, Telegraf, and InfluxDB services. The next step is to setup Grafana for live telemetry and database dashboards.\n\nWe are now at this stage in the setup:\n\n\n\n\n\nflowchart TD\n  %% Customizing colors for subgraphs and nodes\n  style Robot fill:#E3F2FD,stroke:#90CAF9,stroke-width:2px\n  style Server fill:#FFF8E1,stroke:#FFCC80,stroke-width:2px\n  style Clients fill:#F1F8E9,stroke:#A5D6A7,stroke-width:2px\n\n  Y[Robot]\n  Z[Server]\n\n  %% Define a class for black text\n  classDef blackText fill:none,color:#000,stroke:none;\n\n  subgraph Robot\n    E(Jetson)\n  end\n  \n  subgraph Server\n    E &lt;--&gt; F[MQTT Broker]\n    F --&gt;|Metrics| G[Telegraf]\n    G --&gt;|Write| H[InfluxDB]\n  end\n\n  %% Clients section coloring applied to individual floating nodes\n    K[Python Script] --&gt;|Commands| F\n    L[Laptop] --&gt;|SSH| E\n    L --&gt;|MQTT Explorer| F\n    L --&gt; K\n    H --&gt;|Data| M[Data Explorer and API]\n  \n  %% Styling for the floating client items\n  style K fill:#F1F8E9,stroke:#A5D6A7,stroke-width:2px\n  style L fill:#F1F8E9,stroke:#A5D6A7,stroke-width:2px\n  style M fill:#F1F8E9,stroke:#A5D6A7,stroke-width:2px\n  style Y fill:#E3F2FD,stroke:#90CAF9,stroke-width:2px\n  style Z fill:#FFF8E1,stroke:#FFCC80,stroke-width:2px",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Telemetry and Database Systems for Capstone"
    ]
  },
  {
    "objectID": "content/projects/RLUnicycle/telemetry/telemetry.html#grafana-live-telemetry",
    "href": "content/projects/RLUnicycle/telemetry/telemetry.html#grafana-live-telemetry",
    "title": "Telemetry and Database Systems for Capstone",
    "section": "Grafana Live Telemetry",
    "text": "Grafana Live Telemetry\n\nGrafana is a powerful open-source platform for creating dashboards and visualizing time-series data. Grafana supports a wide range of data sources and can be used to display both live and historical data. While it is able to refresh data from InfluxDB at a rate of every 5s, this is too slow for live monitoring of a dynamic system. Instead the live telemetry is pulled directly from the MQTT broker.\nTo begin, download and install Grafana on the server. The default port for Grafana is 3000. Once installed, open up a web browser and navigate to http://localhost:3000/ to access the Grafana dashboard. The default login is admin with the password admin.\n\nAdding Data Sources\nGrafana is primarily a visualization tool, so it needs to be provided with a pointer and credentials to the datasources that will be monitored: InfluxDB and MQTT. The InfluxDB datasource is included by default as a plugin. Note that the correct query language should be specified and it will require some credentials along with an address (http://localhost:8086).\nTo setup Grafana, install the software on the Lenovo server first. The default port for Grafana is 3000. The program operates through a web browser and can be accessed by navigating to the IP address of the Lenovo server on port 3000.\n\n\n\n\n\nWhat is more of interest is to view the MQTT data in real time. A plugin can be installed to connect Grafana to the Mosquitto broker: Repo. A simpler installation can be managed by logging into the Grafana server through a browser and navigating to Add new connection.\nOnce logged in, add a data source by selecting MQTT from the list of available data sources. It will show as available if the plugin is correctly installed. Name the data source, then specify the connection which will be the MQTT broker tcp://localhost:1883. Add a username and password if one has been configured for the broker. Now Grafana is aware of the data stream and where to access it.\n\n\n\n\nCreating Dashboards and Panels\nNow it is time to setup a dashboard, a collection of data panels. The Grafana interface is user friendly, but we are interested in some key settings.\n\nThe window of time that is being displayed in the dashboard.\nThe MQTT topics that are being fetched for display.\nThe keys from the JSON that are being displayed.\n\nLog into the Grafana homepage from any device connected to the private network and with the server IP: http://172.22.1.1:3000/. Select Dashboards from the left hand menu and then New Dashboard, then New -&gt; New Dashboard. A prompt to select a datasource will come up, select the MQTT datasource that has been configured. Now it is in dashboard view.\nFor this stage, it is recommended to either have a robot sensor streaming data, or a surrogate Python script sending out data to the MQTT broker so that there are live streaming messages to display.\n\n\n\n\n\nStep 1: Create a New Dashboard\nSave the newly created dashboard. Any changes to the settings are not automatically saved, so it is essential to save important changes to settings and panels periodically to avoid losing them from a page refresh. Also, change the time range of the dashboard to the last 30s to see recent data streaming in. Keep in mind that Grafana is designed for many use cases where monitoring is happening over much longer time ranges and a robot is a very dynamic system. Apply the time range.\n\n\n\n\n\n\n\nStep 2: Add a Visualization Panel\nSelect the Add a visualization button to make the first panel. Now, enter the MQTT topic and verify that the topic is streaming data with MQTT Explorer and the topic name. The data should begin streaming in the panel preview. Use the Query Inspector and Data tab to verify that data is being received and processed correctly if the visualization is not showing the expected data. Note that all JSON fields are shown, but it’s important to filter only the desired ones.\n\n\n\nStep 3: Customize the Panel\n Select the Transform Data tab and then Filter fields by name option. Fields that are to be omitted can be removed from the identifier list and will not be displayed in the panel. Finally the right hand side of the panel configuration can be used to fully customize the display of data, panel title, etc.\nSave and apply the panel change. Now is a good time to bookmark the dashboard for easy access in the future. The live telemetry has limitations in how much data can be displayed at once, since it is sampling from a moving buffer and not storing the data like the database. Too many panels with too much data will cause the system to stutter, so the recommendation is to downsample the data to about 10Hz or less unless full sample resolution is needed.\n\n\n\nNotes on Downsampling\nAs of writing, I have not found an effective way to downsample the incoming data in Grafana. Telegraf has some dataprocessing that can be used to downsample data, but it would need to be rebroadcast over a new MQTT topic. The simplest solution that we have found is to simply downsample from the robot side by sending full resolution data to each of the “robot/” topics and every \\(n\\)th message to a “downsampled/” topic. This can be done with few lines of code, does not add much overhead to the software code, and is likely the easiest solution to implement.\n\nWe are now at this stage in the setup:\n\n\n\n\n\nflowchart TD\n  %% Customizing colors for subgraphs and nodes\n  style Robot fill:#E3F2FD,stroke:#90CAF9,stroke-width:2px\n  style Server fill:#FFF8E1,stroke:#FFCC80,stroke-width:2px\n  style Clients fill:#F1F8E9,stroke:#A5D6A7,stroke-width:2px\n\n  Y[Robot]\n  Z[Server]\n\n  %% Define a class for black text\n  classDef blackText fill:none,color:#000,stroke:none;\n\n  subgraph Robot\n    E(Jetson)\n  end\n  \n  subgraph Server\n    E &lt;--&gt; F[MQTT Broker]\n    F --&gt;|Metrics| G[Telegraf]\n    G --&gt;|Write| H[InfluxDB]\n    F --&gt;|Visualization| I[Grafana]\n  end\n\n  %% Clients section coloring applied to individual floating nodes\n    I --&gt;|Live Telemetry| J[Dashboard]\n    H --&gt;|Data| M[Data Explorer and API]\n\n  \n  %% Styling for the floating client items\n  style J fill:#F1F8E9,stroke:#A5D6A7,stroke-width:2px\n  style M fill:#F1F8E9,stroke:#A5D6A7,stroke-width:2px\n  style Y fill:#E3F2FD,stroke:#90CAF9,stroke-width:2px\n  style Z fill:#FFF8E1,stroke:#FFCC80,stroke-width:2px",
    "crumbs": [
      "Home",
      "Projects",
      "Learning to Balance",
      "Telemetry and Database Systems for Capstone"
    ]
  }
]