---
title: "Trace Estimation for Implicit Matrices"
subtitle: "CPSC 536M Project"
date: 2026-01-18
author: "Simon Ghyselincks"
description: >-
  Implicit matrix trace estimation using only matrix-vector products is a problem that commonly arises in many areas of scientific computing and machine learning. This work explores randomized algorithms including Hutchinson's and Hutch++.
categories:
    - Randomized Algorithms
    - Linear Algebra
    - Machine Learning

bibliography: references.bib
biblatexoptions: "style=numeric,sorting=nyt"
bibliostyle: numeric-comp

draft: false

execute:
  jupyter: python3

format:
  html:
    code-fold: true
    code-summary: "Show the code"
    toc: true
    number-sections: true
    theme: cosmo
    html-math-method: mathjax
---

$$
\newcommand{\mA}{\mathbf{A}}
\newcommand{\mI}{\mathbf{I}}
\newcommand{\mH}{\mathbf{H}}
\newcommand{\mD}{\mathbf{D}}
\newcommand{\mF}{\mathbf{F}}
\newcommand{\mQ}{\mathbf{Q}}
\newcommand{\mS}{\mathbf{S}}
\newcommand{\mG}{\mathbf{G}}
\newcommand{\mU}{\mathbf{U}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\va}{\mathbf{a}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Pr}{\mathbb{P}}
\newcommand{\mathbb}{\mathbf}
$$

::: {.callout-note appearance="simple"}
## Abstract
Implicit matrix trace estimation using only matrix-vector products is a problem that commonly arises in many areas of scientific computing and machine learning. When the matrix is only accessible through matrix-vector products $\mA \vv$ or is excessively large, classical deterministic methods for computing the trace are infeasible. This work explores randomized algorithms for trace estimation, building upon Hutchinson's estimator and its variants. The theoretical bounds, including $(\epsilon, \delta)$-approximations, are analyzed, and the performance of these methods is evaluated through numerical experiments on matrices with different spectral properties. A review of prior work and recent advancements in this area is also provided.
:::

# Introduction

Implicit matrix trace estimation for a matrix $\mA$ using only the matrix-vector product $\mA \vv$ over some set of $\mA \vv_1,\mA \vv_2,...\mA \vv_m \in \mathbb{R}^N$ is a problem that commonly arises in scientific computing and machine learning [@UbaruSaad2018]. When $\mA$ is only accessible through the map $x \mapsto \mA x$, there is no direct access to its matrix elements, and we call this an implicit representation. Even in the case where $\mA$ is explicitly computable, it may be of an excessively large dimension, making it infeasible to store and compute the full matrix. Similarly, for $\mA \in \mathbb{R}^{N \times N}$, although $\tr(\mA)$ is simply the sum of diagonal elements, it requires evaluating $\sum_{i\in [N]} e_i^\top \mA e_i$, which for large $n$ may be computationally expensive, intractable, or unnecessary for the level of precision required. 

Often these problems arise within an iterative process, where an estimation will suffice under the expectation that future algorithmic steps will further refine the problem solution [@woodruffOptimalQueryComplexities]. This problem formulation is well suited to randomized numerical linear algebra (RNLA) techniques, and as discussed below, can even be effectively paired with other randomized algorithms such as low-rank matrix estimation for further efficiency gains [@meyer2021HutchOptimalStochastic]. 

The goal of trace *estimation* then is to find an approximation within some error tolerance such that the total required matrix-vector products scale sublinearly in $N$, avoiding $O(N)$ complexity. A naive approach would be to randomly sample a subset of the diagonal elements of $\mA$ as an estimator, but the diagonal elements may be highly concentrated, resulting in the same variance issues posed by sparse sampling [see @Harvey2021Randomized2, Ch 24.3.1]. Thus we must employ more sophisticated methods to gain better approximations at a cheaper cost, as measured by both computational complexity and the total matrix-vector products. This motivates the need for generalizable methods capable of probing the structure of $\mA$ using a limited number of matrix-vector products or other implicit representations.

## Trace Estimation Applications
Examples of trace estimation problems are abundant [@UbaruSaad2018]. In deep learning, large and complex models often require the estimation of traces of Hessian matrices $\mH$ for optimization and uncertainty quantification. A common architecture such as ResNet-50 has a $25 \text{ million} \times 25 \text{ million}$ matrix $\mH$ that is prohibitive to store, but whose matrix-vector product $\mH \vv$ can be efficiently computed using Pearlmutter's trick [@pearlmutter1994fast]. High-dimensional trace estimation is also used for training Physics Informed Neural Networks (PINNs) [@HU2024116883]. In probabilistic machine learning, trace estimation is used in score-matching methods [@haber_lecture11_2025].

## Project Contribution
This work explores and summarizes the progression of Randomized Numerical Linear Algebra (RNLA) methods for trace estimation, analyzes their theoretical properties, and evaluates their performance through numerical experiments. Specifically, we:

1. Review the foundational Hutchinson's estimator [@Hutchinson01011990].
2. Analyze the $(\epsilon, \delta)$ bounds derived by @avron2011RandomizedAlgorithmsEstimating.
3. Implement and verify the Hutch++ algorithm [@meyer2021HutchOptimalStochastic], which combines low-rank approximation with stochastic estimation.
4. Compare these methods numerically on matrices with different spectral decay properties.

# Background {#sec-background}

## Notation and Definitions
In this work, lower case $a$ denotes a scalar, capital $A$ denotes a random variable, lower case bold $\va$ denotes a vector, and upper case bold $\mA$ denotes a matrix. A summary of the notation used is found below in @tbl-notation.

| Symbol | Description |
| :--- | :--- |
| $\mA \in \mathbb{R}^{N \times N}$ | Symmetric Positive Semi-Definite (SPSD) matrix |
| $\tr(\mA)$ | Trace of matrix $\mA$ |
| $\lambda_i$ | The $i$-th eigenvalue of $\mA$ |
| $\vv$ | Vector in $\mathbb{R}^N$ |
| $m$ | Number of Matrix-Vector Product (MVP) $\mA \vv$ queries |
| $\epsilon$ | Relative error tolerance |
| $\delta$ | Probability of failure |
| $\|\mA\|_F$ | Frobenius norm, $\sqrt{\sum_{i,j} A_{ij}^2}$ |
: Summary of Notation {#tbl-notation}

## Trace and SPSD Matrices
The trace operator $\tr(\mA)$ is defined only over square matrices and has two equivalent definitions: either the sum of the diagonal elements or the sum of the eigenvalues $\lambda_1, \lambda_2, \ldots, \lambda_N$ of $\mA$,
$$
\tr(\mA) := \sum_{i=1}^N \mA_{ii} = \sum_{i=1}^N \lambda_i.
$$

### Trace as the sum of eigenvalues.
Recall that for a square matrix $\mA$, the eigenvalues are defined as the roots of the characteristic polynomial
$$
p(t) := \det(\mA - t\mI).
$$
The expansion of the determinant has $t^n$ and $t^{n-1}$ coefficients that will only arise from the products of diagonal elements. The first two terms then are
$$
p(t)= (-1)^n \left(t^n - \tr(\mA) t^{n-1} + \cdots + (-1)^n \det(\mA)\right),
$$
so the coefficient of $t^{n-1}$ is $-\tr(\mA)$. But the characteristic polynomial is also defined by its roots which are the eigenvalues, giving
$$
p(t) = \prod_{i=1}^N (\lambda_i - t) = (-1)^n \left( t^n - \left(\sum_{i=1}^N \lambda_i\right) t^{n-1} + \cdots + \prod_{i=1}^N \lambda_i \right).
$$
Since the two definitions are equivalent and valid for all $t$, the coefficients must be equal, and we have shown that $\tr(\mA) = \sum_{i=1}^N \lambda_i$.

### Symmetric Positive Semi-Definite Matrices
The analysis and algorithms ahead are restricted to symmetric positive semi-definite (SPSD) matrices, where $A = A^\top$ and all eigenvalues are non-negative ($\lambda_i \geq 0$), denoted $\mA \succeq 0$. This restriction is commonly imposed in the literature, as it simplifies the theoretical analysis and many of the aforementioned practical problems have SPSD matrices [@meyer2021HutchOptimalStochastic], while analysis by @avron2011RandomizedAlgorithmsEstimating also assumes this. An SPSD matrix has useful properties such as real-valued eigenvalues, orthogonal eigenvectors, a diagonalizable form which coincides with the singular value decomposition, strictly non-negative diagonal elements, and a non-negative quadratic form $\vv^\top \mA \vv \geq 0$ for all $\vv \in \mathbb{R}^n$.

### Matrix-Vector Product Oracle
To generalize the problem setting to both large explicit and implicit matrices, we assume only that there exists an *oracle* that can compute matrix-vector products (MVPs) $\mA \vz$ for any vector $\vz \in \mathbb{R}^N$. The oracle is assumed to operate at a fixed cost per query, such that the total computational cost of an algorithm is proportional to the number of queries made.

## Problem Statement
The goal is to construct a randomized algorithm for sampling $\mA \vv$ that will give an estimate of the trace $\tr(\mA)$ for an SPSD matrix $\mA \succeq 0$ with failure rate at most $\delta \in (0,1)$ and relative error tolerance $\epsilon \in (0,1)$.

::: {#def-estimator .definition}
## Randomized Trace Estimator

A randomized estimator $T$ is an $(\epsilon, \delta)$-approximation of $\tr(\mA)$ if:
$$
\Pr\Big[ | T - \tr(\mA) | \leq \epsilon \tr(\mA) \Big] \geq 1 - \delta,
$$
where $\epsilon \in (0, 1)$ is the error tolerance and $\delta \in (0, 1)$ is the failure probability.
:::

While many estimators exist, the challenge is to find an unbiased estimator $T$ that minimizes the number of MVP queries $m$ required for a given $(\epsilon, \delta)$-approximation.

## Prior Work and Complexity Bounds
@Hutchinson01011990 first proposed a randomized algorithm for an unbiased trace estimator in 1989 in the context of calculating Laplacian smoothing splines. Its advancements over prior work include the use of Rademacher random variables instead of Gaussian vector entries, and performing a variance analysis of the estimator. However, the analysis does not provide rigorous $(\epsilon, \delta)$-bounds for the estimator, focusing instead on variance alone. Later work by @avron2011RandomizedAlgorithmsEstimating in 2011 revisits Hutchinson's estimator and provides the first rigorous $(\epsilon, \delta)$-bounds for a variety of related estimators, including Hutchinson's. Their contributions include constructing methods that are oblivious to the basis of $\mA$ and proving rigorous $O(\epsilon^{-2} \ln(1/\delta))$ bounds for the number of samples required. More recent work by @meyer2021HutchOptimalStochastic improves these guarantees using Hutch++, achieving only \(O(\epsilon^{-1} \ln(1/\delta))\) matrix–vector products for a \((1\pm\epsilon)\) approximation by combining the exact trace estimate for a randomized low-rank approximation with a stochastic estimator for the residual. A summary of prior work is given in @tbl-prior-work.

| Algorithm | Query Complexity | Notes |
| :--- | :--- | :--- |
| Hutchinson [@Hutchinson01011990] | $O(1/\epsilon^{2})$ | Original analysis, only variance |
| Avron & Toledo [@avron2011RandomizedAlgorithmsEstimating] | $O(\ln(1/\delta)/\epsilon^2)$ | Bounds for many classes of random vectors $\vv$ |
| Hutch++ [@meyer2021HutchOptimalStochastic] | $O( \frac{1}{\epsilon} \ln(1/\delta))$ | Optimal complexity, preprocessing with low-rank approximation |
: Summary of Prior Work on Trace Estimation {#tbl-prior-work}

# The Hutchinson Estimator {#sec-hutchinson}

@Hutchinson01011990 proposes the Rademacher-based estimator $T_{H}$
$$
T_{H} := \frac{1}{m} \sum_{j=1}^m \vv_j^\top \mA \vv_j, \quad \vv_j \in \{\pm 1\}^N \text{ i.i.d. Rademacher random vectors,}
$$
with $m$ calls to the MVP oracle. This estimator is unbiased, i.e., $\mathbb{E}[T_H] = \tr(\mA)$.

#### Unbiased Estimators
Let $\vv$ be a random vector with i.i.d. entries $v_i \sim V$ such that $\mathbb{E}[v_i] = 0$ and $\mathbb{E}[v_i^2] = \sigma^2$. Then 
$$
\begin{align}
\mathbb{E}[\vv^\top \mA \vv]
&= \sigma^2 \tr(\mA), \\
\mathrm{Var}(\vv^\top \mA \vv)
&= 2 \sigma^4 \sum_{i \neq j} A_{ij}^2 + \left(\mathbb{E}\left[V^4\right] - \sigma^4\right) \sum_{i} A_{ii}^2.
\end{align}
$$

::: {.proof}
$$
\begin{align*}
\mathbb{E}[\vv^\top \mA \vv]
&= \sum_{i,j} A_{ij} \mathbb{E}[v_i v_j] \quad \text{(by linearity of expectation)} \\
&= \sum_{i} A_{ii} \mathbb{E}[v_i^2] + \sum_{i \neq j} A_{ij} \mathbb{E}[v_i] \mathbb{E}[v_j] \quad \text{(by independence)} \\
&= \sigma^2 \tr(\mA) \quad \text{(since $\mathbb{E}[v_i] = 0$)}.
\end{align*}
$$

For the case of variance, we again write out the summation form of expectation with four cases considered:
$$
\begin{align*}
\mathbb{E}[(\vv^\top \mA \vv)^2] &= \mathbb{E}\left[\left(\sum_{i,j} A_{ij} v_i v_j\right) \left(\sum_{k,l} A_{kl} v_k v_l\right)\right] \\
&= \sum_{i,j,k,l} A_{ij} A_{kl} \mathbb{E}[v_i v_j v_k v_l] \\
&= \underbrace{0}_{\text{one distinct }i,j,k,l} + \underbrace{\sigma^4 \sum_{i \neq k} A_{ii}A_{kk}}_{i=j, k=l, i\neq k} + \underbrace{\sigma^4 \sum_{i \neq j}  A_{ij}^2}_{i=k, j=l, i\neq j} + \underbrace{\sigma^4 \sum_{i \neq j}  A_{ij}^2}_{i=l, j=k, i\neq j} + \underbrace{\mathbb{E}\left[V^4 \right] \sum_{i} A_{ii}^2}_{i=j=k=l} \\
\Var(\vv^\top \mA \vv) &= \mathbb{E}[(\vv^\top \mA \vv)^2] - \sigma^4 \sum_{i,j}A_{ii}A_{jj} \\
&= 2 \sigma^4 \sum_{i \neq j} A_{ij}^2 + \left(\mathbb{E}\left[V^4\right] - \sigma^4\right) \sum_{i} A_{ii}^2
\end{align*}
$$
:::

When at least one index is distinct, the expectation is zero by independence and zero mean. When indices are paired, we get contributions of $\sigma^2$ for each unique double pair. When all four indices are equal, we get a contribution of $\mathbb{E}[V^4]$. The variance is then found by subtracting the square of the mean. 

Any random vector with the above properties will then function as an unbiased estimator when $\Var(V)=\sigma^2=1$ and will have minimum variance when $\mathbb{E}[V^4] - \sigma^4$ is minimized. The Rademacher random variable satisfies $\mathbb{E}[V] = 0$, $\Var(V) = 1$, and minimizes $\mathbb{E}[V^4] - \sigma^4$. In fact, by Jensen's inequality for any $V$, $\mathbb{E}[V^4] \geq (\mathbb{E}[V^2])^2 = \sigma^4$, such that the minimum is achieved when $\mathbb{E}[V^4] = \sigma^4$. Thus, the Rademacher distribution gives the minimum variance unbiased estimator for the trace using this framework of MVPs with i.i.d. entries. By extension, we expect this to translate to the best possible $\epsilon, \delta$ bounds when applying concentration inequalities, as is seen below in Section~\ref{sec:avron}.

Note that because $\vv^\top \mA \vv$ is an unbiased estimator, the average of $m$ independent samples is also unbiased with variance reduced by a factor of $m$. This is because for independent random variables $X_1, X_2, \ldots, X_m$, we have $\Var\left(\frac{1}{m} \sum_{i=1}^m X_i\right) = \frac{1}{m^2} \sum_{i=1}^m \Var(X_i) = \frac{1}{m} \Var(X_1)$.

Let $\vv_1, \ldots, \vv_m$ be i.i.d. Rademacher random vectors. Then the Hutchinson estimator $T_H$ is unbiased with variance
$$
\mathbb{E}[T_H] = \tr(\mA), \quad \Var(T_H) = \frac{1}{m} \left( 2 \sum_{i \neq j} A_{ij}^2 \right).
$$
This is the minimum variance unbiased estimator using i.i.d. entries with a mean of zero for the random vectors $\vv_j$, *for a fixed coordinate system*.

#### Variance Analysis
The variance only depends on the off-diagonal elements of $\mA$, so in the special case of a diagonal matrix, the variance is zero. In the context of SPSD matrices, this would amount to the spectral basis being aligned with the standard basis. This estimator clearly performs best when the diagonal elements dominate the off-diagonal elements, or equivalently when the eigenvectors are closely aligned with the standard basis. This poses the question: are there better choices of random vectors that are oblivious to the basis of $\mA$? If the eigenbasis of $\mA$ is known, then one could trivially perform a change of basis on $\vv$ to align it with the diagonalization of $\mA$.

Rademacher vectors are sensitive to the basis in which $\mA$ is represented, while the Gaussian version of the estimator is rotationally invariant, however at the cost of strictly higher variance. Given i.i.d. standard Gaussian vector entries, the fourth moment is $\mathbb{E}[G^4] = 3$, giving the Gaussian trace estimator
$$
\Var(T_G) =  \frac{2}{m} \left( \sum_{i \neq j} A_{ij}^2 + \sum_{i} A_{ii}^2 \right)= \frac{2}{m} \|\mA\|_F^2.
$$
However, for matrix $\mA$ there are no guarantees on how well aligned the eigenbasis is to the standard basis, and so the variance for Rademacher may approach that of Gaussian in the worst case. In both cases, the variance in the worst case scales with the squared Frobenius norm $\|\mA\|_F^2$ which is equal to the sum of squared eigenvalues $\sum_{i} \lambda_i^2$.

## Limitations of the Original Analysis

While Hutchinson's derivation proves unbiasedness and minimal variance among independent vector distributions, the rest of the analysis is focused on the Laplacian smoothing splines application. It is interesting to note the large number of citations that the work has received since its publication, largely due to the variance analysis of the generalized estimator and not for the specific application.

The focus purely on variance is limiting, as it does not translate directly to optimal $(\epsilon, \delta)$ bounds. The extension of the analysis to $(\epsilon, \delta)$-approximations that later follows is presented in the next section. As @avron2011RandomizedAlgorithmsEstimating point out, for the simple case where $\mA$ is a SPSD matrix of all $1$'s, the trace is $N$ but the variance is $O(N^2)$. This precludes the use of Chebyshev's inequality as a concentration bound, since for a single sample $m=1$, we have $\Pr[ |T_G-\tr \mA| \geq \epsilon \tr\mA ] \leq \frac{2}{\epsilon^2} = \delta$, which is trivially $\delta > 1$ for any $\epsilon < 1$. This motivates the need for a more robust analysis that can provide meaningful bounds for all SPSD matrices.

# Theoretical Analysis of $(\epsilon, \delta)$-Estimators {#sec-avron}

@avron2011RandomizedAlgorithmsEstimating revisit the Hutchinson estimator in 2011 to provide the first instance of rigorous $(\epsilon, \delta)$-guarantees as formulated in @def-estimator. Their work generalizes the analysis to a broader class of probing vectors than solely Rademacher or Gaussian and addresses the issue of basis sensitivity with randomized mixing matrices.

::: {#def-avron-estimators}
## Avron and Toledo Estimators
The three main estimators analyzed by Avron and Toledo are:
$$
\begin{align*}
    T_{GM} &= \frac{1}{m} \sum_{j=1}^m \vv_j^\top \mA \vv_j, \quad \vv_j \in \{\vv \mid \text{ i.i.d. entries }\sim \mathcal{N}(0,1)\}, \\
    T_{RM} &= \frac{1}{m} \sum_{j=1}^m \vv_j^\top \mA \vv_j, \quad \vv_j \in \{\vv \mid \|\vv\|=N, \ \mathbb{E}\vv^\top \mA \vv=\tr \mA\}, \\
    T_{HM} &= \frac{1}{m} \sum_{j=1}^m \vv_j^\top \mA \vv_j, \quad \vv_j \in \{\vv \mid \text{ i.i.d. entries } \sim \{-1,1\} \}, \\
    T_{UM} &= \frac{N}{m} \sum_{j=1}^m \vv_j^\top \mA \vv_j, \quad \vv_j \in \{\ve_1, \ve_2, \ldots, \ve_N\}.
\end{align*}
$$
:::


The normalized Rayleigh-quotient estimator $T_{RM}$ and the unit vector estimator $T_{UM}$ are new forms not previously discussed by Hutchinson. The advantage of the newly proposed estimators is that they require fewer random bits to generate—$O(\log n)$ versus $\Omega(n)$ for the others—as their sample spaces are smaller. To address the basis sensitivity posed by the sparse sampling regime of $T_{UM}$, the authors use a unitary random mixing matrix $\mathcal{F}=\mF \mD$ based on the Fast Johnson-Lindenstrauss (FJL) Transform [@ailonChazelle2006ApproximateNearest].

::: {#def-mixing-matrix}
## Randomized Mixing Matrix
A randomized mixing matrix $\mathcal{F} = \mF \mD$ is composed of a fixed unitary matrix $\mF$ (e.g., Discrete Fourier, Discrete Cosine, or Walsh-Hadamard matrix) and a random diagonal matrix $\mD$ with i.i.d. Rademacher entries.
:::

While there are subtleties to the cost of forming vectors $\vv_j$ in practice, the MVP computation is typically the dominant cost, so Fourier-type transforms are suggested by Avron and Toledo, in contrast to the Walsh-Hadamard used in the analysis of FJL which requires $N$ to be padded to a power of two [@Harvey2021Randomized2].

A summary and comparison of the estimators and their properties as derived by @avron2011RandomizedAlgorithmsEstimating is given in @tbl-estimator-comparison. The basis-oblivious estimators $T_{GM}$ and $T_{UM}$ (mixed) have bounds that do not depend on the structure of $\mA$, while the others do. The mixed unit vector, however, does depend on the dimension $n$. Note that the variance for $T_{RM}$ and $T_{UM}$ (mixed) are not given. Notice that despite worse variance, $T_{GM}$ has a better $(\epsilon,\delta)$ bound. All of the results are of order $\epsilon^{-2} \ln(1/\delta)$, indicating a large similarity in their performance and an unfortunate limitation in the error bound. To halve the error we require four times as many samples; however, this result is later improved upon by Hutch++ in Section~\ref{sec:hutchplusplus}.

| Estimator | Variance | Bound on \# samples for $(\varepsilon,\delta)$-approx | Random bits |
| :--- | :--- | :--- | :--- |
| **$T_{GM}$ (Gaussian)** | $2\lVert A\rVert_F^2$ | $20\varepsilon^{-2}\ln(2/\delta)$ | $\Theta(n)$ |
| **$T_{RM}$ (Rayleigh-Quotient)** | -- | $\tfrac12\,\varepsilon^{-2}n^{-2}\operatorname{rank}^2(\mA)\ln(2/\delta)\kappa_f^2(\mA)$ | $\Theta(n)$ |
| **$T_{HM}$ (Rademacher)** | $2\big(\lVert A\rVert_F^2-\sum_{i=1}^n A_{ii}^2\big)$ | $6 \epsilon^{-2}\ln\left(2 \operatorname{rank}(\mA)/\delta\right)$ | $\Theta(\log n)$ |
| **$T_{UM}$ (Unit Vector)** | $n\sum_{i=1}^n A_{ii}^2-\tr^2(\mA)$ | $\tfrac12\,\varepsilon^{-2}\ln(2/\delta)\,n\,\dfrac{\max_i A_{ii}}{\tr(\mA)}$ | $\Theta(\log n)$ |
| **$T_{UM}$ (With Mixing $\mathcal{F}$)** | -- | $8\varepsilon^{-2}\ln(4n^2/\delta)\ln(4/\delta)$ | $\Theta(\log n)$ |
: Comparison of estimators, variances, sample complexity, and random bit usage. {#tbl-estimator-comparison}

## Gaussian Estimator $T_{GM}$ Example
For brevity, only one of the estimator derivations is given in detail, as the full proofs are available in @avron2011RandomizedAlgorithmsEstimating.
Proceeding from the variance in Equation~\ref{eq:gaussian_variance}, Avron and Toledo take the typical path of applying a Chernoff-style argument. The Gaussian distribution is invariant under rotation, as shown by @Harvey2021Randomized2 [Section 24.4], so we can diagonalize $\mA = \mU \Lambda \mU^\top$ and consider $\vw_j = \mU^\top \vv_j$ as a Gaussian vector, without loss of generality. Then the estimator becomes:

$$
T_{GM} =\vv^\top \mA \vv = \frac{1}{m}\sum_j^{m}\sum_{i=1}^N \lambda_i w_{ij}^2 = \frac{1}{m}\sum_{i=1}^N \lambda_i \sum_{j=1}^m w_{ij}^2,
$$

where $w_{ij} \sim \mathcal{N}(0,1)$, and so each $w_{ij}^2$ is a $\chi^2$ random variable with one degree of freedom. The sum of $m$ i.i.d. $\chi^2$ random variables is a $\chi^2$ random variable with $m$ degrees of freedom, $\sum_j^m w_{ij}^2 \sim \chi^2_m$.

The tail bounds for the $\chi^2$ distribution are well studied. Using the following lemma we can derive a weak $(\epsilon, \delta)$ bound for $T_{GM}$, after which a stronger bound by Chernoff-style analysis is given by Avron and Toledo.

### Weak $(\epsilon, \delta)$ Bound for $T_{GM}$

::: {#lem-chi-squared-tail}
## Chi-Squared Tail Bound
Let $X \sim \chi^2_m$. Then for any $\epsilon \in (0,1)$,
$$
\Pr\left[|X - m| \geq \epsilon m\right] \leq 2 \exp\left(-\frac{m}{8}\epsilon^2\right).
$$
Source: @Harvey2021Randomized2 [Lemma 24.2.4]
:::


Let $X_i = \sum_{j=1}^m w_{ij}^2$, then $X_i \sim \chi^2_m$ with $\mathbb{E}[X_i] = m$. The estimator can be written as
$$
T_{GM} = \frac{1}{m} \sum_{i=1}^N \lambda_i X_i.
$$
@lem-chi-squared-tail gives $\epsilon$ failure probability in terms of $m$ such that $\delta \leq 2 \exp\left(-\frac{m}{8}\epsilon^2\right)$. Then a standard union bound over all $N$ non-zero eigenvalues gives $\delta_{\text{total}} \leq 2 \text{rank}(\mA) \exp\left(-\frac{m}{8}\epsilon^2\right)$. Solving for $m$ then gives the required number of samples for an $(\epsilon, \delta)$-approximation of
$$
m \geq \frac{8}{\epsilon^2} \ln\left(\frac{2 \operatorname{rank}(\mA)}{\delta}\right).
$$

To improve this bound further, Avron and Toledo apply a Chernoff-style argument directly to the weighted sum of $\chi^2$ random variables, giving the final bound in @tbl-estimator-comparison. A detailed proof using elementary symmetric polynomials, Markov's inequality, and moment generating functions for $\chi^2$ random variables is shown in Appendix \ref{app:stronger_bound}.

## Summary of Avron and Toledo Analysis
By rederiving the bounds in a Chernoff-style for particular matrix structures, even stronger bounds can be attained in the Gaussian case. For $T_{RM}$ the proof proceeds with a much simpler Hoeffding's inequality argument after bounding the output of the MVP. The original Hutchinson's $T_{HM}$ is bounded by diagonalizing $\mA$ and applying a lemma for bounding the oblivious Rademacher vectors that have been mixed by the arbitrary unitary matrix that is the basis of $\mA=\mU^\top \Lambda \mU$, then union bounding over $\text{rank}(\mA)$, finally conjecturing a tighter bound without rank may exist. Comparing bounds directly between estimators is not always conclusive, as the authors themselves conjecture that they have not found the tightest bounds. The final estimator $T_{UM}$ is bounded similarly using a lemma for the mixed unit vectors.

Their work shows that each estimator requires a specialized treatment to achieve their best possible $(\epsilon, \delta)$ bounds, and that the basis sensitivity of estimators can be mitigated by the use of randomized mixing matrices. The $m = O(\epsilon^{-2})$ dependence on the number of samples greatly limits the practical use of the method for high-accuracy trace estimation, with the suggestion for example of using a $\epsilon=0.01$ estimate to seed iterative methods for higher accuracy. Reducing this dependence to $\epsilon^{-1}$ would make higher degrees of accuracy more feasible, motivating the development of Hutch++ in the next section. In many applications of trace estimation the eigenvalue spectrum is dominated by few large values, which contribute the majority of the variance, as is seen in Principal Component Analysis and the SVD decomposition. Given a method of isolating the bulk of the variance, a stochastic estimator can be improved, forming the motivation behind Hutch++ [@meyer2021HutchOptimalStochastic].