<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.53">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Simon Ghyselincks">
<meta name="dcterms.date" content="2026-01-18">
<meta name="description" content="Implicit matrix trace estimation using only matrix-vector products is a problem that commonly arises in many areas of scientific computing and machine learning. This work explores randomized algorithms including Hutchinson’s and Hutch++.">

<title>Trace Estimation for Implicit Matrices – Simon’s Personal Website</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Simon’s Personal Website</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../content/publications/index.html"> 
<span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../content/eosc555/index.html"> 
<span class="menu-text">EOSC 555</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../content/projects/RLUnicycle/introduction.html"> 
<span class="menu-text">Learning to Balance</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../content/about/biography.html"> 
<span class="menu-text">Bio</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#trace-estimation-applications" id="toc-trace-estimation-applications" class="nav-link" data-scroll-target="#trace-estimation-applications"><span class="header-section-number">1.1</span> Trace Estimation Applications</a></li>
  <li><a href="#project-contribution" id="toc-project-contribution" class="nav-link" data-scroll-target="#project-contribution"><span class="header-section-number">1.2</span> Project Contribution</a></li>
  </ul></li>
  <li><a href="#sec-background" id="toc-sec-background" class="nav-link" data-scroll-target="#sec-background"><span class="header-section-number">2</span> Background</a>
  <ul class="collapse">
  <li><a href="#notation-and-definitions" id="toc-notation-and-definitions" class="nav-link" data-scroll-target="#notation-and-definitions"><span class="header-section-number">2.1</span> Notation and Definitions</a></li>
  <li><a href="#trace-and-spsd-matrices" id="toc-trace-and-spsd-matrices" class="nav-link" data-scroll-target="#trace-and-spsd-matrices"><span class="header-section-number">2.2</span> Trace and SPSD Matrices</a>
  <ul class="collapse">
  <li><a href="#trace-as-the-sum-of-eigenvalues." id="toc-trace-as-the-sum-of-eigenvalues." class="nav-link" data-scroll-target="#trace-as-the-sum-of-eigenvalues."><span class="header-section-number">2.2.1</span> Trace as the sum of eigenvalues.</a></li>
  <li><a href="#symmetric-positive-semi-definite-matrices" id="toc-symmetric-positive-semi-definite-matrices" class="nav-link" data-scroll-target="#symmetric-positive-semi-definite-matrices"><span class="header-section-number">2.2.2</span> Symmetric Positive Semi-Definite Matrices</a></li>
  <li><a href="#matrix-vector-product-oracle" id="toc-matrix-vector-product-oracle" class="nav-link" data-scroll-target="#matrix-vector-product-oracle"><span class="header-section-number">2.2.3</span> Matrix-Vector Product Oracle</a></li>
  </ul></li>
  <li><a href="#problem-statement" id="toc-problem-statement" class="nav-link" data-scroll-target="#problem-statement"><span class="header-section-number">2.3</span> Problem Statement</a></li>
  <li><a href="#prior-work-and-complexity-bounds" id="toc-prior-work-and-complexity-bounds" class="nav-link" data-scroll-target="#prior-work-and-complexity-bounds"><span class="header-section-number">2.4</span> Prior Work and Complexity Bounds</a></li>
  </ul></li>
  <li><a href="#sec-hutchinson" id="toc-sec-hutchinson" class="nav-link" data-scroll-target="#sec-hutchinson"><span class="header-section-number">3</span> The Hutchinson Estimator</a>
  <ul class="collapse">
  <li><a href="#limitations-of-the-original-analysis" id="toc-limitations-of-the-original-analysis" class="nav-link" data-scroll-target="#limitations-of-the-original-analysis"><span class="header-section-number">3.1</span> Limitations of the Original Analysis</a></li>
  </ul></li>
  <li><a href="#sec-avron" id="toc-sec-avron" class="nav-link" data-scroll-target="#sec-avron"><span class="header-section-number">4</span> Theoretical Analysis of <span class="math inline">\((\epsilon, \delta)\)</span>-Estimators</a>
  <ul class="collapse">
  <li><a href="#gaussian-estimator-t_gm-example" id="toc-gaussian-estimator-t_gm-example" class="nav-link" data-scroll-target="#gaussian-estimator-t_gm-example"><span class="header-section-number">4.1</span> Gaussian Estimator <span class="math inline">\(T_{GM}\)</span> Example</a>
  <ul class="collapse">
  <li><a href="#weak-epsilon-delta-bound-for-t_gm" id="toc-weak-epsilon-delta-bound-for-t_gm" class="nav-link" data-scroll-target="#weak-epsilon-delta-bound-for-t_gm"><span class="header-section-number">4.1.1</span> Weak <span class="math inline">\((\epsilon, \delta)\)</span> Bound for <span class="math inline">\(T_{GM}\)</span></a></li>
  </ul></li>
  <li><a href="#summary-of-avron-and-toledo-analysis" id="toc-summary-of-avron-and-toledo-analysis" class="nav-link" data-scroll-target="#summary-of-avron-and-toledo-analysis"><span class="header-section-number">4.2</span> Summary of Avron and Toledo Analysis</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Trace Estimation for Implicit Matrices</h1>
<p class="subtitle lead">CPSC 536M Project</p>
  <div class="quarto-categories">
    <div class="quarto-category">Randomized Algorithms</div>
    <div class="quarto-category">Linear Algebra</div>
    <div class="quarto-category">Machine Learning</div>
  </div>
  </div>

<div>
  <div class="description">
    Implicit matrix trace estimation using only matrix-vector products is a problem that commonly arises in many areas of scientific computing and machine learning. This work explores randomized algorithms including Hutchinson’s and Hutch++.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Simon Ghyselincks </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 18, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p><span class="math display">\[
\newcommand{\mA}{\mathbf{A}}
\newcommand{\mI}{\mathbf{I}}
\newcommand{\mH}{\mathbf{H}}
\newcommand{\mD}{\mathbf{D}}
\newcommand{\mF}{\mathbf{F}}
\newcommand{\mQ}{\mathbf{Q}}
\newcommand{\mS}{\mathbf{S}}
\newcommand{\mG}{\mathbf{G}}
\newcommand{\mU}{\mathbf{U}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\va}{\mathbf{a}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Pr}{\mathbb{P}}
\newcommand{\mathbb}{\mathbf}
\]</span></p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Abstract
</div>
</div>
<div class="callout-body-container callout-body">
<p>Implicit matrix trace estimation using only matrix-vector products is a problem that commonly arises in many areas of scientific computing and machine learning. When the matrix is only accessible through matrix-vector products <span class="math inline">\(\mA \vv\)</span> or is excessively large, classical deterministic methods for computing the trace are infeasible. This work explores randomized algorithms for trace estimation, building upon Hutchinson’s estimator and its variants. The theoretical bounds, including <span class="math inline">\((\epsilon, \delta)\)</span>-approximations, are analyzed, and the performance of these methods is evaluated through numerical experiments on matrices with different spectral properties. A review of prior work and recent advancements in this area is also provided.</p>
</div>
</div>
<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Implicit matrix trace estimation for a matrix <span class="math inline">\(\mA\)</span> using only the matrix-vector product <span class="math inline">\(\mA \vv\)</span> over some set of <span class="math inline">\(\mA \vv_1,\mA \vv_2,...\mA \vv_m \in \mathbb{R}^N\)</span> is a problem that commonly arises in scientific computing and machine learning <span class="citation" data-cites="UbaruSaad2018">(<a href="#ref-UbaruSaad2018" role="doc-biblioref">Ubaru and Saad 2018</a>)</span>. When <span class="math inline">\(\mA\)</span> is only accessible through the map <span class="math inline">\(x \mapsto \mA x\)</span>, there is no direct access to its matrix elements, and we call this an implicit representation. Even in the case where <span class="math inline">\(\mA\)</span> is explicitly computable, it may be of an excessively large dimension, making it infeasible to store and compute the full matrix. Similarly, for <span class="math inline">\(\mA \in \mathbb{R}^{N \times N}\)</span>, although <span class="math inline">\(\tr(\mA)\)</span> is simply the sum of diagonal elements, it requires evaluating <span class="math inline">\(\sum_{i\in [N]} e_i^\top \mA e_i\)</span>, which for large <span class="math inline">\(n\)</span> may be computationally expensive, intractable, or unnecessary for the level of precision required.</p>
<p>Often these problems arise within an iterative process, where an estimation will suffice under the expectation that future algorithmic steps will further refine the problem solution <span class="citation" data-cites="woodruffOptimalQueryComplexities">(<a href="#ref-woodruffOptimalQueryComplexities" role="doc-biblioref">Woodruff, Zhang, and Zhang 2022</a>)</span>. This problem formulation is well suited to randomized numerical linear algebra (RNLA) techniques, and as discussed below, can even be effectively paired with other randomized algorithms such as low-rank matrix estimation for further efficiency gains <span class="citation" data-cites="meyer2021HutchOptimalStochastic">(<a href="#ref-meyer2021HutchOptimalStochastic" role="doc-biblioref">Meyer et al. 2021</a>)</span>.</p>
<p>The goal of trace <em>estimation</em> then is to find an approximation within some error tolerance such that the total required matrix-vector products scale sublinearly in <span class="math inline">\(N\)</span>, avoiding <span class="math inline">\(O(N)\)</span> complexity. A naive approach would be to randomly sample a subset of the diagonal elements of <span class="math inline">\(\mA\)</span> as an estimator, but the diagonal elements may be highly concentrated, resulting in the same variance issues posed by sparse sampling <span class="citation" data-cites="Harvey2021Randomized2">(see <a href="#ref-Harvey2021Randomized2" role="doc-biblioref">Harvey 2021</a>, Ch 24.3.1)</span>. Thus we must employ more sophisticated methods to gain better approximations at a cheaper cost, as measured by both computational complexity and the total matrix-vector products. This motivates the need for generalizable methods capable of probing the structure of <span class="math inline">\(\mA\)</span> using a limited number of matrix-vector products or other implicit representations.</p>
<section id="trace-estimation-applications" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="trace-estimation-applications"><span class="header-section-number">1.1</span> Trace Estimation Applications</h2>
<p>Examples of trace estimation problems are abundant <span class="citation" data-cites="UbaruSaad2018">(<a href="#ref-UbaruSaad2018" role="doc-biblioref">Ubaru and Saad 2018</a>)</span>. In deep learning, large and complex models often require the estimation of traces of Hessian matrices <span class="math inline">\(\mH\)</span> for optimization and uncertainty quantification. A common architecture such as ResNet-50 has a <span class="math inline">\(25 \text{ million} \times 25 \text{ million}\)</span> matrix <span class="math inline">\(\mH\)</span> that is prohibitive to store, but whose matrix-vector product <span class="math inline">\(\mH \vv\)</span> can be efficiently computed using Pearlmutter’s trick <span class="citation" data-cites="pearlmutter1994fast">(<a href="#ref-pearlmutter1994fast" role="doc-biblioref">Pearlmutter 1994</a>)</span>. High-dimensional trace estimation is also used for training Physics Informed Neural Networks (PINNs) <span class="citation" data-cites="HU2024116883">(<a href="#ref-HU2024116883" role="doc-biblioref">Hu et al. 2024</a>)</span>. In probabilistic machine learning, trace estimation is used in score-matching methods <span class="citation" data-cites="haber_lecture11_2025">(<a href="#ref-haber_lecture11_2025" role="doc-biblioref">Haber 2024</a>)</span>.</p>
</section>
<section id="project-contribution" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="project-contribution"><span class="header-section-number">1.2</span> Project Contribution</h2>
<p>This work explores and summarizes the progression of Randomized Numerical Linear Algebra (RNLA) methods for trace estimation, analyzes their theoretical properties, and evaluates their performance through numerical experiments. Specifically, we:</p>
<ol type="1">
<li>Review the foundational Hutchinson’s estimator <span class="citation" data-cites="Hutchinson01011990">(<a href="#ref-Hutchinson01011990" role="doc-biblioref">Hutchinson 1990</a>)</span>.</li>
<li>Analyze the <span class="math inline">\((\epsilon, \delta)\)</span> bounds derived by <span class="citation" data-cites="avron2011RandomizedAlgorithmsEstimating">Avron and Toledo (<a href="#ref-avron2011RandomizedAlgorithmsEstimating" role="doc-biblioref">2011</a>)</span>.</li>
<li>Implement and verify the Hutch++ algorithm <span class="citation" data-cites="meyer2021HutchOptimalStochastic">(<a href="#ref-meyer2021HutchOptimalStochastic" role="doc-biblioref">Meyer et al. 2021</a>)</span>, which combines low-rank approximation with stochastic estimation.</li>
<li>Compare these methods numerically on matrices with different spectral decay properties.</li>
</ol>
</section>
</section>
<section id="sec-background" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Background</h1>
<section id="notation-and-definitions" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="notation-and-definitions"><span class="header-section-number">2.1</span> Notation and Definitions</h2>
<p>In this work, lower case <span class="math inline">\(a\)</span> denotes a scalar, capital <span class="math inline">\(A\)</span> denotes a random variable, lower case bold <span class="math inline">\(\va\)</span> denotes a vector, and upper case bold <span class="math inline">\(\mA\)</span> denotes a matrix. A summary of the notation used is found below in <a href="#tbl-notation" class="quarto-xref">Table&nbsp;1</a>.</p>
<div id="tbl-notation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-notation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Summary of Notation
</figcaption>
<div aria-describedby="tbl-notation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Symbol</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\mA \in \mathbb{R}^{N \times N}\)</span></td>
<td style="text-align: left;">Symmetric Positive Semi-Definite (SPSD) matrix</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\tr(\mA)\)</span></td>
<td style="text-align: left;">Trace of matrix <span class="math inline">\(\mA\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\lambda_i\)</span></td>
<td style="text-align: left;">The <span class="math inline">\(i\)</span>-th eigenvalue of <span class="math inline">\(\mA\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\vv\)</span></td>
<td style="text-align: left;">Vector in <span class="math inline">\(\mathbb{R}^N\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(m\)</span></td>
<td style="text-align: left;">Number of Matrix-Vector Product (MVP) <span class="math inline">\(\mA \vv\)</span> queries</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\epsilon\)</span></td>
<td style="text-align: left;">Relative error tolerance</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\delta\)</span></td>
<td style="text-align: left;">Probability of failure</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\|\mA\|_F\)</span></td>
<td style="text-align: left;">Frobenius norm, <span class="math inline">\(\sqrt{\sum_{i,j} A_{ij}^2}\)</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="trace-and-spsd-matrices" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="trace-and-spsd-matrices"><span class="header-section-number">2.2</span> Trace and SPSD Matrices</h2>
<p>The trace operator <span class="math inline">\(\tr(\mA)\)</span> is defined only over square matrices and has two equivalent definitions: either the sum of the diagonal elements or the sum of the eigenvalues <span class="math inline">\(\lambda_1, \lambda_2, \ldots, \lambda_N\)</span> of <span class="math inline">\(\mA\)</span>, <span class="math display">\[
\tr(\mA) := \sum_{i=1}^N \mA_{ii} = \sum_{i=1}^N \lambda_i.
\]</span></p>
<section id="trace-as-the-sum-of-eigenvalues." class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="trace-as-the-sum-of-eigenvalues."><span class="header-section-number">2.2.1</span> Trace as the sum of eigenvalues.</h3>
<p>Recall that for a square matrix <span class="math inline">\(\mA\)</span>, the eigenvalues are defined as the roots of the characteristic polynomial <span class="math display">\[
p(t) := \det(\mA - t\mI).
\]</span> The expansion of the determinant has <span class="math inline">\(t^n\)</span> and <span class="math inline">\(t^{n-1}\)</span> coefficients that will only arise from the products of diagonal elements. The first two terms then are <span class="math display">\[
p(t)= (-1)^n \left(t^n - \tr(\mA) t^{n-1} + \cdots + (-1)^n \det(\mA)\right),
\]</span> so the coefficient of <span class="math inline">\(t^{n-1}\)</span> is <span class="math inline">\(-\tr(\mA)\)</span>. But the characteristic polynomial is also defined by its roots which are the eigenvalues, giving <span class="math display">\[
p(t) = \prod_{i=1}^N (\lambda_i - t) = (-1)^n \left( t^n - \left(\sum_{i=1}^N \lambda_i\right) t^{n-1} + \cdots + \prod_{i=1}^N \lambda_i \right).
\]</span> Since the two definitions are equivalent and valid for all <span class="math inline">\(t\)</span>, the coefficients must be equal, and we have shown that <span class="math inline">\(\tr(\mA) = \sum_{i=1}^N \lambda_i\)</span>.</p>
</section>
<section id="symmetric-positive-semi-definite-matrices" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="symmetric-positive-semi-definite-matrices"><span class="header-section-number">2.2.2</span> Symmetric Positive Semi-Definite Matrices</h3>
<p>The analysis and algorithms ahead are restricted to symmetric positive semi-definite (SPSD) matrices, where <span class="math inline">\(A = A^\top\)</span> and all eigenvalues are non-negative (<span class="math inline">\(\lambda_i \geq 0\)</span>), denoted <span class="math inline">\(\mA \succeq 0\)</span>. This restriction is commonly imposed in the literature, as it simplifies the theoretical analysis and many of the aforementioned practical problems have SPSD matrices <span class="citation" data-cites="meyer2021HutchOptimalStochastic">(<a href="#ref-meyer2021HutchOptimalStochastic" role="doc-biblioref">Meyer et al. 2021</a>)</span>, while analysis by <span class="citation" data-cites="avron2011RandomizedAlgorithmsEstimating">Avron and Toledo (<a href="#ref-avron2011RandomizedAlgorithmsEstimating" role="doc-biblioref">2011</a>)</span> also assumes this. An SPSD matrix has useful properties such as real-valued eigenvalues, orthogonal eigenvectors, a diagonalizable form which coincides with the singular value decomposition, strictly non-negative diagonal elements, and a non-negative quadratic form <span class="math inline">\(\vv^\top \mA \vv \geq 0\)</span> for all <span class="math inline">\(\vv \in \mathbb{R}^n\)</span>.</p>
</section>
<section id="matrix-vector-product-oracle" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="matrix-vector-product-oracle"><span class="header-section-number">2.2.3</span> Matrix-Vector Product Oracle</h3>
<p>To generalize the problem setting to both large explicit and implicit matrices, we assume only that there exists an <em>oracle</em> that can compute matrix-vector products (MVPs) <span class="math inline">\(\mA \vz\)</span> for any vector <span class="math inline">\(\vz \in \mathbb{R}^N\)</span>. The oracle is assumed to operate at a fixed cost per query, such that the total computational cost of an algorithm is proportional to the number of queries made.</p>
</section>
</section>
<section id="problem-statement" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="problem-statement"><span class="header-section-number">2.3</span> Problem Statement</h2>
<p>The goal is to construct a randomized algorithm for sampling <span class="math inline">\(\mA \vv\)</span> that will give an estimate of the trace <span class="math inline">\(\tr(\mA)\)</span> for an SPSD matrix <span class="math inline">\(\mA \succeq 0\)</span> with failure rate at most <span class="math inline">\(\delta \in (0,1)\)</span> and relative error tolerance <span class="math inline">\(\epsilon \in (0,1)\)</span>.</p>
<div id="def-estimator" class="definition theorem">
<p><span class="theorem-title"><strong>Definition 1 (Randomized Trace Estimator)</strong></span> A randomized estimator <span class="math inline">\(T\)</span> is an <span class="math inline">\((\epsilon, \delta)\)</span>-approximation of <span class="math inline">\(\tr(\mA)\)</span> if: <span class="math display">\[
\Pr\Big[ | T - \tr(\mA) | \leq \epsilon \tr(\mA) \Big] \geq 1 - \delta,
\]</span> where <span class="math inline">\(\epsilon \in (0, 1)\)</span> is the error tolerance and <span class="math inline">\(\delta \in (0, 1)\)</span> is the failure probability.</p>
</div>
<p>While many estimators exist, the challenge is to find an unbiased estimator <span class="math inline">\(T\)</span> that minimizes the number of MVP queries <span class="math inline">\(m\)</span> required for a given <span class="math inline">\((\epsilon, \delta)\)</span>-approximation.</p>
</section>
<section id="prior-work-and-complexity-bounds" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="prior-work-and-complexity-bounds"><span class="header-section-number">2.4</span> Prior Work and Complexity Bounds</h2>
<p><span class="citation" data-cites="Hutchinson01011990">Hutchinson (<a href="#ref-Hutchinson01011990" role="doc-biblioref">1990</a>)</span> first proposed a randomized algorithm for an unbiased trace estimator in 1989 in the context of calculating Laplacian smoothing splines. Its advancements over prior work include the use of Rademacher random variables instead of Gaussian vector entries, and performing a variance analysis of the estimator. However, the analysis does not provide rigorous <span class="math inline">\((\epsilon, \delta)\)</span>-bounds for the estimator, focusing instead on variance alone. Later work by <span class="citation" data-cites="avron2011RandomizedAlgorithmsEstimating">Avron and Toledo (<a href="#ref-avron2011RandomizedAlgorithmsEstimating" role="doc-biblioref">2011</a>)</span> in 2011 revisits Hutchinson’s estimator and provides the first rigorous <span class="math inline">\((\epsilon, \delta)\)</span>-bounds for a variety of related estimators, including Hutchinson’s. Their contributions include constructing methods that are oblivious to the basis of <span class="math inline">\(\mA\)</span> and proving rigorous <span class="math inline">\(O(\epsilon^{-2} \ln(1/\delta))\)</span> bounds for the number of samples required. More recent work by <span class="citation" data-cites="meyer2021HutchOptimalStochastic">Meyer et al. (<a href="#ref-meyer2021HutchOptimalStochastic" role="doc-biblioref">2021</a>)</span> improves these guarantees using Hutch++, achieving only (O(^{-1} (1/))) matrix–vector products for a ((1)) approximation by combining the exact trace estimate for a randomized low-rank approximation with a stochastic estimator for the residual. A summary of prior work is given in <a href="#tbl-prior-work" class="quarto-xref">Table&nbsp;2</a>.</p>
<div id="tbl-prior-work" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-prior-work-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Summary of Prior Work on Trace Estimation
</figcaption>
<div aria-describedby="tbl-prior-work-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Algorithm</th>
<th style="text-align: left;">Query Complexity</th>
<th style="text-align: left;">Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Hutchinson <span class="citation" data-cites="Hutchinson01011990">(<a href="#ref-Hutchinson01011990" role="doc-biblioref">Hutchinson 1990</a>)</span></td>
<td style="text-align: left;"><span class="math inline">\(O(1/\epsilon^{2})\)</span></td>
<td style="text-align: left;">Original analysis, only variance</td>
</tr>
<tr class="even">
<td style="text-align: left;">Avron &amp; Toledo <span class="citation" data-cites="avron2011RandomizedAlgorithmsEstimating">(<a href="#ref-avron2011RandomizedAlgorithmsEstimating" role="doc-biblioref">Avron and Toledo 2011</a>)</span></td>
<td style="text-align: left;"><span class="math inline">\(O(\ln(1/\delta)/\epsilon^2)\)</span></td>
<td style="text-align: left;">Bounds for many classes of random vectors <span class="math inline">\(\vv\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Hutch++ <span class="citation" data-cites="meyer2021HutchOptimalStochastic">(<a href="#ref-meyer2021HutchOptimalStochastic" role="doc-biblioref">Meyer et al. 2021</a>)</span></td>
<td style="text-align: left;"><span class="math inline">\(O( \frac{1}{\epsilon} \ln(1/\delta))\)</span></td>
<td style="text-align: left;">Optimal complexity, preprocessing with low-rank approximation</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
</section>
<section id="sec-hutchinson" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> The Hutchinson Estimator</h1>
<p><span class="citation" data-cites="Hutchinson01011990">Hutchinson (<a href="#ref-Hutchinson01011990" role="doc-biblioref">1990</a>)</span> proposes the Rademacher-based estimator <span class="math inline">\(T_{H}\)</span> <span class="math display">\[
T_{H} := \frac{1}{m} \sum_{j=1}^m \vv_j^\top \mA \vv_j, \quad \vv_j \in \{\pm 1\}^N \text{ i.i.d. Rademacher random vectors,}
\]</span> with <span class="math inline">\(m\)</span> calls to the MVP oracle. This estimator is unbiased, i.e., <span class="math inline">\(\mathbb{E}[T_H] = \tr(\mA)\)</span>.</p>
<section id="unbiased-estimators" class="level4" data-number="3.0.0.1">
<h4 data-number="3.0.0.1" class="anchored" data-anchor-id="unbiased-estimators"><span class="header-section-number">3.0.0.1</span> Unbiased Estimators</h4>
<p>Let <span class="math inline">\(\vv\)</span> be a random vector with i.i.d. entries <span class="math inline">\(v_i \sim V\)</span> such that <span class="math inline">\(\mathbb{E}[v_i] = 0\)</span> and <span class="math inline">\(\mathbb{E}[v_i^2] = \sigma^2\)</span>. Then <span class="math display">\[
\begin{align}
\mathbb{E}[\vv^\top \mA \vv]
&amp;= \sigma^2 \tr(\mA), \\
\mathrm{Var}(\vv^\top \mA \vv)
&amp;= 2 \sigma^4 \sum_{i \neq j} A_{ij}^2 + \left(\mathbb{E}\left[V^4\right] - \sigma^4\right) \sum_{i} A_{ii}^2.
\end{align}
\]</span></p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{align*}
\mathbb{E}[\vv^\top \mA \vv]
&amp;= \sum_{i,j} A_{ij} \mathbb{E}[v_i v_j] \quad \text{(by linearity of expectation)} \\
&amp;= \sum_{i} A_{ii} \mathbb{E}[v_i^2] + \sum_{i \neq j} A_{ij} \mathbb{E}[v_i] \mathbb{E}[v_j] \quad \text{(by independence)} \\
&amp;= \sigma^2 \tr(\mA) \quad \text{(since $\mathbb{E}[v_i] = 0$)}.
\end{align*}
\]</span></p>
<p>For the case of variance, we again write out the summation form of expectation with four cases considered: <span class="math display">\[
\begin{align*}
\mathbb{E}[(\vv^\top \mA \vv)^2] &amp;= \mathbb{E}\left[\left(\sum_{i,j} A_{ij} v_i v_j\right) \left(\sum_{k,l} A_{kl} v_k v_l\right)\right] \\
&amp;= \sum_{i,j,k,l} A_{ij} A_{kl} \mathbb{E}[v_i v_j v_k v_l] \\
&amp;= \underbrace{0}_{\text{one distinct }i,j,k,l} + \underbrace{\sigma^4 \sum_{i \neq k} A_{ii}A_{kk}}_{i=j, k=l, i\neq k} + \underbrace{\sigma^4 \sum_{i \neq j}  A_{ij}^2}_{i=k, j=l, i\neq j} + \underbrace{\sigma^4 \sum_{i \neq j}  A_{ij}^2}_{i=l, j=k, i\neq j} + \underbrace{\mathbb{E}\left[V^4 \right] \sum_{i} A_{ii}^2}_{i=j=k=l} \\
\Var(\vv^\top \mA \vv) &amp;= \mathbb{E}[(\vv^\top \mA \vv)^2] - \sigma^4 \sum_{i,j}A_{ii}A_{jj} \\
&amp;= 2 \sigma^4 \sum_{i \neq j} A_{ij}^2 + \left(\mathbb{E}\left[V^4\right] - \sigma^4\right) \sum_{i} A_{ii}^2
\end{align*}
\]</span></p>
</div>
<p>When at least one index is distinct, the expectation is zero by independence and zero mean. When indices are paired, we get contributions of <span class="math inline">\(\sigma^2\)</span> for each unique double pair. When all four indices are equal, we get a contribution of <span class="math inline">\(\mathbb{E}[V^4]\)</span>. The variance is then found by subtracting the square of the mean.</p>
<p>Any random vector with the above properties will then function as an unbiased estimator when <span class="math inline">\(\Var(V)=\sigma^2=1\)</span> and will have minimum variance when <span class="math inline">\(\mathbb{E}[V^4] - \sigma^4\)</span> is minimized. The Rademacher random variable satisfies <span class="math inline">\(\mathbb{E}[V] = 0\)</span>, <span class="math inline">\(\Var(V) = 1\)</span>, and minimizes <span class="math inline">\(\mathbb{E}[V^4] - \sigma^4\)</span>. In fact, by Jensen’s inequality for any <span class="math inline">\(V\)</span>, <span class="math inline">\(\mathbb{E}[V^4] \geq (\mathbb{E}[V^2])^2 = \sigma^4\)</span>, such that the minimum is achieved when <span class="math inline">\(\mathbb{E}[V^4] = \sigma^4\)</span>. Thus, the Rademacher distribution gives the minimum variance unbiased estimator for the trace using this framework of MVPs with i.i.d. entries. By extension, we expect this to translate to the best possible <span class="math inline">\(\epsilon, \delta\)</span> bounds when applying concentration inequalities, as is seen below in Section~<span class="math inline">\(\ref{sec:avron}\)</span>.</p>
<p>Note that because <span class="math inline">\(\vv^\top \mA \vv\)</span> is an unbiased estimator, the average of <span class="math inline">\(m\)</span> independent samples is also unbiased with variance reduced by a factor of <span class="math inline">\(m\)</span>. This is because for independent random variables <span class="math inline">\(X_1, X_2, \ldots, X_m\)</span>, we have <span class="math inline">\(\Var\left(\frac{1}{m} \sum_{i=1}^m X_i\right) = \frac{1}{m^2} \sum_{i=1}^m \Var(X_i) = \frac{1}{m} \Var(X_1)\)</span>.</p>
<p>Let <span class="math inline">\(\vv_1, \ldots, \vv_m\)</span> be i.i.d. Rademacher random vectors. Then the Hutchinson estimator <span class="math inline">\(T_H\)</span> is unbiased with variance <span class="math display">\[
\mathbb{E}[T_H] = \tr(\mA), \quad \Var(T_H) = \frac{1}{m} \left( 2 \sum_{i \neq j} A_{ij}^2 \right).
\]</span> This is the minimum variance unbiased estimator using i.i.d. entries with a mean of zero for the random vectors <span class="math inline">\(\vv_j\)</span>, <em>for a fixed coordinate system</em>.</p>
</section>
<section id="variance-analysis" class="level4" data-number="3.0.0.2">
<h4 data-number="3.0.0.2" class="anchored" data-anchor-id="variance-analysis"><span class="header-section-number">3.0.0.2</span> Variance Analysis</h4>
<p>The variance only depends on the off-diagonal elements of <span class="math inline">\(\mA\)</span>, so in the special case of a diagonal matrix, the variance is zero. In the context of SPSD matrices, this would amount to the spectral basis being aligned with the standard basis. This estimator clearly performs best when the diagonal elements dominate the off-diagonal elements, or equivalently when the eigenvectors are closely aligned with the standard basis. This poses the question: are there better choices of random vectors that are oblivious to the basis of <span class="math inline">\(\mA\)</span>? If the eigenbasis of <span class="math inline">\(\mA\)</span> is known, then one could trivially perform a change of basis on <span class="math inline">\(\vv\)</span> to align it with the diagonalization of <span class="math inline">\(\mA\)</span>.</p>
<p>Rademacher vectors are sensitive to the basis in which <span class="math inline">\(\mA\)</span> is represented, while the Gaussian version of the estimator is rotationally invariant, however at the cost of strictly higher variance. Given i.i.d. standard Gaussian vector entries, the fourth moment is <span class="math inline">\(\mathbb{E}[G^4] = 3\)</span>, giving the Gaussian trace estimator <span class="math display">\[
\Var(T_G) =  \frac{2}{m} \left( \sum_{i \neq j} A_{ij}^2 + \sum_{i} A_{ii}^2 \right)= \frac{2}{m} \|\mA\|_F^2.
\]</span> However, for matrix <span class="math inline">\(\mA\)</span> there are no guarantees on how well aligned the eigenbasis is to the standard basis, and so the variance for Rademacher may approach that of Gaussian in the worst case. In both cases, the variance in the worst case scales with the squared Frobenius norm <span class="math inline">\(\|\mA\|_F^2\)</span> which is equal to the sum of squared eigenvalues <span class="math inline">\(\sum_{i} \lambda_i^2\)</span>.</p>
</section>
<section id="limitations-of-the-original-analysis" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="limitations-of-the-original-analysis"><span class="header-section-number">3.1</span> Limitations of the Original Analysis</h2>
<p>While Hutchinson’s derivation proves unbiasedness and minimal variance among independent vector distributions, the rest of the analysis is focused on the Laplacian smoothing splines application. It is interesting to note the large number of citations that the work has received since its publication, largely due to the variance analysis of the generalized estimator and not for the specific application.</p>
<p>The focus purely on variance is limiting, as it does not translate directly to optimal <span class="math inline">\((\epsilon, \delta)\)</span> bounds. The extension of the analysis to <span class="math inline">\((\epsilon, \delta)\)</span>-approximations that later follows is presented in the next section. As <span class="citation" data-cites="avron2011RandomizedAlgorithmsEstimating">Avron and Toledo (<a href="#ref-avron2011RandomizedAlgorithmsEstimating" role="doc-biblioref">2011</a>)</span> point out, for the simple case where <span class="math inline">\(\mA\)</span> is a SPSD matrix of all <span class="math inline">\(1\)</span>’s, the trace is <span class="math inline">\(N\)</span> but the variance is <span class="math inline">\(O(N^2)\)</span>. This precludes the use of Chebyshev’s inequality as a concentration bound, since for a single sample <span class="math inline">\(m=1\)</span>, we have <span class="math inline">\(\Pr[ |T_G-\tr \mA| \geq \epsilon \tr\mA ] \leq \frac{2}{\epsilon^2} = \delta\)</span>, which is trivially <span class="math inline">\(\delta &gt; 1\)</span> for any <span class="math inline">\(\epsilon &lt; 1\)</span>. This motivates the need for a more robust analysis that can provide meaningful bounds for all SPSD matrices.</p>
</section>
</section>
<section id="sec-avron" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Theoretical Analysis of <span class="math inline">\((\epsilon, \delta)\)</span>-Estimators</h1>
<p><span class="citation" data-cites="avron2011RandomizedAlgorithmsEstimating">Avron and Toledo (<a href="#ref-avron2011RandomizedAlgorithmsEstimating" role="doc-biblioref">2011</a>)</span> revisit the Hutchinson estimator in 2011 to provide the first instance of rigorous <span class="math inline">\((\epsilon, \delta)\)</span>-guarantees as formulated in <a href="#def-estimator" class="quarto-xref">Definition&nbsp;1</a>. Their work generalizes the analysis to a broader class of probing vectors than solely Rademacher or Gaussian and addresses the issue of basis sensitivity with randomized mixing matrices.</p>
<div id="def-avron-estimators" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 (Avron and Toledo Estimators)</strong></span> The three main estimators analyzed by Avron and Toledo are: <span class="math display">\[
\begin{align*}
    T_{GM} &amp;= \frac{1}{m} \sum_{j=1}^m \vv_j^\top \mA \vv_j, \quad \vv_j \in \{\vv \mid \text{ i.i.d. entries }\sim \mathcal{N}(0,1)\}, \\
    T_{RM} &amp;= \frac{1}{m} \sum_{j=1}^m \vv_j^\top \mA \vv_j, \quad \vv_j \in \{\vv \mid \|\vv\|=N, \ \mathbb{E}\vv^\top \mA \vv=\tr \mA\}, \\
    T_{HM} &amp;= \frac{1}{m} \sum_{j=1}^m \vv_j^\top \mA \vv_j, \quad \vv_j \in \{\vv \mid \text{ i.i.d. entries } \sim \{-1,1\} \}, \\
    T_{UM} &amp;= \frac{N}{m} \sum_{j=1}^m \vv_j^\top \mA \vv_j, \quad \vv_j \in \{\ve_1, \ve_2, \ldots, \ve_N\}.
\end{align*}
\]</span></p>
</div>
<p>The normalized Rayleigh-quotient estimator <span class="math inline">\(T_{RM}\)</span> and the unit vector estimator <span class="math inline">\(T_{UM}\)</span> are new forms not previously discussed by Hutchinson. The advantage of the newly proposed estimators is that they require fewer random bits to generate—<span class="math inline">\(O(\log n)\)</span> versus <span class="math inline">\(\Omega(n)\)</span> for the others—as their sample spaces are smaller. To address the basis sensitivity posed by the sparse sampling regime of <span class="math inline">\(T_{UM}\)</span>, the authors use a unitary random mixing matrix <span class="math inline">\(\mathcal{F}=\mF \mD\)</span> based on the Fast Johnson-Lindenstrauss (FJL) Transform <span class="citation" data-cites="ailonChazelle2006ApproximateNearest">(<a href="#ref-ailonChazelle2006ApproximateNearest" role="doc-biblioref">Ailon and Chazelle 2006</a>)</span>.</p>
<div id="def-mixing-matrix" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3 (Randomized Mixing Matrix)</strong></span> A randomized mixing matrix <span class="math inline">\(\mathcal{F} = \mF \mD\)</span> is composed of a fixed unitary matrix <span class="math inline">\(\mF\)</span> (e.g., Discrete Fourier, Discrete Cosine, or Walsh-Hadamard matrix) and a random diagonal matrix <span class="math inline">\(\mD\)</span> with i.i.d. Rademacher entries.</p>
</div>
<p>While there are subtleties to the cost of forming vectors <span class="math inline">\(\vv_j\)</span> in practice, the MVP computation is typically the dominant cost, so Fourier-type transforms are suggested by Avron and Toledo, in contrast to the Walsh-Hadamard used in the analysis of FJL which requires <span class="math inline">\(N\)</span> to be padded to a power of two <span class="citation" data-cites="Harvey2021Randomized2">(<a href="#ref-Harvey2021Randomized2" role="doc-biblioref">Harvey 2021</a>)</span>.</p>
<p>A summary and comparison of the estimators and their properties as derived by <span class="citation" data-cites="avron2011RandomizedAlgorithmsEstimating">Avron and Toledo (<a href="#ref-avron2011RandomizedAlgorithmsEstimating" role="doc-biblioref">2011</a>)</span> is given in <a href="#tbl-estimator-comparison" class="quarto-xref">Table&nbsp;3</a>. The basis-oblivious estimators <span class="math inline">\(T_{GM}\)</span> and <span class="math inline">\(T_{UM}\)</span> (mixed) have bounds that do not depend on the structure of <span class="math inline">\(\mA\)</span>, while the others do. The mixed unit vector, however, does depend on the dimension <span class="math inline">\(n\)</span>. Note that the variance for <span class="math inline">\(T_{RM}\)</span> and <span class="math inline">\(T_{UM}\)</span> (mixed) are not given. Notice that despite worse variance, <span class="math inline">\(T_{GM}\)</span> has a better <span class="math inline">\((\epsilon,\delta)\)</span> bound. All of the results are of order <span class="math inline">\(\epsilon^{-2} \ln(1/\delta)\)</span>, indicating a large similarity in their performance and an unfortunate limitation in the error bound. To halve the error we require four times as many samples; however, this result is later improved upon by Hutch++ in Section~<span class="math inline">\(\ref{sec:hutchplusplus}\)</span>.</p>
<div id="tbl-estimator-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-estimator-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: Comparison of estimators, variances, sample complexity, and random bit usage.
</figcaption>
<div aria-describedby="tbl-estimator-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Estimator</th>
<th style="text-align: left;">Variance</th>
<th style="text-align: left;">Bound on # samples for <span class="math inline">\((\varepsilon,\delta)\)</span>-approx</th>
<th style="text-align: left;">Random bits</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong><span class="math inline">\(T_{GM}\)</span> (Gaussian)</strong></td>
<td style="text-align: left;"><span class="math inline">\(2\lVert A\rVert_F^2\)</span></td>
<td style="text-align: left;"><span class="math inline">\(20\varepsilon^{-2}\ln(2/\delta)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\Theta(n)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong><span class="math inline">\(T_{RM}\)</span> (Rayleigh-Quotient)</strong></td>
<td style="text-align: left;">–</td>
<td style="text-align: left;"><span class="math inline">\(\tfrac12\,\varepsilon^{-2}n^{-2}\operatorname{rank}^2(\mA)\ln(2/\delta)\kappa_f^2(\mA)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\Theta(n)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong><span class="math inline">\(T_{HM}\)</span> (Rademacher)</strong></td>
<td style="text-align: left;"><span class="math inline">\(2\big(\lVert A\rVert_F^2-\sum_{i=1}^n A_{ii}^2\big)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(6 \epsilon^{-2}\ln\left(2 \operatorname{rank}(\mA)/\delta\right)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\Theta(\log n)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong><span class="math inline">\(T_{UM}\)</span> (Unit Vector)</strong></td>
<td style="text-align: left;"><span class="math inline">\(n\sum_{i=1}^n A_{ii}^2-\tr^2(\mA)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\tfrac12\,\varepsilon^{-2}\ln(2/\delta)\,n\,\dfrac{\max_i A_{ii}}{\tr(\mA)}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\Theta(\log n)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong><span class="math inline">\(T_{UM}\)</span> (With Mixing <span class="math inline">\(\mathcal{F}\)</span>)</strong></td>
<td style="text-align: left;">–</td>
<td style="text-align: left;"><span class="math inline">\(8\varepsilon^{-2}\ln(4n^2/\delta)\ln(4/\delta)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\Theta(\log n)\)</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<section id="gaussian-estimator-t_gm-example" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="gaussian-estimator-t_gm-example"><span class="header-section-number">4.1</span> Gaussian Estimator <span class="math inline">\(T_{GM}\)</span> Example</h2>
<p>For brevity, only one of the estimator derivations is given in detail, as the full proofs are available in <span class="citation" data-cites="avron2011RandomizedAlgorithmsEstimating">Avron and Toledo (<a href="#ref-avron2011RandomizedAlgorithmsEstimating" role="doc-biblioref">2011</a>)</span>. Proceeding from the variance in Equation~<span class="math inline">\(\ref{eq:gaussian_variance}\)</span>, Avron and Toledo take the typical path of applying a Chernoff-style argument. The Gaussian distribution is invariant under rotation, as shown by <span class="citation" data-cites="Harvey2021Randomized2">Harvey (<a href="#ref-Harvey2021Randomized2" role="doc-biblioref">2021, sec. 24.4</a>)</span>, so we can diagonalize <span class="math inline">\(\mA = \mU \Lambda \mU^\top\)</span> and consider <span class="math inline">\(\vw_j = \mU^\top \vv_j\)</span> as a Gaussian vector, without loss of generality. Then the estimator becomes:</p>
<p><span class="math display">\[
T_{GM} =\vv^\top \mA \vv = \frac{1}{m}\sum_j^{m}\sum_{i=1}^N \lambda_i w_{ij}^2 = \frac{1}{m}\sum_{i=1}^N \lambda_i \sum_{j=1}^m w_{ij}^2,
\]</span></p>
<p>where <span class="math inline">\(w_{ij} \sim \mathcal{N}(0,1)\)</span>, and so each <span class="math inline">\(w_{ij}^2\)</span> is a <span class="math inline">\(\chi^2\)</span> random variable with one degree of freedom. The sum of <span class="math inline">\(m\)</span> i.i.d. <span class="math inline">\(\chi^2\)</span> random variables is a <span class="math inline">\(\chi^2\)</span> random variable with <span class="math inline">\(m\)</span> degrees of freedom, <span class="math inline">\(\sum_j^m w_{ij}^2 \sim \chi^2_m\)</span>.</p>
<p>The tail bounds for the <span class="math inline">\(\chi^2\)</span> distribution are well studied. Using the following lemma we can derive a weak <span class="math inline">\((\epsilon, \delta)\)</span> bound for <span class="math inline">\(T_{GM}\)</span>, after which a stronger bound by Chernoff-style analysis is given by Avron and Toledo.</p>
<section id="weak-epsilon-delta-bound-for-t_gm" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="weak-epsilon-delta-bound-for-t_gm"><span class="header-section-number">4.1.1</span> Weak <span class="math inline">\((\epsilon, \delta)\)</span> Bound for <span class="math inline">\(T_{GM}\)</span></h3>
<div id="lem-chi-squared-tail" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 1 (Chi-Squared Tail Bound)</strong></span> Let <span class="math inline">\(X \sim \chi^2_m\)</span>. Then for any <span class="math inline">\(\epsilon \in (0,1)\)</span>, <span class="math display">\[
\Pr\left[|X - m| \geq \epsilon m\right] \leq 2 \exp\left(-\frac{m}{8}\epsilon^2\right).
\]</span> Source: <span class="citation" data-cites="Harvey2021Randomized2">Harvey (<a href="#ref-Harvey2021Randomized2" role="doc-biblioref">2021</a>, Lemma 24.2.4)</span></p>
</div>
<p>Let <span class="math inline">\(X_i = \sum_{j=1}^m w_{ij}^2\)</span>, then <span class="math inline">\(X_i \sim \chi^2_m\)</span> with <span class="math inline">\(\mathbb{E}[X_i] = m\)</span>. The estimator can be written as <span class="math display">\[
T_{GM} = \frac{1}{m} \sum_{i=1}^N \lambda_i X_i.
\]</span> <a href="#lem-chi-squared-tail" class="quarto-xref">Lemma&nbsp;1</a> gives <span class="math inline">\(\epsilon\)</span> failure probability in terms of <span class="math inline">\(m\)</span> such that <span class="math inline">\(\delta \leq 2 \exp\left(-\frac{m}{8}\epsilon^2\right)\)</span>. Then a standard union bound over all <span class="math inline">\(N\)</span> non-zero eigenvalues gives <span class="math inline">\(\delta_{\text{total}} \leq 2 \text{rank}(\mA) \exp\left(-\frac{m}{8}\epsilon^2\right)\)</span>. Solving for <span class="math inline">\(m\)</span> then gives the required number of samples for an <span class="math inline">\((\epsilon, \delta)\)</span>-approximation of <span class="math display">\[
m \geq \frac{8}{\epsilon^2} \ln\left(\frac{2 \operatorname{rank}(\mA)}{\delta}\right).
\]</span></p>
<p>To improve this bound further, Avron and Toledo apply a Chernoff-style argument directly to the weighted sum of <span class="math inline">\(\chi^2\)</span> random variables, giving the final bound in <a href="#tbl-estimator-comparison" class="quarto-xref">Table&nbsp;3</a>. A detailed proof using elementary symmetric polynomials, Markov’s inequality, and moment generating functions for <span class="math inline">\(\chi^2\)</span> random variables is shown in Appendix <span class="math inline">\(\ref{app:stronger_bound}\)</span>.</p>
</section>
</section>
<section id="summary-of-avron-and-toledo-analysis" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="summary-of-avron-and-toledo-analysis"><span class="header-section-number">4.2</span> Summary of Avron and Toledo Analysis</h2>
<p>By rederiving the bounds in a Chernoff-style for particular matrix structures, even stronger bounds can be attained in the Gaussian case. For <span class="math inline">\(T_{RM}\)</span> the proof proceeds with a much simpler Hoeffding’s inequality argument after bounding the output of the MVP. The original Hutchinson’s <span class="math inline">\(T_{HM}\)</span> is bounded by diagonalizing <span class="math inline">\(\mA\)</span> and applying a lemma for bounding the oblivious Rademacher vectors that have been mixed by the arbitrary unitary matrix that is the basis of <span class="math inline">\(\mA=\mU^\top \Lambda \mU\)</span>, then union bounding over <span class="math inline">\(\text{rank}(\mA)\)</span>, finally conjecturing a tighter bound without rank may exist. Comparing bounds directly between estimators is not always conclusive, as the authors themselves conjecture that they have not found the tightest bounds. The final estimator <span class="math inline">\(T_{UM}\)</span> is bounded similarly using a lemma for the mixed unit vectors.</p>
<p>Their work shows that each estimator requires a specialized treatment to achieve their best possible <span class="math inline">\((\epsilon, \delta)\)</span> bounds, and that the basis sensitivity of estimators can be mitigated by the use of randomized mixing matrices. The <span class="math inline">\(m = O(\epsilon^{-2})\)</span> dependence on the number of samples greatly limits the practical use of the method for high-accuracy trace estimation, with the suggestion for example of using a <span class="math inline">\(\epsilon=0.01\)</span> estimate to seed iterative methods for higher accuracy. Reducing this dependence to <span class="math inline">\(\epsilon^{-1}\)</span> would make higher degrees of accuracy more feasible, motivating the development of Hutch++ in the next section. In many applications of trace estimation the eigenvalue spectrum is dominated by few large values, which contribute the majority of the variance, as is seen in Principal Component Analysis and the SVD decomposition. Given a method of isolating the bulk of the variance, a stochastic estimator can be improved, forming the motivation behind Hutch++ <span class="citation" data-cites="meyer2021HutchOptimalStochastic">(<a href="#ref-meyer2021HutchOptimalStochastic" role="doc-biblioref">Meyer et al. 2021</a>)</span>.</p>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-ailonChazelle2006ApproximateNearest" class="csl-entry" role="listitem">
Ailon, Nir, and Bernard Chazelle. 2006. <span>“Approximate Nearest Neighbors and the Fast Johnson-Lindenstrauss Transform.”</span> In <em>Proceedings of the Thirty-Eighth Annual ACM Symposium on Theory of Computing</em>, 557–63. STOC ’06. New York, NY, USA: Association for Computing Machinery. <a href="https://doi.org/10.1145/1132516.1132597">https://doi.org/10.1145/1132516.1132597</a>.
</div>
<div id="ref-avron2011RandomizedAlgorithmsEstimating" class="csl-entry" role="listitem">
Avron, Haim, and Sivan Toledo. 2011. <span>“Randomized Algorithms for Estimating the Trace of an Implicit Symmetric Positive Semi-Definite Matrix.”</span> <em>J. ACM</em> 58 (2): 8:1–34. <a href="https://doi.org/10.1145/1944345.1944349">https://doi.org/10.1145/1944345.1944349</a>.
</div>
<div id="ref-haber_lecture11_2025" class="csl-entry" role="listitem">
Haber, Eldad. 2024. <span>“EOSC 555: Lecture 11 Notes.”</span> <a href="https://chipnbits.github.io/content/eosc555/lectures/lecture11/">https://chipnbits.github.io/content/eosc555/lectures/lecture11/</a>.
</div>
<div id="ref-Harvey2021Randomized2" class="csl-entry" role="listitem">
Harvey, Nicholas J. A. 2021. <span>“A Second Course in Randomized Algorithms.”</span> University of British Columbia, Course Textbook Draft. <a href="https://www.cs.ubc.ca/~nickhar/Book2.pdf">https://www.cs.ubc.ca/~nickhar/Book2.pdf</a>.
</div>
<div id="ref-HU2024116883" class="csl-entry" role="listitem">
Hu, Zheyuan, Zekun Shi, George Em Karniadakis, and Kenji Kawaguchi. 2024. <span>“Hutchinson Trace Estimation for High-Dimensional and High-Order Physics-Informed Neural Networks.”</span> <em>Computer Methods in Applied Mechanics and Engineering</em> 424: 116883. https://doi.org/<a href="https://doi.org/10.1016/j.cma.2024.116883">https://doi.org/10.1016/j.cma.2024.116883</a>.
</div>
<div id="ref-Hutchinson01011990" class="csl-entry" role="listitem">
Hutchinson, M. F. 1990. <span>“A Stochastic Estimator of the Trace of the Influence Matrix for Laplacian Smoothing Splines.”</span> <em>Communications in Statistics - Simulation and Computation</em> 19 (2): 433–50. <a href="https://doi.org/10.1080/03610919008812866">https://doi.org/10.1080/03610919008812866</a>.
</div>
<div id="ref-meyer2021HutchOptimalStochastic" class="csl-entry" role="listitem">
Meyer, Raphael A., Cameron Musco, Christopher Musco, and David P. Woodruff. 2021. <span>“Hutch++: <span>Optimal</span> <span>Stochastic</span> <span>Trace</span> <span>Estimation</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2010.09649">https://doi.org/10.48550/arXiv.2010.09649</a>.
</div>
<div id="ref-pearlmutter1994fast" class="csl-entry" role="listitem">
Pearlmutter, Barak A. 1994. <span>“Fast Exact Multiplication by the Hessian.”</span> <em>Neural Computation</em> 6 (1): 147–60. <a href="https://mural.maynoothuniversity.ie/id/eprint/5501/">https://mural.maynoothuniversity.ie/id/eprint/5501/</a>.
</div>
<div id="ref-UbaruSaad2018" class="csl-entry" role="listitem">
Ubaru, Shashanka, and Yousef Saad. 2018. <span>“Applications of Trace Estimation Techniques.”</span> In <em>High Performance Computing in Science and Engineering</em>, edited by Tomáš Kozubek, Martin Čermák, Petr Tichý, Radim Blaheta, Jakub Šístek, Dalibor Lukáš, and Jiří Jaroš, 19–33. Cham: Springer International Publishing.
</div>
<div id="ref-woodruffOptimalQueryComplexities" class="csl-entry" role="listitem">
Woodruff, David P., Fred Zhang, and Qiuyi (Richard) Zhang. 2022. <span>“Optimal Query Complexities for Dynamic Trace Estimation.”</span> In <em>Proceedings of the 36th International Conference on Neural Information Processing Systems</em>. NIPS ’22. Red Hook, NY, USA: Curran Associates Inc.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/chipnbits\.github\.io");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2025, Simon Ghyselincks</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://chipnbits.github.io/">
      <i class="bi bi-house" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/chipnbits">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>