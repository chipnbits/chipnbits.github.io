{
  "hash": "8bbdbc70b7c4d785449a4442c560b6df",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Lecture 10\"\nsubtitle: \"MAP, MLE, and Score Function\"\ndate: 2024-11-22\nauthor: \"Simon Ghyselincks\"\ndescription: >-\n    In this mathematical lecture, some of the foundational principles of Bayesian inverse problems and their statistical interpretation are discussed. A set of computational tools are shown that aid in finding the solution to some of these problems.\ncategories:\n  - Machine Learning\n  - Neural Networks\n  - Bayesian Inference\n\nimage: \"imgs/langevin_dynamics.gif\"\ndraft: false\n\neditor: \n  render-on-save: false\n\nfilters:\n  - pseudocode\n  - diagram\n\nbibliography: references.bib\nbiblatexoptions: \"style=numeric,sorting=nyt\"  # Uses numbered style and sorts by name-year-title\nbiblio-style: numeric-comp  # A numeric style in biblatex, similar to IEEE, with compressed citation ranges\n\npseudocode:\n  caption-prefix: \"Algorithm\"\n  reference-prefix: \"Algorithm\"\n  caption-number: true\n---\n\n::: {.hidden}\n$$\n\\def\\argmin{\\operatorname*{argmin}}\n\\def\\bmat#1{\\begin{bmatrix}#1\\end{bmatrix}}\n\\def\\Diag{\\mathbf{Diag}}\n\\def\\ip#1{\\langle #1 \\rangle}\n\n\\def\\maximize#1{\\displaystyle\\maxim_{#1}}\n\n\\def\\minimize#1{\\displaystyle\\minim_{#1}}\n\\def\\norm#1{\\|#1\\|}\n\\def\\proj{\\mathbf{proj}}\n\\def\\R{\\mathbb R}\n\\def\\Re{\\mathbb R}\n\\def\\Rn{\\R^n}\n\\def\\rank{\\mathbf{rank}}\n\\def\\range{{\\mathbf{range}}}\n\\def\\span{{\\mathbf{span}}}\n\\def\\textt#1{\\quad\\text{#1}\\quad}\n\\def\\trace{\\mathbf{trace}}\n\\def\\bf#1{\\mathbf{#1}}\n$$\n:::\n\n\n\n## Gradient Descent with Score Function\n\n::: {.callout-note icon=false}\n# Definitions\n\n| **Component**        | **Description**                   | **Dimensions**              |\n|----------------------|-----------------------------------|-----------------------------|\n| $F(x)$                 | Forward operator                  | $\\mathbb{R}^n \\rightarrow \\mathbb{R}^m$                |\n| $x$                    | Model parameters                  | $\\mathbb{R}^n$              |\n| $b$                    | Observed data                     | $\\mathbb{R}^m$              |\n| $\\epsilon$            | Noise                             | $\\mathbb{R}^m$              |\n| $\\pi(x)$              | Probability distribution                |             | \n| $\\phi(x,\\theta)$      | Potential function                | $\\mathbb{R}^n \\rightarrow \\mathbb{R}$                |\n| $\\theta$            | Learnable Parameters                   | $\\mathbb{R}^p$              |\n| $Z(\\theta)$          | Partition Function                | $\\mathbb{R}$                |\n| $s(x, \\theta)$      | Score Function                    | $\\mathbb{R}^n$              |\n:::\n\nThe classic inverse problem is defined as \n\n$$b = F(x) + \\epsilon$$\n\nwhere $F(x)$ is the forward operator, $b$ is the observed data, and $\\epsilon$ represents the noise in the measurement or process. We often assume that $\\epsilon$ is Gaussian with zero mean and covariance matrix $\\Sigma$.\n\n$$ \\epsilon \\sim \\mathcal{N}(0, \\Sigma) $$\n\nThe probability of deviation of the observed data from the forward model in this case is given by:\n$$\n\\pi(\\epsilon) = \\frac{1}{(2\\pi)^{m/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}\\epsilon^\\intercal \\Sigma^{-1} \\epsilon\\right)\n$$\n\nWithout any prior information about the error, it is difficult to estimate the covariance matrix $\\Sigma$. For the purpose of this analysis we can assume that it is a normal distriution with zero mean and a diagonal $\\sigma^2 I$ covariance matrix. The likelihood function simplifies to:\n\n$$ \\pi(\\epsilon) = \\frac{1}{(2\\pi \\sigma^2)^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^2}\\|b-F(x)\\|^2 \\right) $$\n\nTo model the probability distribution of the inverse problem parameters $x$, we introduce a prior distribution $\\pi(x)$. To ensure positivity of $\\pi(x)$ over the entire domain and proper normalization, we define it using a **potential function** $\\phi(x, \\theta)$:\n$$\\pi(x; \\theta) = \\frac{e^{-\\phi(x, \\theta)}}{Z(\\theta)}$$\n\nWhere the **partion function** $Z(\\theta)$ is given by:\n\n$$Z(\\theta) = \\int_\\Omega e^{-\\phi(x, \\theta)} dx$$\n\nNote the partition function is required to make the probability distribution integrate to $1$. The exponential operator on the potential ensures that all $\\pi(x)$ values are positve since $e^\\phi > 0$ for all $z \\in \\mathbb{R}$. In practice, it is often intractable to directly compute the partition function when updating the model parameters $\\theta$ for distributions that are more complex than a Gaussian. \n\n$\\phi(x, \\theta): \\mathbb{R}^n \\rightarrow \\mathbb{R}$ maps $x$ to a scalar value, and $\\theta$ are the parameters of the model. For example if we are modeling a Gaussian, the parameters might include the covariance matrix $\\Sigma$. It has a physical interpretation as an energy of a system, where $\\phi$ values correspond to low probability density regions. For this reason it is often called the **energy function** in physics-inspired models.\n\n### Maximum A Posteriori Estimation\n\nThe goal of **maximum a posteriori (MAP)** estimation is to find the most likely $x$ given the observed data $b$ and model parameters $\\theta$, maximize the posterior probability $\\pi(x|b; \\theta)$:\n\n$$\n\\begin{align*}\n\\max_x \\pi(x|b; \\theta) &= \\max_x \\frac{\\pi(b|x; \\theta) \\pi(x; \\theta)}{\\pi(b)}\\\\\n\\max_x \\underbrace{\\pi(x|b; \\theta)}_{\\text{Posterior}} & = \\max_x \\underbrace{\\pi(b|x; \\theta)}_{\\text{Likelihood}} \\underbrace{\\pi(x; \\theta)}_{\\text{Prior}}\\\\\n\\end{align*}\n$$\n\nSince $\\pi(b)$ is independent of $x$, it does not affect the maximization problem. Substituting the likelihood and prior distributions, we have:\n\n$$\n\\begin{align*}\n& = \\max_x \\frac{1}{(2\\pi \\sigma^2)^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^2}\\|b-F(x)\\|^2 \\right) \\frac{1}{Z(\\theta)} e^{-\\phi(x, \\theta)}\n\\end{align*}\n$$\n\nThe logarithm is a monotonic function, we can maximize the log-likelihood instead of the likelihood with no loss of generality. $\\max_x \\pi(x) = \\max_x \\log(\\pi(x))$. Intuitively, since the logarithim is always increasing in output, $\\log(z) > \\log(y)$ implies $z > y$. In addition the product of two exponentials is the same as the sum of the exponents, and the maximum of a function is the same as the minimum of the negative of the function. This allows us to rewrite the log-likelihood as:\n\n::: {.callout-note icon=false}\n# Minimization Objective\n$$\\max_x \\log \\pi(x|b; \\theta) = \\min_x \\left( \\frac{1}{2\\sigma^2}\\|b-F(x)\\|^2 + \\phi(x, \\theta) \\right)$$\n:::\n\nWe have looked at methods previously of how to differentiate the forward operator $F$ and perform gradient descent. We take the gradient with respect to $x$ to find the minimum of the function. \n\n$$\n\\begin{align*}\ng &= \\nabla_x \\left(\\frac{1}{2\\sigma^2}\\|b-F(x)\\|^2 - \\phi(x, \\theta)\\right)\\\\\n&= \\frac{1}{\\sigma^2} \\frac{\\partial F}{\\partial x} (F(x) - b) - \\nabla_x \\phi(x, \\theta)\\\\\n&= \\frac{1}{\\sigma^2} J^T(x) (F(x) - b) + s(x, \\theta)\n\\end{align*}\n$$\n\nUsing gradient descent, we can update the model parameters $\\theta$ by taking steps in away from the direction of the gradient $g$:\n\n$$x_{k+1} = x_k - \\alpha g$$\n\n\n## Score Function\n\n$s(x;\\theta) is known as the **score function** of $\\pi(x; theta)$. \n$$s(x, \\theta):= \\nabla_x \\log (\\pi(x)) = - \\nabla_x \\phi(x, \\theta)$$\n\nIt is the negative gradient of the potential function $\\phi(x, \\theta)$ with respect to $x$. The score function is a generalization of the gradient of the log-likelihood function, and is described in more detail in Schervish's \"Theory of Statistics\"  [@Schervish2012-sk].\n\nScore has a physical intution connected to energy potentials and fields. In physics, the electric field $\\mathbf{E}$ is the negative gradient of the electric potential $V$:\n$$ \\mathbf{E} = -\\nabla_x V(x)$$\n\nSimalarly, the score function is the negative gradient of the potential function $\\phi(x, \\theta)$ in the case where $\\pi(x) = e^{-\\phi(x, \\theta)}$. The score function is the direction in which the probability distribution is most likely to change.\n\n#### Example: 2D Gaussian Distribution\n\nConsider a 2D Gaussian distribution with zero mean and covariance $\\sigma^2 I$:\n\n| **Function**                                    | **Expression**                                                                                                                                   |\n|-------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Probability Distribution**          | $\\pi(x) = \\frac{1}{2\\pi \\sigma^2}\\exp\\left( -\\frac{1}{2\\sigma^2} \\| x \\|^2 \\right)$                                                               |\n| **Potential Function**      | $\\phi(x, \\theta) =  - \\frac{1}{2\\sigma^2} \\| x \\|^2 - \\log(2\\pi \\sigma^2)$                                                                          |\n| **Score Function**  | $-\\nabla_x \\phi(x, \\theta) = -\\left( -\\frac{x}{\\sigma^2} x \\right) = \\frac{x}{\\sigma^2}$                                                          |\n\nIn regions of high probability density, the potential function is low becuase the relation $\\pi(x) = e^{-\\phi(x, \\theta)}$ is monotonic in $\\phi(x, \\theta)$. The score funtion is always pointing in the local direction of the largest directional derivative of the probability distribution $\\pi$. \n\n#### Visualization\n\nBelow is a visualization of the probability density function (PDF), potential function, and score function of a 2D Gaussian distribution.\n\n::: {#fig-scores .cell fig-width='12' layout-ncol='3' execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Make plots for a 2D Gaussian distribution heatmap, potential function, and score function\n\n# Define the 2D Gaussian distribution\ndef gaussian_pdf(x, y, sigma=1):\n    return np.exp(-0.5*(x**2 + y**2)/sigma**2)/(2*np.pi*sigma**2)\n\n# Define the potential function\ndef potential_function(x, y, sigma=1):\n    return 0.5*(x**2 + y**2)/sigma**2 - np.log(2*np.pi*sigma**2)\n\n# Define the score function\ndef score_function(x, y, sigma=1):\n    return -np.array([x, y])/sigma**2\n\n# Create a grid of points\nx = np.linspace(-3, 3, 500)\ny = np.linspace(-3, 3, 500)\nX, Y = np.meshgrid(x, y)\n\n# Compute the PDF, potential function, and score function\npdf = gaussian_pdf(X, Y)\npotential = potential_function(X, Y)\nscore = score_function(X, Y)\n\n# Plot the Probability Distribution with a colorbar\nplt.figure(figsize=(4, 3))\nim = plt.imshow(pdf, cmap='viridis', extent=[-3, 3, -3, 3])\nplt.axis('off')\nplt.colorbar(im, shrink=0.8, label=\"Density\")\nplt.show()\n\n# Plot the Potential Function with a colorbar\nplt.figure(figsize=(4, 3))\nim = plt.imshow(potential, cmap='viridis', extent=[-3, 3, -3, 3])\nplt.axis('off')\nplt.colorbar(im, shrink=0.8, label=\"Potential\")\nplt.show()\n\n# Downsample the grid for quiver plotting\nstep = 50  # Downsample by taking every 50th point\nX_downsampled = X[::step, ::step]\nY_downsampled = Y[::step, ::step]\nscore_downsampled = score_function(X_downsampled, Y_downsampled)\n\n# Plot the Score Function as a quiver plot over the PDF\nplt.figure(figsize=(4, 3))\nplt.imshow(pdf, cmap='viridis', extent=[-3, 3, -3, 3])\nplt.quiver(\n    X_downsampled, Y_downsampled, \n    score_downsampled[0], score_downsampled[1], \n    color='black'\n)\nplt.axis('off')\n\n# Save to file\nplt.savefig(\"imgs/score_function.png\", bbox_inches='tight')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Probability Distribution](index_files/figure-html/fig-scores-output-1.png){#fig-scores-1 width=322 height=241}\n:::\n\n::: {.cell-output .cell-output-display}\n![Potential Function](index_files/figure-html/fig-scores-output-2.png){#fig-scores-2 width=301 height=241}\n:::\n\n::: {.cell-output .cell-output-display}\n![Score Function](index_files/figure-html/fig-scores-output-3.png){#fig-scores-3 width=241 height=241}\n:::\n\nPDF, Potential Function, and Score Function of a 2D Gaussian Normal Distribution\n:::\n\n\n## Maximum Likelihood Estimate from Samples\n\nA common problem is estimating a probability distribution $\\pi(x)$ based on a set of empirical samples $\\{x_1, x_2, ..., x_N\\}$. Often in statistical analysis, we are working with an unknown $\\pi$ and attempting to make our best estimate from sampled data. If we assume that the drawn samples are independent and identically distributed (i.i.d.), then the **likelihood** of the samples is the product of the likelihood of each sample. Recalling that $Z(\\theta) = \\int_\\Omega \\pi(x) dx$, the likelihood of the samples is:\n\n$$ \n\\begin{align*}\n\\pi(x_1, x_2, ..., x_N) &= \\prod_{i=1}^N \\pi(x_i) = \\pi(x_1) \\pi(x_2) ... \\pi(x_N) \\\\\n&= \\frac{1}{Z(\\theta)} \\exp\\left(-\\phi(x_1, \\theta)\\right) \\frac{1}{Z(\\theta)} \\exp\\left(-\\phi(x_2, \\theta)\\right) ... \\frac{1}{Z(\\theta)} \\exp\\left(-\\phi(x_N, \\theta)\\right) \\\\\n&= \\frac{1}{\\left[Z(\\theta)\\right]^N} \\exp\\left(-\\sum_{i=1}^N \\phi(x_i, \\theta)\\right)\n\\end{align*}\n$$\n\nSince some $x$ are observed, the challenge is to find the potential function $\\phi(x, \\theta)$ that maximizes the likelihood of the samples, given the model parameters $\\theta$ and a fixed family of $\\phi(,;\\theta)$ functions.\n\nThe **Maximum Likelihood Estimation (MLE)** is the process of finding the parameters $\\theta$ that maximize the likelihood of having observed the samples. Unlike the MAP estimate, there is no posterior and prior distribution. So in this case $\\pi(x|\\theta)$ is being directly maximized. This is equivalent to minimizing the negative log-likelihood as before:\n\n$$ \n\\begin{align*}\n\\text{MLE} &= \\text{argmax}_\\theta \\frac{1}{\\left[Z(\\theta)\\right]^N} \\exp\\left(-\\sum_{i=1}^N \\phi(x_i, \\theta)\\right)\\\\\n&= \\argmin_\\theta N \\log(Z(\\theta)) + \\sum_{i=1}^N \\phi(x_i, \\theta)\\\\\n&= \\argmin_\\theta \\log(Z(\\theta)) + \\frac{1}{N} \\sum_{i=1}^N \\phi(x_i, \\theta)\n\\end{align*}\n$$\n\nHowever we again run into the problem of the partition function $Z(\\theta)$ which for most distributions in higher dimensions is intractable to compute. For example we may be trying to solve an integral in $100$ dimensions with no analytical solution.\n\n$$ \\int_{x_1} \\int_{x_2} ... \\int_{x_{100}} e^{-\\phi(x, \\theta)} dx_1 dx_2 ... dx_{100} $$\n\n#### Minimizing with Gradient Descent\n\n::: {.callout-note icon=false}\n# Minimization Objective\n$$ \\text{MLE} = \\argmin_\\theta \\left( \\log(Z(\\theta)) + \\frac{1}{N} \\sum_{i=1}^N \\phi(x_i, \\theta) \\right) $$\n:::\n\nThe gradient of the MLE objective with respect to $\\theta$ is:\n\n$$\n\\begin{align*}\ng &= \\nabla_\\theta \\left( \\log(Z(\\theta)) + \\frac{1}{N} \\sum_{i=1}^N \\phi(x_i, \\theta) \\right)\\\\\n&= \\nabla_\\theta \\log(Z(\\theta)) + \\frac{1}{N} \\sum_{i=1}^N \\nabla_\\theta \\phi(x_i, \\theta)\n\\end{align*}\n$$\n\nThe left side term can be further reduced by using the definition of the partion function $Z(\\theta) =  \\int_\\Omega e^{-\\phi(x, \\theta)} dx$, the probability distribution $\\pi_\\theta(x) = e^{-\\phi(x, \\theta)}$:\n\n$$\n\\begin{align*}\n\\nabla_\\theta \\log(Z(\\theta)) &= \\frac{1}{Z(\\theta)}\\nabla_\\theta \\int_\\Omega e^{-\\phi(x, \\theta)} dx\\\\\n&= \\int_\\Omega \\frac{1}{Z(\\theta)}  \\nabla_\\theta e^{-\\phi(x, \\theta)} dx\\\\\n&= - \\int_\\Omega \\frac{1}{Z(\\theta)} e^{-\\phi(x, \\theta)} \\nabla_\\theta \\phi(x, \\theta) dx\\\\\n&= - \\int_\\Omega \\pi(x) \\nabla_\\theta \\phi(x, \\theta) dx\\\\\n&=  \\mathbb{E}_{x \\sim \\pi_\\theta(x)} \\left[ -\\nabla_\\theta \\phi(x, \\theta) \\right]\\\\\n& \\approxeq \\frac{1}{M} \\sum_{i=1}^M -\\nabla_\\theta \\phi(x_i, \\theta)\n\\end{align*}\n$$\n\nWe estimate the value of $\\nabla_\\theta \\log(Z(\\theta))$ by taking the expectation value of the score function over the samples. This is a Monte Carlo approximation of the true integral using the available i.i.d. samples $\\{ x_1, x_2, ..., x_N \\}$. The gradient of the MLE objective is then:\n\n::: {.callout-note icon=false}\n# Score Matching Gradient\n$$ g = \\underbrace{-\\frac{1}{M} \\sum_{i=1}^M \\nabla_\\theta \\phi(\\tilde x_i, \\theta)}_{\\text{Synthesis}} + \\underbrace{\\frac{1}{N} \\sum_{i=1}^N \\nabla_\\theta \\phi(x_i, \\theta)}_{\\text{Analysis}}$$\n:::\n\nAn optimal point is reached by first order optimality conditions, where the gradient is zero. The MLE objective is minimized when the **synthesis** and **analysis** terms are equal. This is known as **score matching** and is a method for estimating the parameters of a probability distribution from samples. The synthesis $\\tilde x_i$ terms are drawn randomly from the proposed score $\\phi(\\tilde x_i, \\theta)$ , while the analysis $x_i$ terms are taken over the samples $\\{x_1, x_2, ..., x_N\\}$.\n\nAs before gradient descent can be used to update the model parameters $\\theta$:\n\n$$\\theta_{k+1} = \\theta_k - \\alpha g$$\n\n## An Application of Score: Langevin Dynamics\n\nAn example of the usage of a learned score function is in **Langevin Dynamics**. Langevin Dynamics is a method for sampling from a probability distribution $\\pi(x)$ by simulating a stochastic differential equation (SDE) that converges to the distribution. The SDE is given by:\n\n$$dx = -\\nabla_x \\phi(x, \\theta) dt + \\sqrt{2} dW$$\n\nThe score function pushes samples towards regions of high probability density, since it is a vector that points in the direction of maximum increase of the probability distribution. The noise term $dW$ is a Wiener process, which is a continuous-time stochastic process that is normally distributed with mean zero and variance $dt$. The Langevin Dynamics algorithm is a discretized version of the SDE:\n\n$$x_{k+1} = x_k - \\underbrace{\\Delta t \\nabla_x \\phi(x_k, \\theta)}_\\text{Score Term} + \\underbrace{\\sqrt{2 \\Delta t} z_k}_\\text{Stochastic Term}$$\n\nWhere $z_k \\sim \\mathcal{N}(0,1)$ is a random sample from a normal distribution with zero mean and unit variance. \n\nThe score points in the same direciton as the gradient of the probability distribution, so at each time step, the score term moves the sample to a higher probability region. The stochastic term adds noise to the sample, providing randomness to the process.\n\n### Code Example\n\nBelow is an example of Langevin Dynamics applied to a 2D Gaussian distribution. The score function is used to push the samples towards the center of the distribution, while the stochastic element adds noise and randomization to the process. The animation show an intial uniform sampling grid of points converging to the shape of the Gaussian distribution.\n\n::: {#fig-langevin .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport seaborn as sns\n\n# Ensure the Pillow writer is available\nfrom matplotlib.animation import PillowWriter\n\n# Define the 2D Gaussian distribution\ndef gaussian_pdf(x, y, sigma=1):\n    return np.exp(-0.5*(x**2 + y**2)/sigma**2)/(2*np.pi*sigma**2)\n\n# Define the potential function\ndef potential_function(x, y, sigma=1):\n    return 0.5*(x**2 + y**2)/sigma**2 - np.log(2*np.pi*sigma**2)\n\n# Define the score function\ndef score_function(x, y, sigma=1):\n    return -np.array([x, y])/sigma**2\n\n# Define the Langevin Dynamics update\ndef langevin_dynamics(samples, sigma=1, dt=0.05):\n    z = np.random.normal(0, 1, samples.shape)\n    score = score_function(samples[:, 0], samples[:, 1], sigma)\n    samples = samples + dt * score.T + np.sqrt(2 * dt) * z\n    return samples\n\n# Create a grid of points for the contour plot\nx = np.linspace(-3, 3, 500)\ny = np.linspace(-3, 3, 500)\nX, Y = np.meshgrid(x, y)\n\n# Compute the PDF for contour plotting\nsigma = .5\npdf = gaussian_pdf(X, Y, sigma)\n\n# Initialize samples on a 5x5 grid\ngrid_size = 10\ngrid_range = np.linspace(-2.5, 2.5, grid_size)\ninitial_samples = np.array([[x, y] for x in grid_range for y in grid_range])\nsamples = initial_samples.copy()\n\n# Set up the figure and axis\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Plot the initial contour\ncontour = ax.contourf(X, Y, pdf, levels=50, cmap='viridis')\nscatter = ax.scatter(samples[:, 0], samples[:, 1], color='red', s=30, label='Samples')\n\n# Add a legend\nax.legend()\n\n# Function to update the animation at each frame\ndef update(frame):\n    global samples\n    samples = langevin_dynamics(samples, dt=0.002, sigma=sigma)\n    scatter.set_offsets(samples)\n    ax.set_title(f'Langevin Dynamics Iteration: {frame+1}')\n    return scatter,\n\n# Create the animation\nani = animation.FuncAnimation(\n    fig, update, frames=400, interval=200, blit=True, repeat=False\n)\n\n# Save the animation as a GIF\nani.save(\"imgs/langevin_dynamics.gif\", writer=\"imagemagick\", fps=10)\n\n# Display the animation inline (if supported)\nplt.close(fig)\n```\n:::\n\n\n![Langevin Dynamics to a 2d Gaussian](imgs/langevin_dynamics.gif){.lightbox}\n\n## MAP Estimation with General Gaussian\n\nRevisitng the MAP estimation problem, we can consider a more general prior $\\pi(x)$ with mean $\\mu = 0$ and covariance $\\Sigma$:\n\n$$\n\\begin{align*}\n\\pi(b|x) &= \\frac{1}{(2\\pi \\sigma^2)^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^2}\\|b-F(x)\\|^2 \\right)\\\\\n\\pi_\\theta(x) &= \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}x^\\intercal \\Sigma^{-1} x\\right)\\\\\n\\text{MAP} &= \\min_x \\left( \\frac{1}{2\\sigma^2}\\|b-F(x)\\|^2 + \\frac{1}{2}x^\\intercal \\Sigma^{-1} x \\right)\n\\end{align*}\n$$ \n\nWe now make a choice of which $pi_\\theta(x)$ to use, as before this is a MLE based on the available empirical samples. For the purpose of finding the gradient, let the minimization parameters be $\\theta = \\Sigma^{-1}$, since the true $\\Sigma$ is positive definite, it can be found by inverting this result. The partition function $Z(\\theta)$ is known for the case of a Gaussian distribution, and so the MLE objective is:\n\n$$\n\\begin{align*}\n\\pi(x) &= \\max_\\theta \\Pi_{i=1}^N \\pi_\\theta(x_i)\\\\\n&= \\max_\\theta \\frac{1}{Z(\\theta)^N} \\exp\\left(-\\sum_{i=1}^N \\frac{1}{2}x_i^\\intercal \\Sigma^{-1} x_i\\right)\\\\\n&= \\min_\\theta N \\log(Z(\\theta)) + \\frac{1}{2} \\sum_{i=1}^N x_i^\\intercal \\Sigma^{-1} x_i\\\\\n&= \\min_\\theta -\\log((2\\pi)^{n/2}|\\Sigma|^{1/2}) + \\frac{1}{2N} \\sum_{i=1}^N x_i^\\intercal \\Sigma^{-1} x_i\\\\\n&= \\min_\\theta -\\frac{1}{2}\\log(|\\Sigma|) + \\frac{1}{2N} \\sum_{i=1}^N x_i^\\intercal \\Sigma^{-1} x_i\n\\end{align*}\n$$\n\n\n#### Finding the Gradient\n\n**Left Term:**\nTo find the min using gradient descent we take the gradient of each term with respect to $\\Sigma^{-1}$. The gradient of a log determinant can be found in the matrix cookbook [@Petersen2012] formula (57):\n$$ \\nabla_{A} \\log(|A|) = (A^{-1})^\\intercal $$\n\nThis can be rewritten as:\n\n$$ \\nabla_A \\log(|A^{-1}|) = A^\\intercal$$\n\nApplying this to the left term we get:\n\n$$ \\nabla_{\\Sigma^{-1}} \\left( -\\frac{1}{2}\\log(|\\Sigma|) \\right) = -\\frac{1}{2} \\Sigma$$\n\n**Right Term:**\nThe gradient of the second term is found by reformulating it as a trace of a new matrix $X = \\sum x_i x_i^\\intercal$:\n\n$$\n\\begin{align*}\n\\sum x_j^\\intercal \\Sigma^{-1} x_j &= \\sum \\text{trace} \\left( x_j^\\intercal  \\Sigma^{-1} x_j \\right)\\\\\n&= \\sum \\text{trace} \\left(  \\Sigma^{-1} x_j x_j^\\intercal \\right)\\\\\n&= \\text{trace} \\left(  \\Sigma^{-1} X\\right)\\\\\n\\end{align*}\n$$\n\nThe matrix cookbook [@Petersen2012] formula (100) gives the gradient of the trace of a matrix product:\n\n$$ \\nabla_X \\text{trace}(XB) = B^\\intercal$$\n\nApplying this we get:\n\n$$ \\nabla_{\\Sigma^{-1}} \\left( \\frac{1}{2N} \\sum x_i^\\intercal \\Sigma^{-1} x_i \\right) = \\frac{1}{2N} X^\\intercal = \\frac{1}{2N} \\sum x_i x_i^\\intercal$$\n\n**Combined Result:**\n\nCombining the two terms we get the gradient of the MLE objective with respect to $\\Sigma^{-1}$:\n$$\n\\begin{align*}\ng &= \\nabla_{\\Sigma^{-1}} \\left( \\frac{1}{2}\\log(|\\Sigma|^{-1}) + \\frac{1}{2N} \\sum_{i=1}^N x_i^\\intercal \\Sigma^{-1} x_i \\right)\\\\\n&= \\frac{1}{2} \\Sigma^\\intercal - \\frac{1}{2N} \\sum_{i=1}^N x_i x_i^\\intercal\n\\end{align*}\n$$\n\nSolving for where the gradient is zero, we find the optimal value of $\\Sigma$, since $\\Sigma = \\Sigma^\\intercal$:\n\n$$ \\Sigma = \\frac{1}{N} \\sum_{i=1}^N x_i x_i^\\intercal$$\n\nThis is the maximum likelihood estimate of the covariance matrix $\\Sigma$ given the samples $\\{x_1, x_2, ..., x_N\\}$, which can be interpreted as the expectation value of the outer product of the samples. The estimated covariance matrix is the actual true covariance matrix of the sampled data. This is a common result in statistics, where the sample mean is the MLE of the true mean of the distribution.\n\nUnfortunately, estimating parameters can be very difficult to do for non-Gaussian $\\pi_\\theta$ due to the partition function $Z(\\theta)$ being intractable. \n\n## Conclusion\n\nMAP estimation on an inverse problem may require the use of a prior distribution $\\pi(x)$ to regularize the solution. The potential function $\\phi(x, \\theta)$ is used to define the probability distribution $\\pi(x; \\theta)$, and the score function $s(x, \\theta)$ is the negative gradient of the potential function. \n\nIf the prior is not known, an MLE estimate can be made to find the parameters $\\theta$ that maximize the likelihood of the samples based on an assumed parameterized model. The score matching gradient is used to estimate the gradient of the MLE objective with respect to $\\theta$.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}