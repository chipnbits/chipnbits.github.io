{
  "hash": "5bd9d26735284357012681aede92850c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Lecture 6: Autodiff and Implementing Gauss-Newton\"\nsubtitle: \"A look at some of the foundations of automatic differentiation and the Gauss-Newton optimization method.\"\ndate: 2024-10-08\nauthor: \"Simon Ghyselincks\"\ndescription: >-\n    Automatic differentiation is a powerful tool for solving optimization problems that can be used to automate the process of Gauss-Newton optimization. Here we put together an implementation of the Gauss-Newton method using PyTorch.\ncategories:\n  - Optimization\n  - Gauss-Newton\n  - Automatic Differentiation\n  - PyTorch\n\nimage: imgs/fitting_animation_time_varying.gif\ndraft: false\n\neditor: \n  render-on-save: false\n\n\nfilters:\n  - pseudocode\n\npseudocode:\n  caption-prefix: \"Algorithm\"\n  reference-prefix: \"Algorithm\"\n  caption-number: true\n---\n\n\n\n::: {.hidden}\n$$\n\\def\\argmin{\\operatorname*{argmin}}\n\\def\\bmat#1{\\begin{bmatrix}#1\\end{bmatrix}}\n\\def\\Diag{\\mathbf{Diag}}\n\\def\\ip#1{\\langle #1 \\rangle}\n\n\\def\\maximize#1{\\displaystyle\\maxim_{#1}}\n\n\\def\\minimize#1{\\displaystyle\\minim_{#1}}\n\\def\\norm#1{\\|#1\\|}\n\\def\\proj{\\mathbf{proj}}\n\\def\\R{\\mathbb R}\n\\def\\Re{\\mathbb R}\n\\def\\Rn{\\R^n}\n\\def\\rank{\\mathbf{rank}}\n\\def\\range{{\\mathbf{range}}}\n\\def\\span{{\\mathbf{span}}}\n\\def\\textt#1{\\quad\\text{#1}\\quad}\n\\def\\trace{\\mathbf{trace}}\n\\def\\bf#1{\\mathbf{#1}}\n$$\n:::\n\n\n\n\n# Automatic Differentiation\n\nReturning to the Lotka-Volterra model, we can now use automatic differentiation to compute the Jacobian matrix of the forward model. In fact, it can be shown that we can perform Gauss-Newton optimization more efficiently by using the Jacobian-vector product (JVP) and the vector-Jacobian product (VJP) instead of the full Jacobian matrix, since in the algorithm what we are truly interested in is the product of the Jacobian with a vector or its transpose. This equates to a directional derivative.\n\n### Application to the Lotka-Volterra Model\n\nTake a forward model $F(p)$ for which we want a linear approximation at $p_k$. We can write the Taylor expansion of the forward model as:\n\n$$ F(p_k + \\epsilon v) = F(p_k) + J_k \\epsilon v + \\mathcal{O}(\\epsilon^2)$$\n\nwhere $J_k$ is the Jacobian of $F(p_k)$. If we take the derivative of both sides in this expansion with respect to $\\epsilon$ we get:\n\n$$ \\frac{d}{d \\epsilon} F(p_k + \\epsilon v) = J_k v + \\mathcal{O}(\\epsilon)$$\n\nIf we make $\\epsilon$ very small then the Jacobian of the forward problem can be numerically approximated and bounded by a small $\\mathcal{O}(\\epsilon)$. The next step to fully recover the Jacobian is to take the gradient with respect to $v$ of the left-hand side of the equation. \n\n$$ \\nabla_v \\frac{d}{d \\epsilon} F(p_k + \\epsilon v) = J_k$$\n\nThe gradient with respect to $v$ can be traced through with automatic differentiation. So we apply a chain of operations, the `pytorch` Jacobian vector product, followed by backpropagation on a surrogate $v$ that was passed to the function to get the Jacobian of the forward model. The same principles can be used to recover $J_k^T$.\n\nThere is also the direct method that is avaible for computing the Jacobian matrix using the torch library. Both cases are shown below. Note that the tensors have a `requires_grad=True` flag set to allow for the gradients to be computed, it indicates that the tensor is part of the computational graph for backpropagation and tracing by how much each element of $v$ contributed to the `jvp` result.\n\nThe fundamental use of the `jvp` or the `vjp` is to compute the directional derivate or its transpose without computing the gradient with respect to $v$. This is because the jacobian matrix encodes the directional derivatives of the function at a point.\n\n$$d_k = J_k^T v$$\n\n::: {#jvp .cell execution_count=1}\n``` {.python .cell-code code-fold=\"false\"}\nimport torch\nfrom torch.autograd.functional import jvp\nfrom torch.autograd.functional import jacobian\n\n\n# Define a simple forward function\ndef F(p):\n    return torch.stack([p[0] ** 2 + p[1], p[1] ** 3 + p[0]])\n\n\n# Input point p_k\np_k = torch.tensor([1.0, 1.0])\n\n# Arbitrary vector v, same size as p_k\nv = torch.tensor([1.0, 1.0], requires_grad=True)\n\n# Compute the Jacobian-vector product (J(p) * v)\nF_output, jvp_result = jvp(F, (p_k,), v, create_graph=True)\nprint(\"Function output:\")\nprint(F_output)\nprint(\"Jacobian-vector product:\")\nprint(jvp_result)\n\n# Initialize a list to store each row of the Jacobian\njacobian_rows = []\n# Compute the gradient of each component of the JVP result separately, retaining the graph to avoid re-computation\nfor i in range(F_output.shape[0]):\n    v.grad = None  # Clear the gradient\n    jvp_result.backward(\n        torch.tensor([1.0 if i == j else 0.0 for j in range(F_output.shape[0])]),\n        retain_graph=True,\n    )\n    jacobian_rows.append(v.grad.clone())  # Append the gradient (row of the Jacobian)\n\n# Stack the rows to get the full Jacobian matrix\njacobian_matrix = torch.stack(jacobian_rows, dim=0)\n\n# Print the Jacobian matrix\nprint(\"Jacobian matrix at p_k:\")\nprint(jacobian_matrix)\n\n# Compute the full Jacobian matrix directly\njacobian_matrix = jacobian(F, p_k)\n\n# Print the Jacobian matrix\nprint(\"Jacobian matrix at p_k:\")\nprint(jacobian_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFunction output:\ntensor([2., 2.], grad_fn=<StackBackward0>)\nJacobian-vector product:\ntensor([3., 4.], grad_fn=<AddBackward0>)\nJacobian matrix at p_k:\ntensor([[2., 1.],\n        [1., 3.]])\nJacobian matrix at p_k:\ntensor([[2., 1.],\n        [1., 3.]])\n```\n:::\n:::\n\n\n# Fitting the Lotka-Volterra Model in PyTorch\n\nNow, all the previous theory can be combined to form a PyTorch training loop that will solve the non-linear least squares problem using the Gauss-Newton method, utilizing the conjugate gradient method to solve the normal equations involved. The data will be fit exclusively to the prey population of the Lotka-Volterra model. This is a simulation of a scenario where the predatory population is not observed and may be difficult to measure, but more reliable measurements are availble from the prey population.\n\nTo make the solution components easier to understand, they are separated into different class objects that contain the necessary components for each part of the solution. The main ingredients that will be required are:\n\n1. **ODE Integrator**\n    - Implements the Runge-Kutta 4th Order Method for numerically solving ordinary differential equations (ODEs).\n\n2. **Trainable Lotka-Volterra Model**\n    - A class that incorporates PyTorch's gradient tracking to enable training of the Lotka-Volterra model parameters.\n\n3. **Gauss-Newton Optimizer**\n    - A class designed to solve the non-linear least squares problem efficiently using the Gauss-Newton optimization technique.\n\n4. **Conjugate Gradient Descent Function**\n    - A function implemented to perform conjugate gradient descent, which is utilized to solve the normal equations arising in the Gauss-Newton method.\n\n---\n\n### RK4 and Lotka-Volterra Model\n\nThe Runge-Kutta 4th order method is a numerical solver for ODEs that is of higher order than the Euler method, reducing the error in the solution to $O(h^4)$. A more detailed description of the method can be found in the [Wikipedia article](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods).\n\nThe Lotka-Volterra model is implemented this time in PyTorch, but to run the custom optimization algorithm, it is better to avoid the object-oriented approach and use a functional form of the model. The model is defined as a function that takes the parameters and returns the population at the next time step. The model is also made time variant by adding a perturbation term to the parameters.\n\n::: {#cell-lotka-volterra .cell execution_count=2}\n``` {.python .cell-code}\nimport torch\nimport matplotlib.pyplot as plt\n\n\ndef runge_kutta_4(func, x0, params, time_horizon, time_steps):\n    dt = time_horizon / time_steps\n    X = [x0]\n    for i in range(time_steps):\n        x = X[-1]\n        k1 = func(x, params[i])\n        k2 = func(x + dt * k1 / 2, params[i])\n        k3 = func(x + dt * k2 / 2, params[i])\n        k4 = func(x + dt * k3, params[i])\n        X_next = x + (dt / 6) * (k1 + 2 * k2 + 2 * k3 + k4)\n        X.append(X_next)\n    return torch.stack(X, dim=1)\n\n\ndef lv_func(x, params):\n    alpha, beta, gamma, delta = params\n    dxdt = torch.zeros(2)\n    dxdt[0] = alpha * x[0] - beta * x[0] * x[1]  # Prey population change\n    dxdt[1] = -gamma * x[1] + delta * x[0] * x[1]  # Predator population change\n    return dxdt\n\n\ndef lotka_volterra(params, x0, T=10, nt=1000):\n    \"\"\"\n    Simulate the Lotka-Volterra model using the Runge-Kutta 4 method.\n\n    Parameters:\n    params (torch.Tensor): The parameters of the Lotka-Volterra model.\n    x0 (torch.Tensor): The initial population of prey and predators.\n    T (float): The time horizon of the simulation.\n    nt (int): The number of time steps to simulate.\n\n    Returns:\n    torch.Tensor: The population of prey and predators at each time step.\n\n    Notes:\n    The parameters should be in the order alpha, beta, gamma, delta.\n    They can either be fixed as [4,] or time-varying as [nt, 4].\n    \"\"\"\n\n    # Check if params has shape [4,] and expand to [nt, 4] if needed\n    if params.ndim == 1 and params.shape[0] == 4:\n        # Repeat params along the time dimension to make it [nt, 4]\n        params = params.unsqueeze(0).expand(nt, -1)\n    elif params.shape != (nt, 4):\n        raise ValueError(\"params must be either [4,] or [nt, 4]\")\n\n    # Proceed with the Runge-Kutta 4 integration\n    return runge_kutta_4(lv_func, x0, params, T, nt)\n\n\nperiod = 40.0  # Time horizon as a single float\nn_time_steps = 200\nparams = torch.tensor([2 / 3, 4 / 3, 1.0, 1.0], requires_grad=True)\ninitial_pop = torch.tensor([0.1, 1.0])\n\nsolution = lotka_volterra(params, initial_pop, T=period, nt=n_time_steps)\n\n# Plot the results\nplt.plot(solution[0].detach(), label=\"Prey\")\nplt.plot(solution[1].detach(), label=\"Predator\")\nplt.xlabel(\"Time Steps\")\nplt.ylabel(\"Population\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![The Lotka-Volterra model implemented in PyTorch.](index_files/figure-html/lotka-volterra-output-1.png){#lotka-volterra width=626 height=429}\n:::\n:::\n\n\nTo take the model a step further, it can be used to generate a toy dataset that will be used to fit the model parameters using the Gauss-Newton optimization method. To make a dataset that will not have a perfect fit, the time variant parameters and the pertubation variables are used to produce and interesting dataset. We define a function that can generate multiple realizations of the Lotka-Volterra model with perturbations. Then select the first realization to plot the time series and phase space of the model.\n\n::: {#4a182145 .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nfrom matplotlib.collections import LineCollection\nfrom torch.nn.functional import pad\n\n\ndef generate_data_set(\n    initial_pop=initial_pop, period=40.0, n_time_steps=2000, n_realizations=10\n):\n    pop_data_runs = []\n    perturbations = []\n\n    for run_idx in range(n_realizations):\n        print(f\"Computing realization {run_idx + 1}/{n_realizations}\")\n\n        # Generate noise for perturbing alpha across time steps\n        noise = torch.randn(\n            1, n_time_steps\n        )  # Shape [1, n_time_steps] for a single parameter over time\n        for _ in range(250):  # Smooth out the noise to resemble realistic fluctuations\n            noise = pad(noise, pad=(1, 1), mode=\"reflect\")\n            noise = (noise[:, :-2] + 2 * noise[:, 1:-1] + noise[:, 2:]) / 4\n        noise = noise.squeeze()  # Shape [n_time_steps]\n\n        # Base parameters without perturbation, as shape [n_time_steps, 4]\n        base_params = torch.tensor([4 / 3, 2 / 3, 1, 1]).expand(n_time_steps, 4)\n\n        # Apply perturbation to alpha (the first parameter)\n        params = base_params.clone()\n        params[:, 0] += noise  # Modify alpha over time\n\n        # Solve ODE with perturbed parameters\n        pop_data = lotka_volterra(params, initial_pop, T=period, nt=n_time_steps)\n\n        pop_data_runs.append(pop_data)\n        perturbations.append(noise)\n\n    return pop_data_runs, perturbations\n\n\ninitial_pop = torch.rand(2)\nXX, M = generate_data_set(\n    initial_pop=initial_pop, period=period, n_time_steps=n_time_steps, n_realizations=1\n)\n\nX = XX[0]\npert = M[0]\nd_true = X[0, :]  # Use the prey population as the data to fit\n\n# Time series plot\nplt.figure(figsize=(7.5, 4.5))\nplt.subplot(2, 1, 1)\nplt.plot(X[0, :].detach(), label=\"Prey\")\nplt.plot(X[1, :].detach(), label=\"Predator\")\nplt.plot(pert.detach(), label=\"Perturbation\")\nplt.legend()\nplt.title(\"Time Series\")\n\n# Phase space plot with color gradient\nplt.subplot(2, 1, 2)\n\n# Prepare data for LineCollection\nprey = X[0, :].detach().numpy()\npredator = X[1, :].detach().numpy()\npoints = np.array([prey, predator]).T.reshape(-1, 1, 2)\nsegments = np.concatenate([points[:-1], points[1:]], axis=1)\n\ncmap = \"viridis\"\n\n# Create a LineCollection with the chosen colormap\nlc = LineCollection(segments, cmap=cmap, norm=plt.Normalize(0, 1))\nlc.set_array(np.linspace(0, 1, len(segments)))  # Normalize color range to [0,1]\nlc.set_linewidth(2)\n\n# Add the LineCollection to the plot\nplt.gca().add_collection(lc)\n\n# Set plot limits to the data range\nplt.xlim(prey.min(), prey.max())\nplt.ylim(predator.min(), predator.max())\n\nplt.title(\"Phase Space with Time-Varying Color\")\nplt.xlabel(\"Prey Population\")\nplt.ylabel(\"Predator Population\")\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nComputing realization 1/1\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Time variant Lotka-Volterra model with perturbations.](index_files/figure-html/cell-4-output-2.png){width=710 height=422}\n:::\n:::\n\n\nAs can be seen by the data, the pertubations over time make the dynamics of the system only roughly periodic. This will make the optimization problem more interesting to solve.\n\n### Jacobian Vector Product and Directional Derivatives\n\nNow is a good time to code and check a working system to take the jacobian vector products that are required for the Gauss-Newton method using the functions defined earlier. We will assume that we have the prey data and we are trying to recover. To check the correctness of the coding, we can compare the results of the `jvp` and the `vjp` functions by checking using the adjoint. The value $\\langle w, J_k v \\rangle$ is scalar and so it should equal its transpose:\n$$ \\langle w, J_k v \\rangle = \\langle v, J_k^T w  \\rangle$$\n\nOne other thing to note is that both the `jvp` and the `vjp` functions will output the value of the function evaluated at the point that is passed to it, about which the jacobian is computed. So we get both $F(p)$ and $J_k v$ from the `jvp` function, and a similar result for the `vjp` function.\n\n::: {#jacobian-vector-product .cell execution_count=4}\n``` {.python .cell-code}\nfrom torch.autograd.functional import jvp, vjp\n\n# fix all parts of the problem except the parameters\ndef forward_model(params):\n    X = lotka_volterra(params, initial_pop, T=period, nt=n_time_steps)\n    prey = X[0, :]\n    return prey\n\n# set an initial guess for the parameters\nparams = torch.tensor([2 / 3, 4 / 3, 1.0, 1.0], requires_grad=True)\nv = torch.randn_like(params)\n\nd, q = jvp(forward_model, params, v)\n\nw = torch.randn_like(d)\nd, a = vjp(forward_model, params, w)\n\n# Check adjoint consistency\nprint(torch.sum(q * w), torch.sum(a * v))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor(-27.4492) tensor(-27.4493)\n```\n:::\n:::\n\n\n### Conjugate Gradient Descent and Gauss-Newton Optimizer\n\nThe Gauss-Newton method will need to make use of some important subfunctions to operate efficiently. One will be the computation of its components using the `jvp` and `vjp` functions, and the other will be the conjugate gradient descent method to solve the normal equations. \n\nTo implement this in code, we will also need to make a conjugate gradient solver for the problem\n$$ J_G(p_k)^T J_G(p_k)s_k = J_k^T r_k$$\n\nkeeping in mind that we want to avoid explicit computation of the entire jacobian when the goal is only to take a directional derivative. To do this the $J_G(p_k)^T J_G(p_k)$ operator can be coded as a single function `Hmv` that takes a vector and returns the product of the Hessian estimate with the vector. We then use this defined function in a standard implementation of the conjugate gradient method. The conjugate gradient method below has been setup to accept a callable funtion $A$ that acts like the matrix operator $A$, except we have bypassed the need to compute the full matrix, since we are only ever using it with a product.\n\n::: {#conjugate-gradient .cell execution_count=5}\n``` {.python .cell-code code-fold=\"false\"}\nfrom functools import partial\n\ndef Hmv(forProb, p, sk):\n    q = torch.autograd.functional.jvp(forProb, p, sk)[1]\n    a = torch.autograd.functional.vjp(forProb, p, q)[1]\n    return a\n\ndef conj_gradient(A, b, x0=None, niter=20, tol=1e-2, alpha=1e-2, verbose=True):\n    \"\"\"\n    Solve Ax = b using the conjugate gradient method.\n\n    Paramters:\n        A (callable): A function that computes the matrix-vector product Ax.\n        b (torch.Tensor): The right-hand side vector.\n        x0 (torch.Tensor, optional): The initial guess. Defaults to None.\n        niter (int, optional): Maximum number of iterations. Defaults to 20.\n        tol (float, optional): Tolerance for the residual. Defaults to 1e-2.\n        alpha (float, optional): Step size for the conjugate gradient method. Defaults to 1e-2.\n    \"\"\"\n    if x0 is None:\n        r = b\n    else:\n        r = b - A(x0)\n\n    q = r\n    x = torch.zeros_like(b)\n    for i in range(niter):\n        Hq = A(q)\n        alpha = (r * r).sum() / (q * Hq).sum()\n        x = x + alpha * q\n        rnew = r - alpha * Hq\n        beta = (rnew**2).sum() / (r**2).sum()\n        q = rnew + beta * q\n        r = rnew.clone()\n        if verbose:\n            print(\"iter = %3d    res = %3.2e\" % (i, r.norm() / b.norm()))\n        if r.norm() / b.norm() < tol:\n            break\n    return x\n\nA = partial(Hmv, forward_model, params)\nb = torch.autograd.functional.vjp(forward_model, params, d_true)[1]\n\nx = conj_gradient(A, b, niter=20, tol=1e-2, alpha=1e-2)\nprint(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\niter =   0    res = 1.15e+00\niter =   1    res = 5.48e-02\niter =   2    res = 1.47e-02\niter =   3    res = 2.79e-02\niter =   4    res = 3.56e-03\ntensor([-2.3992, -3.3430,  4.1509,  3.3726])\n```\n:::\n:::\n\n\n## Building the Gauss-Newton Optimizer\n\nRecall the algorithm for the Gauss-Newton method:\n\n```pseudocode\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"//\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\n\\begin{algorithm}\n\\caption{Gauss-Newton Algorithm for Non-linear Least Squares}\n\\begin{algorithmic}[1]\n\\State \\textbf{Input:} Initial guess $p_0$, maximum iterations $K$, tolerance $\\epsilon$\n\\State \\textbf{Initialize} $p_0$\n\\For{$k = 0, 1, 2, \\ldots$}\n    \\State Compute the Jacobian $J_G$ of $G(p)$ at $p_k$\n    \\State Compute the transpose $J_G^T$ of the Jacobian\n    \\State Compute the residual $r_k =G(p_k)$ (forward model)\n    \\State Compute the step $s_k = (J_G(p_k)^T J_G(p_k) )^{-1} J_G(p_k)^T r_k$\n    \\State Update the parameters $p_{k+1} = p_k + \\mu_k s_k$\n    \\If{$\\|s_k\\| < \\epsilon$}\n        \\State \\textbf{Stop}\n    \\EndIf\n\\EndFor\n\\State \\textbf{Output:} $p_{k+1}$ as the optimal solution\n\\end{algorithmic}\n\\end{algorithm}\n```\n\nThen combining all the previous stages of code we have:\n\n::: {#gauss-newton .cell execution_count=6}\n``` {.python .cell-code code-fold=\"false\"}\n# fix all parts of the problem except the parameters\ndef forward_model(params):\n    X = lotka_volterra(params, initial_pop, T=period, nt=n_time_steps)\n    prey = X[0, :]\n    return prey\n\n\ndef gauss_newton_solver(forward_model, p0, data, max_iter=100, tol=1e-6, mu=1, verbose=True):\n    \"\"\"\n    Solve a non-linear least squares problem using the Gauss-Newton method.\n\n    Parameters:\n        forward_model (callable): A function that computes the forward model.\n        p0 (torch.Tensor): The initial guess for the parameters.\n        data (torch.Tensor): The observed data to fit to.\n        max_iter (int): Maximum number of iterations. Defaults to 100.\n        tol (float): Tolerance for the residual. Defaults to 1e-6.\n        mu (float): Step size for the Gauss-Newton method. Defaults to 1.\n        verbose (bool): Whether to print iteration information. Defaults to True.\n    \"\"\"\n\n    predictions = []  # To store predictions at each iteration for animation\n    \n    params = p0\n    for i in range(max_iter):\n        # Compute residual\n        data_pred = forward_model(params)\n        rk = data - data_pred\n        \n        # Store the current predicted data for animation\n        predictions.append(data_pred.detach())\n        \n        # Compute parts for conjugate gradient\n        b = torch.autograd.functional.vjp(forward_model, params, rk)[1]\n        def A(sk):\n            q = torch.autograd.functional.jvp(forward_model, params, sk)[1]\n            a = torch.autograd.functional.vjp(forward_model, params, q)[1]\n            return a\n        s_k = conj_gradient(A, b, niter=20, tol=1e-2, alpha=1e-2, verbose=False)\n        \n        # Update parameters\n        params = params + mu * s_k\n        \n        # Check for convergence\n        if s_k.norm() < tol:\n            print(f'Converged in {i+1} iterations')\n            break\n        if verbose:\n            print(f'Iteration {i+1}/{max_iter}: Residual = {rk.norm().item()}')\n    \n    return params, predictions\n```\n:::\n\n\n### Testing the Gauss-Newton Optimizer\n\nA run of the Gauss-Newton optimization method can be performed on the Lotka-Volterra model to fit the prey population data. The optimization method will be run for a maximum of $40$ iterations with a tolerance that will exit early if the step size becomes small enough indicating a local minimum. The results of the optimization can be plotted against the true data, both prey and predator, to see how well the optimization method has performed to recover the missing predator population.\n\n::: {#cell-gauss-newton-test .cell execution_count=7}\n``` {.python .cell-code}\nperiod = 40.0  # Time horizon as a single float\nn_time_steps = 200\ninitial_pop = torch.rand(2)\n\n# Making a true data set to fit to\nXX, M = generate_data_set(\n    initial_pop=initial_pop, period=period, n_time_steps=n_time_steps, n_realizations=1\n)\nX = XX[0]\nd_true = X[0, :]  # Use the prey population as the data to fit\n\n# Start with an initial guess for the parameters\np0 = torch.tensor([1.7, 1.7, 0.7, 0.7], requires_grad=True)\n\n# Solve the problem\np_opt, predictions = gauss_newton_solver(\n    forward_model, p0, d_true, max_iter=45, tol=1e-4, mu=1e-1, verbose=False\n)\n\n# Make a final plot of the both pred prey true data and the predicted data\nX_hat = lotka_volterra(p_opt, initial_pop, T=period, nt=n_time_steps)\n\nplt.figure()\nplt.plot(X[0, :].detach().numpy(), label=\"True Prey Population\")\nplt.plot(X[1, :].detach().numpy(), label=\"True Predator Population\")\nplt.plot(X_hat[0, :].detach().numpy(), label=\"Predicted Prey Population\")\nplt.plot(X_hat[1, :].detach().numpy(), label=\"Predicted Predator Population\")\nplt.legend()\nplt.xlabel(\"Time Steps\")\nplt.ylabel(\"Population\")\nplt.title(\"True vs Predicted Population\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nComputing realization 1/1\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Testing the Gauss-Newton optimization method.](index_files/figure-html/gauss-newton-test-output-2.png){#gauss-newton-test width=622 height=449}\n:::\n:::\n\n\nThe plot outputs the successive iterations of the method and the data of the forward model as it is fitting in the predicitons tensor. The optimization process can be animated from the successive predictions to get a visual understanding of the optimization method.\n\n::: {#animation .cell execution_count=8}\n``` {.python .cell-code}\nfrom matplotlib.animation import FuncAnimation\n\n\ndef create_animation(true_data, predictions, filename=\"imgs/fitting_animation.gif\"):\n    fig, ax = plt.subplots()\n    (line1,) = ax.plot([], [], \"r-\", label=\"Predicted Fit\")\n    (line2,) = ax.plot([], [], \"b--\", label=\"True Data\")\n    ax.legend()\n\n    # Set titles and labels\n    ax.set_xlabel(\"Time Steps\")\n    ax.set_ylabel(\"Population\")\n\n    def init():\n        # Set x and y limits based on true_data and predictions\n        ax.set_xlim(0, len(true_data))\n        ax.set_ylim(\n            min(true_data.min(), predictions[0].min()) - 0.1,\n            max(true_data.max(), predictions[0].max()) + 0.1,\n        )\n        line2.set_data(\n            range(len(true_data)), true_data\n        )  # Set true data once, as it remains constant\n        ax.set_title(\"Iteration: 0\")  # Initial title for iteration count\n        return line1, line2\n\n    def update(i):\n        # Update predicted data and title with the current iteration count\n        line1.set_data(range(len(predictions[i])), predictions[i])\n        ax.set_title(f\"Iteration: {i + 1}\")\n        return line1, line2\n\n    # Create animation with updated frames\n    ani = FuncAnimation(fig, update, frames=len(predictions), init_func=init, blit=True)\n    ani.save(filename, writer=\"imagemagick\")\n\n\n# Create the animation\ncreate_animation(\n    d_true.cpu().detach().numpy(),\n    [pred.cpu().numpy() for pred in predictions],\n    \"imgs/fitting_animation.gif\",\n)\n```\n:::\n\n\n![](imgs/fitting_animation.gif){width=600px}\n\n## Extension to Time Varying Parameters\n\nAlthough the previous examples have been for a fixed set of parameters, it is entirely possible in natural systems that the parameters of the model are time dependent. The formulation of the Lotka-Volterra model has incorporated this design from the start by expanding the initial four parameters across the time dimension. However we can pass a full tensor of time varying parameters that is size $[nt, 4]$ to the model and the optimization algorithm. The rest of the code does not change at all since the PyTorch library can perform the required gradient computations on a 2D tensor as well.\n\nThe range of possible solutions and the dimensionality of the problem expands from $4$ parameters to $4 \\times nt$ parameters which means more parameters than there are actual data points. This means that any set of data could be fit perfectly, but it might not be the correct fit. This issue is a hallmark of ill-posed inverse problems. The optimization algorithm will still converge to a solution, but it might not be the correct one.\n\nSince the ground truth of both predator and prey populations is known, the optimization algorithm can be run with time dependent parameters which will allow more overfitting. The parameters being fixed in time is a sort of regularization that can applied to the problem, and removing it will change the results of the optimization.\n\n::: {#cell-time-varying-parameters .cell execution_count=9}\n``` {.python .cell-code code-fold=\"false\"}\n# Start with an initial guess for the parameters\np0 = torch.tensor([1.7, 1.7, 0.7, 0.7], requires_grad=True)\n# Extend p0 to repeat over the time steps with individual gradients\np0 = p0.unsqueeze(0).expand(n_time_steps, -1)\n\n# Solve the problem\np_opt, predictions = gauss_newton_solver(\n    forward_model, p0, d_true, max_iter=45, tol=1e-4, mu=1e-1, verbose=False\n)\n\n# Make a final plot of the both pred prey true data and the predicted data\nX_hat = lotka_volterra(p_opt, initial_pop, T=period, nt=n_time_steps)\n\nplt.figure()\nplt.plot(X[0, :].detach().numpy(), label=\"True Prey Population\")\nplt.plot(X[1, :].detach().numpy(), label=\"True Predator Population\")\nplt.plot(X_hat[0, :].detach().numpy(), label=\"Predicted Prey Population\")\nplt.plot(X_hat[1, :].detach().numpy(), label=\"Predicted Predator Population\")\nplt.legend()\nplt.xlabel(\"Time Steps\")\nplt.ylabel(\"Population\")\nplt.title(\"True vs Predicted Population\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Fitting the Lotka-Volterra model with time-varying parameters.](index_files/figure-html/time-varying-parameters-output-1.png){#time-varying-parameters width=622 height=449}\n:::\n:::\n\n\n\n\n![](imgs/fitting_animation_time_varying.gif){width=600px}\n\n## Conclusion\n\nThe Gauss-Newton optimization method is a powerful tool for solving non-linear least squares problems in a fast and efficient manner. It can be extended to any problem that is formulated as a vector of residuals or generally $\\| G(p) \\|^2$ that is to be optimized over $p$. Improved efficiency in the normal equations is done by using the Jacobian-vector product to bypass the costly need to compute a full Jacobian when all that is required is the directional derivative. The normal equtions also present a sub-problem in the optimization routine that can be solved using the conjugate gradient method to find the optimal step size $s_k$. This step direction is then used to perform the gradient descent step in the outer optimization algorithm. Increasing the complexity of the problem by allowing time varying parameters can lead to overfitting and ill-posedness, but the optimization algorithm will still converge to a solution.\n\n",
    "supporting": [
      "index_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}