{
  "hash": "cff9140c1d0c16d3f86a9dd593a2d580",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Lecture 7\"\nsubtitle: \"Homotopy Optimization\"\ndate: 2024-10-22\nauthor: \"Simon Ghyselincks\"\ndescription: >-\n    Gaussian homotopy is a technique that can be used to effectively broadcast the gradient of a non-convex function outward to help scape local minima.\ncategories:\n  - Optimization\n  - PyTorch\n  - Homotopy\n  - Fourier Transform\n\nbibliography: references.bib\nbiblatexoptions: \"style=numeric,sorting=nyt\"  # Uses numbered style and sorts by name-year-title\nbiblio-style: numeric-comp  # A numeric style in biblatex, similar to IEEE, with compressed citation ranges\n\nimage: \"imgs/gaussian_homotopy.gif\"\ndraft: false\n\neditor: \n  render-on-save: false\n\nfilters:\n  - pseudocode\n\npseudocode:\n  caption-prefix: \"Algorithm\"\n  reference-prefix: \"Algorithm\"\n  caption-number: true\n---\n\n::: {.hidden}\n$$\n\\def\\argmin{\\operatorname*{argmin}}\n\\def\\bmat#1{\\begin{bmatrix}#1\\end{bmatrix}}\n\\def\\Diag{\\mathbf{Diag}}\n\\def\\ip#1{\\langle #1 \\rangle}\n\n\\def\\maximize#1{\\displaystyle\\maxim_{#1}}\n\n\\def\\minimize#1{\\displaystyle\\minim_{#1}}\n\\def\\norm#1{\\|#1\\|}\n\\def\\proj{\\mathbf{proj}}\n\\def\\R{\\mathbb R}\n\\def\\Re{\\mathbb R}\n\\def\\Rn{\\R^n}\n\\def\\rank{\\mathbf{rank}}\n\\def\\range{{\\mathbf{range}}}\n\\def\\span{{\\mathbf{span}}}\n\\def\\textt#1{\\quad\\text{#1}\\quad}\n\\def\\trace{\\mathbf{trace}}\n\\def\\bf#1{\\mathbf{#1}}\n$$\n:::\n\n\n\n## Motivation\n\nSo far we have examined optimization techniques using gradient descent and the Gauss-Newton method. These methods are powerful but can be limited by the presence of local minima in the optimization landscape. In this lecture we will explore a technique called Gaussian homotopy that can be used to escape local minima in optimization problems.\n\nTo recap the steps used so far in optimization, we have an objective \n$$\\argmin f(x),$$\n\nwhere $x \\in \\mathbb{R}^n$ is an unconstrained optimization variable. The objective can be searched out by stepping in a direction itertively, in general:\n$$x_{k+1} = x_k - \\alpha_k H \\nabla f(x_k),$$\n\nwhere $\\alpha_k$ is the step size. The gradient $\\nabla f(x_k)$ can be computed explicitly or using automatic differentiation. The matrix $H$ is a modifier that depends on the method being used:\n$$H = \n\\begin{cases}\n    I & \\text{Gradient Descent} \\\\\n    (J^T J)^{-1} & \\text{Gauss-Newton}\n\\end{cases}\n$$\n\nHowever, optimization is often performed on non-convex functions, in which case the path to a global minimum can be obstructed by local minima. Three categories of increasingly non-convex functions are shown below.\n\n::: {#fig:function-classes .figure}\n![Three Categories of Increasingly Non-Convex Functions](./path1.svg){width=50% style=\"border: 2px solid #000; padding: 20px; display: block; margin-left: auto; margin-right: auto;\"}\n**Figure:** Three categories of increasingly non-convex functions illustrating potential local minima that can obstruct the path to a global minimum.\n:::\n\nSome examples for each of the three catergories are given in the following table:\n\n| Category | Function | Local Minima |\n|----------|----------|--------------|\n| Convex | $f(x) = x^2$ | Global minimum at $x=0$ |\n| Non-Convex but $f'(x)<0$ | $f(x) = -\\mathcal{N}(x; 0, 1)$ | Global minimum at $x=0$ |\n| Non-Convex with $f'(x) \\geq 0$ | $f(A,B,w) = w^T \\sigma (B \\sigma (A x))$ | Multiple local minima |\n| Non-Convex and Poorly Conditioned $\\nabla^2 f(x)$ | $f(t) = x(t)^T A x(t), \\quad x(t) = \\text{square wave}$ | Multiple local minima and discontinuous |\n\nTo illustrate these functions even more we can plot them as well.\n\n::: {#cell-function-plot .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-5, 5, 100)\ny1 = x**2\ny2 = -np.exp(-x**2)\ny3 = np.sin(x) + .5*x\n\n#square wave\ndef square_wave(x):\n    return 1 if np.sin(3*x) > 0 else 0\n\ny4 = [square_wave(xi)**2 for xi in x]\n\nfig, ax = plt.subplots(2, 2)\nax[0, 0].plot(x, y1)\nax[0, 0].set_title(\"Convex: $f(x) = x^2$\")\nax[0, 1].plot(x, y2)\nax[0, 1].set_title(\"Non-Convex but $f'(x)<0$ \\n $f(x) = -\\mathcal{N}(x; 0, 1)$\")\nax[1, 0].plot(x, y3)\nax[1, 0].set_title(\"Non-Convex with $f'(x) \\geq 0$ \\n $f(x) = sin(x)+.5 x$\")\nax[1, 1].plot(x, y4)\nax[1, 1].set_title(\"Non-Convex and Poorly Conditioned $\\nabla^2 f(x)$\")\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Function Categories.](index_files/figure-html/function-plot-output-1.png){#function-plot width=710 height=467}\n:::\n:::\n\n\n## Direct Search Methods\n\nA direct search [@wikipedia_directsearch] can be performed to try to find the global minimum of a non-convex function [@Lewis2000].\n\n$$ x_{k+1} = x_k + \\alpha_k d_k, \\quad d_k \\in \\mathbb{R}^n.$$\n\nIn this case the direction does not follow the gradient descent rule, there could be a stochastic element. The general algorithms that implement this will have the property that the step size decreases over time such that \n\n$$ \\| \\alpha_k d_k \\| \\to 0, \\ k \\to \\infty$$\n\nThe implementation ignores seeking information about the gradient or the Hessian of the function. Instead some points in the surrounding region are computed and the most optimal decrease for the next step is selected. \n\nThe method has been known since the 1950s but it fell out of favour due to the slow rate of convergence. However, with parallel computing advances it has become more feasible to use again. For a large set of direct search methods, it is possible to rigorously prove that they will converge to a local minimum [@Kolda2003].\n\n## Homotopy\n\nIn mathematics, homotopy refers to the continuous transformation of one function into another. In optimization, homotopy—or continuation optimization—is used to transform a highly non-convex function into a simpler, often more convex surrogate function. This approach enables the optimizer to escape local minima and approach a global minimum by incrementally tackling easier, intermediate optimization problems.\n\nThe core idea behind homotopy optimization is to relax a difficult optimization problem into a series of smoother problems that gradually resemble the original objective function. This relaxation process spreads the gradient and Hessian information outward, making the function landscape easier to navigate and minimizing the risk of getting stuck in local minima.\n\nThis can be accomplished using a convolution with a simple function as a kernel. The kernel that is used can have variable width and parameterization, there are varying degrees of relaxation which can be parameterized using a $t$ time variable. As $t \\to 0$ the function becomes more like the original function, and as $t \\to 1$ the function becomes more like the smoothing function. The homotopy between the two is parameterized by $t \\in [0, 1]$.\n\n<div style=\"text-align: center; margin: 20px 0;\">\n  <a title=\"Jim.belk, CC0, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:HomotopySmall.gif\">\n    <img width=\"300\" alt=\"A continuous deformation of a path\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7e/HomotopySmall.gif?20110614214259\">\n  </a>\n  <p style=\"font-size: 0.9em; color: gray;\">\n    A continuous deformation of a path. Source: <a href=\"https://commons.wikimedia.org/wiki/File:HomotopySmall.gif\" title=\"Jim.belk, CC0, via Wikimedia Commons\">Wikimedia Commons</a>\n  </p>\n</div>\n\n### Homotopy Optimization\n\nIn the case of optimization, the technique is know as homotopy optimization or continuation optimization. The optimization process starts with the smoothed function and then gradually moves back to the original function using the optimization steps from the relaxed prbolem. This is known as a continuation method, and the scheduling of the homotopy parameter $t$ is the continutation schedule. A summary description with more details and advanced techniques can be found in work by Lin et. al [@Lin2023].\n\n#### Example\nLet $f(x)$ be the original function and $g(x)$ be the smoothing function. The homotopy function $h(x, t)$ can be defined as and interpolation between the two functions:\n\n$$h(x, t) = (1-t) f(x) + t g(x).$$\n\nThis new function has the important property that $h(x,0) = f(x)$ and $h(x,1) = g(x)$ so it represents a continuous path of deformation between the two functions, beginning at $t=1$ with a simpler relaxed problem and ending at $t=0$ with the original problem.\n\nThe new minimization problem becomes:\n\n$$\\argmin_{\\mathcal{X}} h(x, t).$$\n\nA schedule can be set up for the times so that a series of times $\\{t_0, t_1, \\dots t_k, \\ldots, t_n\\}$ are used to solve the problem. We solve at $t_0 = 1$ and then gradually decrease the value of $t$ to $0$. The solution $x_k$ at $t_{k}$ is used as the starting point for the next iteration $t_{k+1}$ until reaching $t_n = 0$.\n\nIn the case where the values of $x$ may be constrained, this becomes similar to the Interior Point Method, where the constraints are relaxed and then gradually tightened.\n\n## Gaussian Homotopy\n\nA common case of homotopy is the Gaussian homotopy, where the smoothing function is a Gaussian function. The Gaussian function is a widely used in signal processing and image processing due to its properties as a low-pass filter. For example, a Gaussian blur is applied to images to aid in downsampling since it preserves the lower resolution details while removing high-frequency noise that may cause aliasing.\n\nTo illustrate the low-pass filtering property, consider a Gaussian function $g(x)$ and its Fourier transform $\\hat{g}(k)$:\n\n$$g(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{x^2}{2\\sigma^2}}, \\quad \\hat{g}(k) = e^{-\\frac{k^2 \\sigma^2}{2}}.$$\n\nThe Fourier transform of the Gaussian is another Gaussian, it is an eigenfunction of the Fourier transform operator. The convolution theorem states that the convolution of two functions in the spatial domain is equivalent to the multiplication of their Fourier transforms in the frequency domain:\n\n$$f(x) \\ast g(x) = \\mathcal{F}^{-1} \\left[ \\hat{f}(k) \\hat{g}(k) \\right].$$\n\nThe convolution of a function $f(x)$ with a Gaussian $g(x)$ can be used to remove the high-frequency components of the function while allowing the low-frequency, widely spread components to remain. \n\nA wide Gaussian in the time domain corresponds to a narrow Gaussian in the frequency domain, and vice versa. So a wide gaussian only lets through the lowest frequencies, while a narrow Gaussian lets through the highest frequencies. At the limit as the Gaussian becomes infinitely narrow, it becomes a delta function $g(x) = \\delta(x)$ in the time domain, a constant function $\\hat{g}(k) = 1$ in the frequency domain. The convolution of a delta function with a function $f$ is the function itself, so the delta function does not change the function. The multiplication of the function $\\hat f(k)$ with the constant function $1$ is the function itself, so the constant function does not change the function in the frequency domain.\n\n::: {#gaussian-homotopy .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.fftpack import fft, ifft, fftfreq, fftshift\nfrom matplotlib.animation import FuncAnimation\n\n# Generate a square wave in the time domain\ndef square_wave(t, period=1.0):\n    \"\"\"Creates a square wave with a given period.\"\"\"\n    return np.where(np.sin(2 * np.pi * t / period) >= 0, 1, -1)\n\n# Time domain setup\nt = np.linspace(-1, 1, 500)\nsquare_wave_signal = square_wave(t)\nfreq = fftfreq(t.size, d=(t[1] - t[0]))\nsquare_wave_fft = fft(square_wave_signal)\n\n# Function to update the plot for each frame, based on current sigma_t\ndef update_plot(sigma_t, axs, time_text):\n    sigma_f = 1 / (2 * np.pi * sigma_t)  # Standard deviation in frequency domain\n    gaussian_time_domain = np.exp(-0.5 * (t / sigma_t)**2)\n    gaussian_filter = np.exp(-0.5 * (freq / sigma_f)**2)\n    filtered_fft = square_wave_fft * gaussian_filter\n    smoothed_signal = np.real(ifft(filtered_fft))\n\n    # Update each subplot with new data\n    axs[1, 0].clear()\n    axs[1, 0].plot(t, gaussian_time_domain, color=\"purple\")\n    axs[1, 0].set_title(\"Gaussian Filter (Time Domain)\")\n    axs[1, 0].set_xlabel(\"Time\")\n    axs[1, 0].set_ylabel(\"Amplitude\")\n    axs[1, 0].grid(True)\n\n    axs[1, 1].clear()\n    axs[1, 1].plot(fftshift(freq), fftshift(gaussian_filter), color=\"orange\")\n    axs[1, 1].set_title(\"Gaussian Low-Pass Filter (Frequency Domain)\")\n    axs[1, 1].set_xlabel(\"Frequency\")\n    axs[1, 1].set_ylabel(\"Amplitude\")\n    axs[1, 1].set_xlim(-50, 50)\n    axs[1, 1].grid(True)\n\n    axs[2, 0].clear()\n    axs[2, 0].plot(t, square_wave_signal, color=\"green\", linestyle=\"--\")\n    axs[2, 0].plot(t, smoothed_signal, color=\"red\")\n    axs[2, 0].set_title(\"Smoothed Signal (Time Domain)\")\n    axs[2, 0].set_xlabel(\"Time\")\n    axs[2, 0].set_ylabel(\"Amplitude\")\n    axs[2, 0].grid(True)\n\n    axs[2, 1].clear()\n    axs[2, 1].plot(fftshift(freq), fftshift(np.abs(square_wave_fft)), color='blue', linestyle='--')\n    axs[2, 1].plot(fftshift(freq), fftshift(np.abs(filtered_fft)), color=\"orange\")\n    axs[2, 1].set_title(\"Filtered Spectrum (Frequency Domain)\")\n    axs[2, 1].set_xlabel(\"Frequency\")\n    axs[2, 1].set_ylabel(\"Amplitude\")\n    axs[2, 1].set_xlim(-30, 30)\n    axs[2, 1].grid(True)\n\n    # Update the time text\n    time_text.set_text(f\"T = {T:.2f}\")\n\n# Initialize the figure and plot layout for animation\nfig, axs = plt.subplots(3, 2, figsize=(12, 10))  # Increased figure size for animation\naxs[0, 0].plot(t, square_wave_signal, color='green')\naxs[0, 0].set_title(\"Square Wave (Time Domain)\")\naxs[0, 0].set_xlabel(\"Time\")\naxs[0, 0].set_ylabel(\"Amplitude\")\naxs[0, 0].grid(True)\n\naxs[0, 1].plot(fftshift(freq), fftshift(np.abs(square_wave_fft)), color='blue')\naxs[0, 1].set_title(\"Fourier Transform of Square Wave (Frequency Domain)\")\naxs[0, 1].set_xlabel(\"Frequency\")\naxs[0, 1].set_ylabel(\"Amplitude\")\naxs[0, 1].set_xlim(-30, 30)\naxs[0, 1].grid(True)\n\n# Add a text annotation for time at the bottom of the figure with extra space\ntime_text = fig.text(0.5, 0.02, \"\", ha=\"center\", fontsize=12)  # Adjusted position for clarity\n\n# Adjust subplot spacing specifically for animation\nplt.tight_layout(rect=[0, 0.05, 1, 1])  # Extra space at the bottom for time text\n\n# Animation settings\nsteps = 100\ndef animate(frame):\n    global T  # Declare T as a global variable for use in update_plot\n    T = 1 - (frame / steps)  # Scale frame number to a T value between 1 and 0\n    sigma_t = 0.5 * T + 0.001 * (1 - T)  # Interpolate sigma_t\n    update_plot(sigma_t, axs, time_text)\n\n# Create and display the animation\nani = FuncAnimation(fig, animate, frames=steps, interval=100)\n\n# Save the animation as a GIF\nani.save('imgs/gaussian_homotopy.gif', writer='imagemagick', fps=8)\n\nplt.show()\n```\n:::\n\n\n![](imgs/gaussian_homotopy.gif)\n\nFor Gaussian homotopy, the continuous transformation between the original function and the relaxed version is given by a convolution with a Gaussian kernel. We let $\\sigma(t)$ be the standard deviation of the Gaussian kernel at time $t$. The deviation will be $\\sigma(0) = 0$ and $\\sigma(1) = \\sigma_{\\text{max}}$ so that the homotopy at any time $t$ is given by:\n\n$$h(x, t) = \\int_{-\\infty}^{\\infty} f(x-\\xi) \\exp(-\\frac{\\xi^2}{\\sigma(t)^2}) d\\xi.$$\n\nThe Gaussian kernel should be divided by its partition function $z(t) = \\int \\exp(-\\frac{\\xi^2}{\\sigma(t)^2}) d\\xi$ in theory so that the kernel is normalized, but for the use case where $h(x, t)$ is used as a surrogate function for optimization, the partition function $z(t)$ does not change the minimizer of the function.\n\n### Stochastic Optimization\n\nSince the objective is to minimize over the integral given by $h(x, t)$, a stochastic method can be used to estimate the minimizer in expectation using Monte Carlo methods. The integral can be approximated by sampling $N$ points from the Gaussian kernel and averaging the function values at those points:\n\n$$\n\\begin{align*}\nh(x, t) &= \\int_{-\\infty}^{\\infty} f(x-\\xi) \\exp(-\\frac{\\xi^2}{\\sigma(t)^2}) d\\xi\\\\\n& = \\mathbb{E}_{\\xi \\sim \\mathcal{N}(0, \\sigma(t)^2)} f(x-\\xi) \\\\\n& \\approx \\frac{1}{N} \\sum_{i=1}^N f(x-\\xi_i), \\quad \\xi_i \\sim \\mathcal{N}(0, \\sigma(t)^2). \n\\end{align*}\n$$\n\nFor a given $t$ and point $x$ where we want to estimate the function $h(x, t)$, we can sample $N$ points from a Gaussian kernel centered at $x$ with standard deviation $\\sigma(t)$ and evaluate the function at those points. The average of the function values at those points will be an estimate of the function value at $x$.\n\n### Implementation Choices\n\nThere are two ways to approach this problem when using numerical methods.\n\n1. **Discretize then Optimize:**  The integral is first discretized by choosing a set of points $\\{xi_1, \\xi_2, \\ldots, \\xi_N\\}$ and then the function is evaluated using that same discrete kernel across all points:\n\n$$ \\min_{\\mathcal{X}} \\frac{1}{N} \\sum_{i=1}^N f(x-\\xi_i).$$\n\nThis sum can end up being large if there are many points that are selected.\n\n2. **Optimize then Discretize:** In this case we start with gradient descent and the continuous function $h(x, t)$ and then sample the function at the points $\\{xi_1, \\xi_2, \\ldots, \\xi_N\\}$ to estimate a gradient. \n\n$$\n\\begin{align*}\n x_{k+1} &= x_k - \\alpha_k \\mathbb{E}_{\\xi} \\nabla f(x_k - \\xi)\\\\\n  &\\approx x_k - \\alpha_k \\frac{1}{N} \\sum_{i=1}^N \\nabla f(x_k - \\xi_i).\n\\end{align*}\n$$\n\nThis formulation can technically converge even with $| i | = 1$ but with very slow convergence.\n\n---\n\nNow that the Gaussian homotopy has been introduced, we can move on to the implementation of the algorithm.\n\n# Code Implementation\n\nAs usual for studying a problem we come up with a suitable toy dataset. In this case it should be a function that has multiple minima, is non-convex, and has a poorly conditioned Hessian.\n\n$$ f(x) = -\\exp\\left(-\\frac{(x_1 - 2)^2 + (x_2 - 2)^2}{0.1}\\right) - 2\\exp\\left(-\\frac{(x_1 + 2)^2 + (x_2 + 2)^2}{0.1}\\right).$$\n\nThe function is two superimposed Gaussian functions that impose a local minimum at $(2, 2)$ and $(-2, -2)$ but with very little gradient information far away from the minima.\n\n::: {#cell-objective-function .cell execution_count=4}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport torch\n\ndef objective_function(X):\n    \"\"\"\n    A 2D function with multiple local minima.\n\n    Parameters:\n    X (torch.Tensor): A tensor of shape (N, 2) containing the input points.\n    \"\"\"\n    x1 = X[:, 0]\n    x2 = X[:, 1]\n    y = -torch.exp(-((x1 - 2)**2 + (x2 - 2)**2) / 0.1) \\\n        - 2 * torch.exp(-((x1 + 2)**2 + (x2 + 2)**2) / 0.1)\n    return y\n\n# plot the function\nx1 = torch.linspace(-5, 5, 100)\nx2 = torch.linspace(-5, 5, 100)\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\nX = torch.stack([X1.flatten(), X2.flatten()], dim=1)\ny = objective_function(X).reshape(100, 100)\nplt.contourf(X1, X2, y, levels=100)\nplt.colorbar()\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.title('Objective function')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Objective Function of Two Gaussians.](index_files/figure-html/objective-function-output-1.png){#objective-function width=601 height=449}\n:::\n:::\n\n\nThe next step is to create a time parameterized function $h(x, t)$ that is the Gaussian homotopy of f(x). In practical terms, the starting $\\sigma_{\\text{max}}$ is being modified by a $t$ parameter when multiplied by the random noise. At $t=0$ the function is the original function, and at $t=1$ the function is being convolved numerically with a Gaussian kernel that is $\\mathcal{N}(0, \\sigma_{\\text{max}})$. The time points in between are a homotopy between the two functions.\n\n::: {#0b5be197 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"false\"}\ndef gaussian_homotopy(func, x, batch_size=1000, sigma=1.0, t=0.5):\n    \"\"\"\n    Computes the Gaussian homotopy function h(x, t) using Monte Carlo approximation.\n\n    Args:\n        func: The original objective function to be optimized.\n        x: A tensor of shape (N, D), where N is the number of points and D is the dimension.\n        batch_size: Number of samples to use in Monte Carlo approximation.\n        sigma: Standard deviation of the Gaussian kernel.\n        t: Homotopy parameter, varies from 0 (original function) to 1 (smoothed function).\n\n    Returns:\n        y: A tensor of shape (N,), the approximated h(x, t) values at each point x.\n    \"\"\"\n    N, D = x.shape\n    \n    # Sample from the t=1 gaussian kernel\n    kernel = torch.randn(batch_size, D) * sigma\n    \n    # Repeat x and z to compute all combinations\n    x_repeated = x.unsqueeze(1).repeat(1, batch_size, 1).view(-1, D)\n    kernel_repeated = kernel.repeat(N, 1)\n    \n    # Compute the monte carlo set of points surrounding each x\n    x_input = x_repeated - t * kernel_repeated\n    \n    # Evaluate the function at the sampled points\n    y_input = func(x_input)\n    \n    # Reshape and average over the batch size to approximate the expectation\n    y = y_input.view(N, batch_size).mean(dim=1)\n    return y\n```\n:::\n\n\nThis variation of the function can be seen as having extra parameters:\n\n$$h(x,t,\\sigma_{\\text{max}}, N) = \\frac{1}{N} \\sum_{i=1}^N f(x-\\xi_i), \\quad \\xi_i \\sim \\mathcal{N}(0, t \\cdot \\sigma_{\\text{max}}).$$\n\nThe time parameter is modulating the standard deviation of the Gaussian kernel, and the number of samples $N$ is used to approximate the expectation of the function at each point. As time goes to zero we approach the original function. Now applying an animation to this it is possible to see the stochastic homotopy in action.\n\n::: {#homotopy-animation .cell execution_count=6}\n``` {.python .cell-code}\n# Create grid points\nt_values = torch.linspace(-8, 8, 129)\nx_grid, y_grid = torch.meshgrid(t_values, t_values, indexing=\"ij\")\nX = torch.stack([x_grid.flatten(), y_grid.flatten()], dim=1)\n\n# Define the range of homotopy parameters\nT = torch.linspace(1.0, 0.0, steps=30)\n\n# Initialize figure\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\ndef update(i):\n    ax1.clear()\n    ax2.clear()\n\n    t = T[i]\n    y_original = objective_function(X).view(129, 129).detach().numpy()\n    y_homotopy = gaussian_homotopy(objective_function, X, batch_size=12000, sigma=4.0, t=t).view(129, 129).detach().numpy()\n\n    ax1.contourf(t_values.numpy(), t_values.numpy(), y_original, levels=50, cmap=\"viridis\")\n    ax1.set_title(\"Original Function\")\n    ax1.set_xlabel(\"x1\")\n    ax1.set_ylabel(\"x2\")\n\n    ax2.contourf(t_values.numpy(), t_values.numpy(), y_homotopy, levels=50, cmap=\"viridis\")\n    ax2.set_title(f\"Homotopy Function (t = {t:.2f})\")\n    ax2.set_xlabel(\"x1\")\n    ax2.set_ylabel(\"x2\")\n\n    plt.tight_layout()\n\n# Create animation\nani = FuncAnimation(fig, update, frames=len(T), interval=200)\n\n# Save as GIF\nani.save(\"imgs/homotopy_2d.gif\", writer=\"imagemagick\", fps=4)\n```\n:::\n\n\n![](imgs/homotopy_2d.gif){width=100%}\n\n## Conclusion\n\nThis lecture has covered the theory behind homotopy, its action in the frequency and time domain, and its purpose in optimization. A scheme for continuation scheduling for the optimization along with the homotopy is a field of active research. A more detailed analysis to implement the technique in a practical setting can be understood from Lin et. al [@Lin2023].\n\n",
    "supporting": [
      "index_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}