{
  "hash": "43cd8c13d08827e7a47f9005fac367d3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Lecture 7: Applying Homotopy to Optimize Highly Non-Convex Functions\"\nsubtitle: \"A look at how Gaussian homotopy can be used to escape local minima in optimization problems.\"\ndate: 2024-10-22\nauthor: \"Simon Ghyselincks\"\ndescription: >-\n    Gaussian homotopy is a technique that can be used to effectively broadcast the gradient of a non-convex function outward to help escape local minima.\ncategories:\n  - Optimization\n  - Gauss-Newton\n  - Automatic Differentiation\n  - PyTorch\n\nimage: false\ndraft: false\n\neditor: \n  render-on-save: false\n\nfilters:\n  - pseudocode\n\npseudocode:\n  caption-prefix: \"Algorithm\"\n  reference-prefix: \"Algorithm\"\n  caption-number: true\n---\n\n\n\n::: {.hidden}\n$$\n\\def\\argmin{\\operatorname*{argmin}}\n\\def\\bmat#1{\\begin{bmatrix}#1\\end{bmatrix}}\n\\def\\Diag{\\mathbf{Diag}}\n\\def\\ip#1{\\langle #1 \\rangle}\n\n\\def\\maximize#1{\\displaystyle\\maxim_{#1}}\n\n\\def\\minimize#1{\\displaystyle\\minim_{#1}}\n\\def\\norm#1{\\|#1\\|}\n\\def\\proj{\\mathbf{proj}}\n\\def\\R{\\mathbb R}\n\\def\\Re{\\mathbb R}\n\\def\\Rn{\\R^n}\n\\def\\rank{\\mathbf{rank}}\n\\def\\range{{\\mathbf{range}}}\n\\def\\span{{\\mathbf{span}}}\n\\def\\textt#1{\\quad\\text{#1}\\quad}\n\\def\\trace{\\mathbf{trace}}\n\\def\\bf#1{\\mathbf{#1}}\n$$\n:::\n\n\n\n# Motivation\n\nSo far we have examined optimization techniques using gradient descent and the Gauss-Newton method. These methods are powerful but can be limited by the presence of local minima in the optimization landscape. In this lecture we will explore a technique called Gaussian homotopy that can be used to escape local minima in optimization problems.\n\nTo recap the steps used so far in optimization, we have an objective \n\n$$\\argmin f(x),$$\n\nwhere $x \\in \\mathbb{R}^n$ is an unconstrained optimization variable. The objective can be searched out by stepping in a direction itertively, in general:\n\n$$x_{k+1} = x_k - \\alpha_k H \\nabla f(x_k),$$\n\nwhere $\\alpha_k$ is the step size. The gradient $\\nabla f(x_k)$ can be computed explicitly or using automatic differentiation. The matrix $H$ is a modifier that depends on the method being used:\n\n$$H = \\begin{cases}\n    I & \\text{Gradient Descent} \\\\\n    (J^T J)^{-1} & \\text{Gauss-Newton}\n\\end{cases}$$\n\nHowever, optimization is often performed on non-convex functions, in which case the path to a global minimum can be obstructed by local minima. Three categories of increasingly non-convex functions are shown below.\n\n::: {#fig:function-classes .figure}\n![Three Categories of Increasingly Non-Convex Functions](./path1.svg){width=50% style=\"border: 2px solid #000; padding: 20px; display: block; margin-left: auto; margin-right: auto;\"}\n**Figure:** Three categories of increasingly non-convex functions illustrating potential local minima that can obstruct the path to a global minimum.\n:::\n\nSome examples for each of the three catergories are given in the following table:\n\n| Category | Function | Local Minima |\n|----------|----------|--------------|\n| Convex | $f(x) = x^2$ | Global minimum at $x=0$ |\n| Non-Convex but $f'(x)<0$ | $f(x) = -\\mathcal{N}(x; 0, 1)$ | Global minimum at $x=0$ |\n| Non-Convex with $f'(x) \\geq 0$ | $f(A,B,w) = w^T \\sigma (B \\sigma (A x))$ | Multiple local minima |\n| Non-Convex and Poorly Conditioned $\\nabla^2 f(x)$ | $f(t) = x(t)^T A x(t), \\quad x(t) = \\text{square wave}$ | Multiple local minima and discontinuous |\n\nTo illustrate these functions even more we can plot them as well.\n\n::: {#cell-function-plot .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-5, 5, 100)\ny1 = x**2\ny2 = -np.exp(-x**2)\ny3 = np.sin(x) + .5*x\n\n#square wave\ndef square_wave(x):\n    return 1 if np.sin(3*x) > 0 else 0\n\ny4 = [square_wave(xi)**2 for xi in x]\n\nfig, ax = plt.subplots(2, 2)\nax[0, 0].plot(x, y1)\nax[0, 0].set_title(\"Convex: $f(x) = x^2$\")\nax[0, 1].plot(x, y2)\nax[0, 1].set_title(\"Non-Convex but $f'(x)<0$ \\n $f(x) = -\\mathcal{N}(x; 0, 1)$\")\nax[1, 0].plot(x, y3)\nax[1, 0].set_title(\"Non-Convex with $f'(x) \\geq 0$ \\n $f(x) = sin(x)+.5 x$\")\nax[1, 1].plot(x, y4)\nax[1, 1].set_title(\"Non-Convex and Poorly Conditioned $\\nabla^2 f(x)$\")\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Function Categories.](index_files/figure-html/function-plot-output-1.png){#function-plot width=710 height=467}\n:::\n:::\n\n\n## Direct Search Methods\n\nA direct search can be performed to try to find the global minimum of a non-convex function.\n\n$$ x_{k+1} = x_k + \\alpha_k d_k, \\quad d_k \\in \\mathbb{R}^n.$$\n\nIn this case the direction might not follow the gradient descent rule, there could be a stochastic element. The general algorithms that implement this will have the property that the step size decreases over time such that \n\n$$ \\| \\alpha_k d_k \\| \\to 0, \\ k \\to \\infty$$\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}