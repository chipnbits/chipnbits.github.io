{
  "hash": "ddc9012373d732a66261d473c3768c35",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Lecture 8\"\nsubtitle: \"Regularization and Priors in Inverse Problems\"\ndate: 2024-11-03\nauthor: \"Simon Ghyselincks\"\ndescription: >-\n    Inverse problems often have multiple or infinite solutions. Regularization and priors can be used to constrain the solution space and find a unique solution. This lecture covers the role of regularization and priors in solving inverse problems.\ncategories:\n  - Probability\n  - Bayesian Inference\n\nimage: \"imgs/regularization_effects.png\"\ndraft: false\n\neditor: \n  render-on-save: false\n\nfilters:\n  - pseudocode\n\npseudocode:\n  caption-prefix: \"Algorithm\"\n  reference-prefix: \"Algorithm\"\n  caption-number: true\n---\n\n::: {.hidden}\n$$\n\\def\\argmin{\\operatorname*{argmin}}\n\\def\\bmat#1{\\begin{bmatrix}#1\\end{bmatrix}}\n\\def\\Diag{\\mathbf{Diag}}\n\\def\\ip#1{\\langle #1 \\rangle}\n\n\\def\\maximize#1{\\displaystyle\\maxim_{#1}}\n\n\\def\\minimize#1{\\displaystyle\\minim_{#1}}\n\\def\\norm#1{\\|#1\\|}\n\\def\\proj{\\mathbf{proj}}\n\\def\\R{\\mathbb R}\n\\def\\Re{\\mathbb R}\n\\def\\Rn{\\R^n}\n\\def\\rank{\\mathbf{rank}}\n\\def\\range{{\\mathbf{range}}}\n\\def\\span{{\\mathbf{span}}}\n\\def\\textt#1{\\quad\\text{#1}\\quad}\n\\def\\trace{\\mathbf{trace}}\n\\def\\bf#1{\\mathbf{#1}}\n$$\n:::\n\n\n\n## Motivation\n\nRevisiting the structure of some inverse problems we have seen so far we generalize the problem as follows. We have some model:\n$$u(x) \\rightarrow \\text{model}$$\n\nand also a forward process that may not be perfect or have some noise in it that produces the observed data:\n\n$$ A(u(x)) + \\epsilon = b \\rightarrow \\text{data}$$\n\nwhere the goal is to recover the model $u(x)$ from the data $b$ with a known forward operator $A$ and some noise $\\epsilon$. The problem is that the solution is usually not unique and may have an infinite number of solutions or no solution. We usually will specify the goodness of a solution by some metric, for example, the least squares error:\n\n$$\\|A\\hat u - b\\|^2 < \\text{tol}$$\n\nwill specify that solutions within a certain tolerance of the data are acceptable.\n\n::: {#cell-fig-stem-plot .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![Continuous positive function $u(x)$ with sampled points $x_i$](index_files/figure-html/fig-stem-plot-output-1.png){#fig-stem-plot width=329 height=207}\n:::\n:::\n\n\nSince the real world generally is measured via sampling, the $u(x_i)$ are a sample space that estimate the continuous function $u(x)$. Usually there are a huge number of $x_i$ for which we want to estimate $u(x)$, such that there are far more unknowns in $u$ than there are data points in $b$. This gives the least square problem an infinite number of solutions.\n\n## Simple Regularization\n\nRegularization is a technique used to constrain the solution space of the inverse problem. It may incorporate some prior knowledge of the problem either explicitly or implicitly. The addition of a second term or objective to the least squares problem improves the metric by which the solution is being evaluated to distinguish between the infinite number of solutions. The general form of the regularized least squares problem is:\n\n$$\\min_u \\|A u - b\\|^2 + \\alpha R(u)$$\n\nAn example of this could be using the $L_2$ norm of the solution as the regularization term:\n\n$$R(u) = \\frac{1}{2}\\|u\\|^2$$\n\nThe new metric favours solutions where $u$ is small, but it actually includes an even more explicit probabilistic interpretation which can be derived from the Bayesian perspective.\n\n### Bayesian Inference\n\nLet the set of model parameters be $u$ and the data be $b$. Then we assume without any prior knowledge of the problem that the data is generated by a model with some noise:\n\n$$b = A(u) + \\epsilon$$\n\nwhere $\\epsilon$ is a Gaussian noise term with mean zero and variance $\\sigma^2$. The likelihood of the data given the model is then:\n\n$$p(b|u) \\propto \\exp\\left(-\\frac{\\|A(u) - b\\|^2}{2\\sigma^2}\\right)$$\n\nSimilarly, we can assume a prior distribution on the model parameters $u$:\n\n$$p(u) \\propto \\exp\\left(-\\frac{\\|u\\|^2}{2\\beta}\\right)$$\n\nwhere $\\beta$ is the amount of standard deviation that we assume to be present in the prior.\n\n### Maximum Likelyhood Estimation\n\nIn maximum likelihood estimation (MLE), the objective is to find the model that gives the highest likehood for seeing the data that was observed. Note that the function $\\log (.)$ is a monotonic function so it preserves the ordering of values, so maximizing the likelihood is equivalent to maximizing the log likelihood. The MLE is then:\n\n$$\n\\begin{align*}\n\\hat u &= \\arg\\max_u p(b|u)\\\\ \n&= \\arg\\max_u \\log p(b|u) \\\\\n&= \\arg\\min_u \\frac{\\|A(u) - b\\|^2}{2\\sigma^2} \\\\\n&= \\arg\\min_u \\|A(u) - b\\|^2.\n\\end{align*}\n$$\nGiving the unregularized least squares problem.\n\n### Maximum A Posteriori Estimation\n\nIn maximum a posteriori estimation (MAP), the objective is to find the model that gives the highest likelihood for seeing the data that was observed given the prior information. The reason is that it is unintuitive to ask to fit parameters to maximize likelihood of data measured. The driver of the model should be the data itself which is independent of the model. This reverses the probabilistic objective-- find the model parameters that are most likely given the observed data. Bayes' theorem states that:\n\n$$p(u|b) = \\frac{p(b|u)p(u)}{p(b)}$$\n\nwhere $p(b)$ is the marginal likelihood of the data. The value of $p(b)$ is a constant that does not depend on the model parameters, so we can ignore it when it comes to maximizing the probability.\n\n$$\\underbrace{p(u|b)}_{\\text{posterior}} \\propto \\underbrace{p(b|u)}_{\\text{likelihood}} \\underbrace{p(u)}_{\\text{prior}}$$\n\nAs in the MLE, the log likelihood is maximized to find the MAP:\n\n$$\n\\begin{align*}\n\\hat u &= \\arg\\max_u p(u|b)\\\\\n&= \\arg\\max_u \\log \\Big( p(b|u) p(u) \\Big)\\\\\n&= \\arg\\max_u \\log p(b|u) + \\log p(u) \\\\\n&= \\arg\\max_u -\\frac{\\|A(u) - b\\|^2}{2\\sigma^2} - \\frac{\\|u\\|^2}{2\\beta} \\\\\n&= \\arg\\min_u \\frac{\\|A(u) - b\\|^2}{2\\sigma^2} + \\frac{\\|u\\|^2}{2\\beta} \\\\\n&= \\arg\\min_u \\frac{1}{2}\\|A(u) - b\\|^2 + \\frac{1}{2}\\alpha \\|u\\|^2\n\\end{align*}\n$$\n\nwhere $\\alpha = \\frac{\\sigma^2}{\\beta}$ is the regularization parameter. This gives the regularized least squares problem.\n\nThe choice of the regularization parameter $\\alpha$ is a statistical statement on the expected ratio of noise to prior information. If $\\alpha$ is large, the prior information is trusted more than the data, and if $\\alpha$ is small, the data is trusted more than the prior information.\n\n### Notes on Interpretation\n\nIn the Bayesian perspective, the regularization term is grounded in a probabilistic approach that will blindly pick the solution that is most likely given the data and the prior information. The maximum may not always be the best choice when considering the robustness of the solution to noise. The initial choice to regularize with the $L_2$ norm $\\|u\\|^2$ does not make any claims on the likelihood of the model parameters while still yielding the same results.\n\nProbabilistic approaches to inverse problems with complex distributions are becoming more reliable now with the advent of generative models that learn an underlying probability distribution. While the Gaussian prior is easier to work with, it is simplistic and may not always be the best choice. For example the underlying parameters of a forward model could be the density of a material, or its conductivity, in which case the prior distribution could be something very different from a Gaussian. The choice of prior can be enhanced using a neural network for examples.\n\n## Choice of Regularizer {#choice-of-regularizer}\n\nThe problem of least squares with a regularization term can be generalized as:\n\n$$\\min_u \\frac{1}{2}\\|A u - b\\|^2 + \\alpha R(u)$$\n\nFor the case of the $L_2$ norm, the problem becomes:\n\n$$\n\\begin{align*}\n\\hat u =& \\min_u \\frac{1}{2}\\|A u - b\\|^2 + \\frac{\\alpha}{2}\\|u\\|^2\\\\\n=& A^T(Au - b) + \\alpha u = 0\\\\\n=& (A^TA + \\alpha I)u = A^Tb\n\\end{align*}\n$$\n\nThere are many other choices for the regularization term though.\n\nSome common choices for the regularization term $R(u)$ include:\n\n1. **Gaussian $L_2$ Norm Regularization**:\n\n   $$\n   R(u) = \\frac{1}{2}\\|u\\|^2 = \\frac{1}{2}\\sum_i u_i^2\n   $$\n\n   Assumes a Gaussian prior on the model parameters $u$, favoring solutions with smaller norms. It penalizes large values in $u$, leading to solutions with smaller magnitudes.\n\n2. **Slope Penalty**:\n\n   $$\n   R(u) = \\int_{\\mathcal{X}} \\frac{1}{2}\\|\\nabla_x u\\|^2 \\, dx = \\int_{\\mathcal{X}} \\frac{1}{2}\\Big( \\frac{\\partial u}{\\partial x_1}^2 + \\frac{\\partial u}{\\partial x_2}^2 \\Big) \\, dx\n   $$\n\n   Recalling that $u$ is a function across a domain $\\mathcal{X}$, the slope penalty penalizes the squared magnitude of the gradient of $u$. It encourages flatness in the solution by penalizing rapid changes in $u$.\n\n3. **Smoothness Penalty**:\n\n   $$\n   R(u) = \\int_{\\mathcal{X}} \\frac{1}{2}\\|\\nabla^2 u\\|^2 \\, dx = \\int_{\\mathcal{X}} \\frac{1}{2}\\Big( \\frac{\\partial^2 u}{\\partial x_1^2} + \\frac{\\partial^2 u}{\\partial x_2^2} \\Big) \\, dx\n   $$\n\n    The smoothness penalty promotes smoothness in the solution by penalizing changes in the slope of $u$ across the domain. \n    \n4. **$L_1$ Norm Regularization**:\n\n   $$\n   R(u) = \\|u\\|_1 = \\sum_i |u_i|\n   $$\n\n   The $L_1$ norm promotes sparsity in the solution by penalizing the absolute values of $u$. This leads to many parameters being exactly zero, which is desirable in feature selection and compressed sensing applications.\n\n5. **Total Variation Regularization**:\n\n   $$\n  R(u) = \\text{TV}(u) = \\int |\\nabla u| \\, dx = \\int \\sqrt{\\left| \\frac{\\partial u}{\\partial x_1} \\right|^2 + \\left| \\frac{\\partial u}{\\partial x_2} \\right|^2} \\, dx   $$\n\n   Total Variation (TV) regularization penalizes the total amount of variation in \\( u \\) without squaring the gradient. It preserves sharp edges while removing noise.\n\nA demonstration of the effects of the regularizer are given below for a case of denoising an image. Note that the operator $A$ is the identity matrix in this case which is a simplification of the problem.\n\n::: {#fig-regularization-effects .cell layout-ncol='1' layout-nrow='2' execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import gaussian_filter\nfrom scipy.optimize import minimize\nfrom skimage import data\nfrom skimage.transform import resize\nfrom skimage.restoration import denoise_tv_chambolle\n\n# Load the sample grayscale image and normalize\nZ_true = data.camera().astype(np.float64) / 255.0\nZ_true = Z_true*2 - 1  # Normalize to [-1, 1]\n\n# Downsample the image to 32x32\nZ_true= resize(Z_true, (64,64), anti_aliasing=True)\n\n# Add Gaussian noise to simulate observed data\nnp.random.seed(0)  # For reproducibility\nnoise = 0.1 * np.random.normal(size=Z_true.shape)\nZ_noisy = Z_true + noise\n\n# Define different regularization penalties\ndef gaussian_l2_norm(u):\n    return 0.5 * np.sum(u**2)\n\ndef slope_penalty(u):\n    grad_x, grad_y = np.gradient(u)\n    return 0.5 * np.sum(grad_x**2 + grad_y**2)\n\ndef smoothness_penalty(u):\n    laplace_u = np.gradient(np.gradient(u, axis=0), axis=1)\n    return 0.5 * np.sum(laplace_u**2)\n\ndef l1_norm(u):\n    return np.sum(np.abs(u))\n\n# Prepare observed data vector\nA = np.eye(Z_true.size)  # Identity for simplicity\nb = Z_noisy.flatten()\n\n# Regularization strength\nalpha = 1\n\n# Function to solve the regularized least squares problem\ndef regularized_least_squares(A, b, reg_func, alpha):\n    def objective(u):\n        # Least squares term\n        residual = A @ u - b\n        ls_term = 0.5 * np.sum(residual**2)\n        # Regularization term\n        reg_term = alpha * reg_func(u.reshape(Z_true.shape))\n        return ls_term + reg_term\n\n    # Initial guess (flattened noisy image)\n    u0 = b.copy()\n    result = minimize(objective, u0, method='L-BFGS-B')\n    return result.x.reshape(Z_true.shape)\n\n# Apply different regularizations\nrecovered_gaussian_l2 = regularized_least_squares(A, b, gaussian_l2_norm, alpha)\nrecovered_slope = regularized_least_squares(A, b, slope_penalty, alpha)\nrecovered_smoothness = regularized_least_squares(A, b, smoothness_penalty, alpha)\nrecovered_l1 = regularized_least_squares(A, b, l1_norm, alpha)\nrecovered_tv = denoise_tv_chambolle(Z_noisy, weight=alpha*.1)\n\n# Plot original, noisy, and recovered images for each regularizer\nfig, axs = plt.subplots(1, 2)\n\n#Original\nplt.subplot(1,2,1)\nplt.imshow(Z_true, cmap=\"gray\")\nplt.title(\"Original Image\")\nplt.axis(\"off\")\n\n# Noisy Image\nplt.subplot(1,2,2)\nplt.imshow(Z_noisy, cmap=\"gray\")\nplt.title(\"Noisy Image\")\nplt.axis(\"off\")\n\nplt.show()\n\n# Plot original, noisy, and recovered images for each regularizer\nfig, axs = plt.subplots(2,2)\n\n# Gaussian L2 Norm Regularization\nplt.subplot(2, 2, 1)\nplt.imshow(recovered_gaussian_l2, cmap=\"gray\")\nplt.title(\"Gaussian $L_2$ Norm Regularization\")\nplt.axis(\"off\")\n\n# Slope Penalty Regularization\nplt.subplot(2, 2, 2)\nplt.imshow(recovered_slope, cmap=\"gray\")\nplt.title(\"Slope Penalty Regularization\")\nplt.axis(\"off\")\n\n# Smoothness Penalty Regularization\nplt.subplot(2, 2, 3)\nplt.imshow(recovered_smoothness, cmap=\"gray\")\nplt.title(\"Smoothness Penalty Regularization\")\nplt.axis(\"off\")\n\n# Total Variation Regularization\nplt.subplot(2, 2, 4)\nplt.imshow(recovered_tv, cmap=\"gray\")\nplt.title(\"Total Variation Regularization\")\nplt.axis(\"off\")\n\n# Save recontructions\nplt.savefig(\"imgs/regularization_effects.png\")\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Original and Noised Image](index_files/figure-html/fig-regularization-effects-output-1.png){#fig-regularization-effects-1 width=577 height=292}\n:::\n\n::: {.cell-output .cell-output-display}\n![Reconstructions with Different Regularizations](index_files/figure-html/fig-regularization-effects-output-2.png){#fig-regularization-effects-2 width=554 height=409}\n:::\n\nA comparison of different regularization effects on a noisy image reconstruction.\n:::\n\n\n### Closing Notes\n\nAnother note to be made is that the regularization space itself can be changed by working in a latent space where $u = Dz$ and $D$ is a dictionary or basis that maps the latent space $z$ to the model space $u$. This is somewhat similar to PCA.\n\n$$\\frac{1}{2} \\|A(Dz) - b\\|^2 + \\alpha R(z)$$\n\nA more modern approach chooses $R(u)$ based on data about the problem itself. For example, given a set of data $\\{u_1, u_2, \\ldots, u_n\\}$, find the distribution $\\pi(u)$ or what $R(u)$ should be. This statistically learned regularization approach leads to neural networks and deep learning, a topic that will be explored further in the next lecture.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}