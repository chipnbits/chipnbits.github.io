{
  "hash": "b8d606055a86c56329cf802ab95f9be4",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Project Work 1\"\nsubtitle: \"Applications of Machine Learning in Geophysics\"\ndate: 2024-11-15\nauthor: \"Simon Ghyselincks\"\ndescription: >-\n    Geomagnetic inversions are a common problem in geophysics where the goal is to recover a 3D distribution of the conductivity of a section of Earth using magnetic field measurements taken over a 2D surface above ground. The problem is ill-posed and requires advanced techniques to recover data that matches the true distribution. In this project, a series of advanced techniques are used to try and recover true distributions.\ncategories:\n  - Machine Learning\n  - Neural Networks\n  - Geophysics\n\nimage: \"imgs/magnetic_inversion_plot.png\"\ndraft: false\n\neditor: \n  render-on-save: false\n\nfilters:\n  - pseudocode\n  - diagram\n\nbibliography: references.bib\nbiblatexoptions: \"style=numeric,sorting=nyt\"  # Uses numbered style and sorts by name-year-title\nbiblio-style: numeric-comp  # A numeric style in biblatex, similar to IEEE, with compressed citation ranges\n\npseudocode:\n  caption-prefix: \"Algorithm\"\n  reference-prefix: \"Algorithm\"\n  caption-number: true\n---\n\n\n\n\n::: {.hidden}\n$$\n\\def\\argmin{\\operatorname*{argmin}}\n\\def\\bmat#1{\\begin{bmatrix}#1\\end{bmatrix}}\n\\def\\Diag{\\mathbf{Diag}}\n\\def\\ip#1{\\langle #1 \\rangle}\n\n\\def\\maximize#1{\\displaystyle\\maxim_{#1}}\n\n\\def\\minimize#1{\\displaystyle\\minim_{#1}}\n\\def\\norm#1{\\|#1\\|}\n\\def\\proj{\\mathbf{proj}}\n\\def\\R{\\mathbb R}\n\\def\\Re{\\mathbb R}\n\\def\\Rn{\\R^n}\n\\def\\rank{\\mathbf{rank}}\n\\def\\range{{\\mathbf{range}}}\n\\def\\span{{\\mathbf{span}}}\n\\def\\textt#1{\\quad\\text{#1}\\quad}\n\\def\\trace{\\mathbf{trace}}\n\\def\\bf#1{\\mathbf{#1}}\n$$\n:::\n\n\n\n## Motivation\n\nGeomagnetic inversion from recovered magnetic readings above ground is a classic problem in geophysics. Much of the data that we have about subsurface geology must be collected via indirect methods such as magnetic and seismic surveys, or sparse sampling via boreholes. The data can be expensive to collect, where the objective is to detect underground structures such as ore bodies, oil and gas deposits, or aquifers. The goal is to recover a 3D distribution of some of the physical properties of the Earth from the incomplete data, a non-linear and ill-posed problem.\n\nThis project explores the use of machine learning and optimization techniques to attempt an inversion of the data, using electromagnetic equations to model the propogation of the Earth's magnetic field through the subsurface on a known data set. The goal is to develop a model that can take the forward problem data and invert it to recover the true starting distribution.\n\n### Background\n\nA background on the magnetic principles that underpin the problem can be found at [Geophysics for Practicing Geophysicists](https://gpg.geosci.xyz/content/magnetics/magnetics_basic_principles.html).\n\nThe subsurface of the Earth is composed of materials with differences in magnetic susceptibility $\\chi$ and conductivity $\\sigma$. The magnetic susceptibility relates a material's magnetization $M$ to the magnetic field $H$ via the equation $M = \\chi H$. When the magnetic field of the Earth passes through this material it creates a magnetization response, causing small magnetic anomolies that can be measured above ground. The magnetic field passing through the subsurface is assumed to be uniform and known over the survey area.\n\nThe magnetization effect can be calculated by integrating the magnetic effect over each infinetesimally small volume of the subsurface, since the total effect will be the sum of the parts, similar to analysing electric fields from charge distributions. It is easier to first calculate the magnetic potential and then take the curl to get the field:\n\n$$\\mathbf{B}(r) = \\nabla \\times \\mathbf{A}(r) $$\n$$\\mathbf{A}(r) = \\frac{\\mu_0}{4\\pi} \\int_V \\mathbf{M}(r') \\frac{1}{|\\mathbf{r} - \\mathbf{r'}|} dV'$$\n\nwhere $\\mathbf{B}$ is the magnetic field, $\\mathbf{A}$ is the magnetic potential, $\\mathbf{M}$ is the magnetization, and $\\mu_0$ is the magnetic permeability of free space. The equations above are for the case where there are no free currents, which is a good assumption to make for the Earth's subsurface.\n\nThe integral is of the form of a convolution with the kernel $\\frac{1}{|\\mathbf{r} - \\mathbf{r'}|}$, which can be computed using the Fast Fourier Transform (FFT) to speed up the computation. The operation $\\mathbf{B}(r) = \\nabla \\times \\mathbf{A}(r)$ can also be carried through into the integral since it is a linear operator. \n\nUsing all of the above information yields the integral equation that is dependent on the magnetic susceptibility $\\chi$ and the magnetic field $\\mathbf{H}_0$:\n\n$$ \\mathbf{B}_A(\\mathbf{r}) = \\frac{\\mu_0}{4\\pi} \\int_V \\chi(\\mathbf{r'}) \\mathbf{H}_0 \\cdot \\nabla^2 \\left( \\frac{1}{|\\mathbf{r} - \\mathbf{r'}|} \\right) dV' $$\n\nThe resulting magnetic field anomaly $\\mathbf{B}_A$ is projected onto the direction of the magnetic field $\\mathbf{H}_0$ to get the observed anomaly in magnetic field strength at any point above the Earth's surface in the forward model. For survey work, the probed field is usually measured over a planar surface above the ground, forming a 2D image.\n\nSee [Geophysics for Practicing Geophysicists](https://gpg.geosci.xyz/content/magnetics/magnetics_basic_principles.html) for more images and interactive examples.\n\n## Coding the Forward Model\n\nThe model is programmed into the `Magnetics` class below. Note that the FFT is used to speed up the computation of the convolution integral. An explicit adjoint operation is defined to speed up the process of inverting data when using techniques that make use of it.\n\n::: {#f9f08875 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"false\"}\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom scipy.constants import mu_0\n    \nclass Magnetics(nn.Module):\n    \"\"\"\n    A PyTorch module to perform forward and adjoint computations for magnetic inversion problems.\n    \"\"\"\n    def __init__(self, dim, h, dirs, device='cpu'):\n        \"\"\"\n        Initialize the Magnetics module.\n\n        Parameters:\n            dim (list or tuple): Mesh dimensions [nx, ny, nz].\n            h (list or tuple): Cell sizes [dx, dy, dz].\n            dirs (list or tuple): Magnetic field directions [I, A, I0, A0] in radians.\n                                  I  - Magnetization dip angle\n                                  A  - Magnetization declination angle\n                                  I0 - Geomagnetic dip angle\n                                  A0 - Geomagnetic declination angle\n            device (str): Device to perform computations ('cpu' or 'cuda').\n        \"\"\"\n        super(Magnetics, self).__init__()\n        self.dim = dim\n        self.h = h\n        self.dirs = dirs\n        self.device = device\n\n        # Compute the scaling factor\n        dV = torch.prod(self.h.clone().detach())\n        mu_0 = 1.0  # Magnetic permeability (set to 1 for simplicity)\n        zeta = mu_0 / (4 * np.pi)\n        self.mudV = zeta * dV\n\n    def fft_kernel(self, P, center):\n        \"\"\"\n        Compute the 2D shifted FFT of the kernel P.\n\n        Parameters:\n            P (torch.Tensor): The point spread function (PSF) kernel.\n            center (list): Center indices for fftshift.\n\n        Returns:\n            torch.Tensor: The shifted FFT of P.\n        \"\"\"\n        # Shift the kernel for FFT operations\n        S = torch.roll(P, shifts=center, dims=[0, 1])\n        S = torch.fft.fftshift(S)\n        # Compute the 2D FFT\n        S = torch.fft.fft2(S)\n        # Shift the quadrants back\n        S = torch.fft.fftshift(S)\n        return S\n\n    def forward(self, M, height=0):\n        \"\"\"\n        Perform the forward computation using FFT.\n\n        Parameters:\n            M (torch.Tensor): The magnetization model tensor of shape B,C,X,Y,Z.\n\n        Returns:\n            torch.Tensor: The computed magnetic data.\n        \"\"\"\n        dz = self.h[2]\n        z = height + dz / 2  # Starting depth\n\n        data = 0  # Initialize the data\n\n        # Loop through each layer in the z-direction\n        for i in range(M.shape[-1]):\n            # Extract the i-th layer of the model\n            m_layer = M[..., i].to(self.device)\n\n            # Compute the point spread function (PSF) for the current layer\n            psf, center, _ = self.psf_layer(z)\n\n            # Compute the FFT of the PSF kernel\n            s_fft = self.fft_kernel(psf, center)\n\n            # Compute the FFT of the model layer\n            m_fft = torch.fft.fftshift(m_layer)\n            m_fft = torch.fft.fft2(m_fft)\n            m_fft = torch.fft.fftshift(m_fft)\n\n            # Perform the convolution in the frequency domain\n            b_fft = s_fft * m_fft\n            b_fft = torch.fft.fftshift(b_fft)\n\n            # Convert back to the spatial domain\n            b_spatial = torch.real(torch.fft.ifft2(b_fft))\n\n            # Accumulate the data from each layer\n            data += b_spatial\n\n            # Update depth\n            z += dz\n\n        return self.mudV * data\n\n    def adjoint(self, data, height=0):\n        \"\"\"\n        Perform the adjoint operation.\n\n        Parameters:\n            data (torch.Tensor): The observed magnetic data tensor.\n\n        Returns:\n            torch.Tensor: The adjoint result (model update).\n        \"\"\"\n        dz = self.h[2]\n        z = height + dz / 2  # Starting depth\n\n        # Initialize the result tensor\n        m_adj = torch.zeros(\n            1, 1, self.dim[0], self.dim[1], self.dim[2], device=self.device\n        )\n\n        for i in range(self.dim[2]):\n            # Compute the PSF for the current layer\n            psf, center, _ = self.psf_layer(z)\n\n            # Compute the FFT of the PSF kernel\n            s_fft = self.fft_kernel(psf, center)\n\n            # Compute the FFT of the input data\n            data_fft = torch.fft.fft2(data)\n            data_fft = torch.fft.fftshift(data_fft)\n\n            # Perform the adjoint operation in the frequency domain\n            b_fft = torch.conj(s_fft) * data_fft\n\n            # Convert back to the spatial domain\n            b_spatial = torch.fft.fftshift(b_fft)\n            b_spatial = torch.real(torch.fft.ifft2(b_spatial))\n            b_spatial = torch.fft.fftshift(b_spatial)\n\n            # Store the result for the current layer\n            m_adj[..., i] = b_spatial\n\n            # Update depth\n            z += dz\n\n        return self.mudV * m_adj\n\n    def psf_layer(self, z):\n        \"\"\"\n        Compute the point spread function (PSF) for a layer at depth z.\n\n        Parameters:\n            z (float): The depth of the layer.\n\n        Returns:\n            psf (torch.Tensor): The computed PSF.\n            center (list): Center indices for fftshift.\n            rf (torch.Tensor): The radial factor (unused but computed for completeness).\n        \"\"\"\n        # Unpack magnetic field directions\n        I, A, I0, A0 = self.dirs  # Dip and declination angles for magnetization and geomagnetic field\n\n        # Compute half-dimensions\n        nx2, ny2 = self.dim[0] // 2, self.dim[1] // 2\n\n        dx, dy = self.h[0], self.h[1]\n\n        # Create coordinate grids\n        x = dx * torch.arange(-nx2 + 1, nx2 + 1, device=self.device)\n        y = dy * torch.arange(-ny2 + 1, ny2 + 1, device=self.device)\n        X, Y = torch.meshgrid(x, y, indexing='ij')\n\n        # Center indices for fftshift\n        center = [1 - nx2, 1 - ny2]\n\n        # Compute the radial factor\n        rf = (X**2 + Y**2 + z**2) ** 2.5\n\n        # Compute components of the PSF\n        cos_I = torch.cos(I)\n        sin_I = torch.sin(I)\n        cos_A = torch.cos(A)\n        sin_A = torch.sin(A)\n        cos_I0 = torch.cos(I0)\n        sin_I0 = torch.sin(I0)\n        cos_A0 = torch.cos(A0)\n        sin_A0 = torch.sin(A0)\n\n        PSFx = ((2 * X**2 - Y**2 - z**2) * cos_I * sin_A +\n                3 * X * Y * cos_I * cos_A +\n                3 * X * z * sin_I) / rf\n\n        PSFy = (3 * X * Y * cos_I * sin_A +\n                (2 * Y**2 - X**2 - z**2) * cos_I * cos_A +\n                3 * Y * z * sin_I) / rf\n\n        PSFz = (3 * X * z * cos_I * sin_A +\n                3 * Y * z * cos_I * cos_A +\n                (2 * z**2 - X**2 - Y**2) * sin_I) / rf\n\n        # Combine components to get the total PSF\n        psf = (PSFx * cos_I0 * cos_A0 +\n               PSFy * cos_I0 * sin_A0 +\n               PSFz * sin_I0)\n\n        return psf, center, rf\n```\n:::\n\n\nTo test the forward and adjoint operations we can apply a standard adjoint test. This is always a good policy when implementing new operators to determine if there are any issues with the code. We expect that \n$$ \\langle A(X), Q \\rangle = \\langle X, A^*(Q) \\rangle $$\n\nwhere $A$ is the forward operator and $A^*$ is the adjoint operator. The code below performs the adjoint test for the forward model defined above.\n\n::: {#4217d762 .cell execution_count=2}\n``` {.python .cell-code}\nimport pyvista as pv\npv.set_jupyter_backend(\"static\")\n\ndef adjoint_test(op, adj, arg):\n    \"\"\"\n    Adjoint test for a given operator.\n\n    Parameters:\n        op (callable): Forward operator.\n        adj (callable): Adjoint operator.\n        arg (torch.Tensor): Input to the forward operator.\n    \"\"\"\n\n    X = arg\n    D = op(X)\n    Q = torch.rand_like(D)\n\n    # Compute the inner product <A(X), Q>\n    inner_prod1 = torch.sum(D * Q)\n\n    # Compute the inner product <X, adj(Q)>\n    W = adj(Q)\n    inner_prod2 = torch.sum(X * W)\n\n    print(\"Adjoint test:\", inner_prod1.item(), inner_prod2.item())\n\n\n# Set a magnetics model\ndx, dy, dz = 100.0, 100.0, 100.0\nn1, n2, n3 = 256, 256, 128\ndim = torch.tensor([n1, n2, n3])\nh = torch.tensor([dx, dy, dz])\ndirs = torch.tensor([np.pi / 2, np.pi / 2, np.pi / 2, np.pi / 2])\nforMod = Magnetics(dim, h, dirs)\n\n# set the magnetization of the material\nM = torch.zeros(*dim)\nM[120:140, 120:140, 20:40] = 0.3\nM[20:40, 20:40, 10:20] = 0.1\n\nM = M.unsqueeze(0).unsqueeze(0)\nadjoint_test(forMod.forward, forMod.adjoint, M)  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAdjoint test: 4.638533115386963 4.638533592224121\n```\n:::\n:::\n\n\n## Plotting the Model\n\nA simple toy dataset was used to test the adjoint opearation. It is very helpful to have a set of visualization tools to verify the input and output data against expected values. A library of commands that make use of [Pyvista](https://docs.pyvista.org/) are provided below.\n\n::: {#2ea5582e .cell execution_count=3}\n``` {.python .cell-code}\ndef plot_model_2d(M, n_slices=None, cmap=\"rainbow\", figsize=(12, 6)):\n    \"\"\"\n    Plot an array of 2D slices for a given 3D tensor in a single grid-style plot.\n    \n    Parameters:\n    - M (torch.Tensor): 3D tensor representing the model (e.g., [x, y, z]).\n    - n_slices (int, optional): Number of slices to plot. Defaults to all slices.\n    - cmap (str): Colormap for the plot.\n    - figsize (tuple): Size of the figure.\n\n    Returns:\n    - matplotlib.figure.Figure: The created figure.\n    \"\"\"\n    import numpy as np\n    import matplotlib.pyplot as plt\n\n    # Dimensions of the 3D tensor\n    nx, ny, nz = M.shape[-3], M.shape[-2], M.shape[-1]\n    \n    # Determine the number of slices\n    if n_slices is None:\n        n_slices = nz\n    else:\n        n_slices = min(n_slices, nz)\n    \n    # Determine the grid shape (rows and columns)\n    ncols = int(np.ceil(np.sqrt(n_slices)))\n    nrows = int(np.ceil(n_slices / ncols))\n\n    # Create a blank canvas for the grid plot\n    grid = torch.zeros((nrows * nx, ncols * ny), dtype=M.dtype, device=M.device)\n    \n    # Fill the grid with slices\n    slice_idx = 0\n    for i in range(nrows):\n        for j in range(ncols):\n            if slice_idx < n_slices:\n                grid[i * nx:(i + 1) * nx, j * ny:(j + 1) * ny] = M[..., slice_idx]\n                slice_idx += 1\n\n    # Plot the grid\n    fig = plt.figure(figsize=figsize)\n    plt.imshow(grid.cpu().detach().numpy(), cmap=cmap)\n    plt.colorbar()\n    plt.axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n\ndef plot_model(M, plotter = None, threshold_value=0.05):\n    \"\"\"\n    Plot the magnetization model with an outline showing the overall bounds.\n    \n    Parameters:\n        M (torch.Tensor): Magnetization model tensor of shape [B, C, X, Y, Z] or [X, Y, Z].\n        threshold_value (float): Threshold value for magnetization to visualize.\n    \"\"\"\n    \n    if plotter is None:\n        plotter = pv.Plotter()\n    \n    # Remove unnecessary dimensions if present\n    M = torch.squeeze(M)\n    \n    # Convert the PyTorch tensor to a NumPy array\n    m_plot = M.detach().cpu().numpy()\n    \n    # Define grid parameters\n    spacing = (1.0, 1.0, 1.0)  \n    origin = (0.0, 0.0, 0.0)   \n    \n    # Create a PyVista Uniform Grid (ImageData)\n    grid = pv.ImageData()\n    \n    # Set grid dimensions (number of points = cells + 1)\n    grid.dimensions = np.array(m_plot.shape) + 1\n    grid.spacing = spacing\n    grid.origin = origin\n    \n    # Assign magnetization data to cell data\n    grid.cell_data[\"M\"] = m_plot.flatten(order=\"F\")\n    \n    # Apply threshold to isolate regions with M > threshold_value\n    thresholded = grid.threshold(value=threshold_value, scalars=\"M\")\n    \n    # Create an outline of the entire grid\n    outline = grid.outline()\n        \n    # Add the thresholded mesh\n    plotter.add_mesh(thresholded, cmap = \"rainbow\", opacity=0.5, show_edges=True, label='Magnetization > {:.2f}'.format(threshold_value))\n    \n    # Add the outline mesh\n    plotter.add_mesh(outline, color=\"black\", line_width=2, label='Model Bounds')\n    \n    # Optionally, add axes and a legend for better context\n    plotter.add_axes()\n    plotter.add_legend()\n    \n    # Set camera position for better visualization (optional)\n    plotter.view_isometric()\n    \n    return plotter\n\ndef plot_model_with_slider(M):\n    \"\"\"\n    Plot the magnetization model with an interactive slider for threshold value.\n\n    Parameters:\n        M (torch.Tensor): Magnetization model tensor of shape [B, C, X, Y, Z] or [X, Y, Z].\n    \"\"\"\n\n    # Remove unnecessary dimensions if present\n    M = torch.squeeze(M)\n\n    # Convert the PyTorch tensor to a NumPy array\n    m_plot = M.detach().cpu().numpy()\n\n    # Define grid parameters\n    spacing = (1.0, 1.0, 1.0)  \n    origin = (0.0, 0.0, 0.0)   \n\n    # Create a PyVista Uniform Grid (ImageData)\n    grid = pv.ImageData()\n\n    # Set grid dimensions (number of points = cells + 1)\n    grid.dimensions = np.array(m_plot.shape) + 1\n    grid.spacing = spacing\n    grid.origin = origin\n\n    # Assign magnetization data to cell data\n    grid.cell_data[\"M\"] = m_plot.flatten(order=\"F\")\n\n    # Create an outline of the entire grid\n    outline = grid.outline()\n\n    # Create a PyVista plotter\n    plotter = pv.Plotter()\n    plotter.add_mesh(outline, color=\"black\", line_width=2, label='Model Bounds')\n\n    # Add axes and a legend\n    plotter.add_axes()\n    plotter.add_legend()\n\n    # Set camera position for better visualization\n    plotter.view_isometric()\n\n    # Define a callback function for the slider\n    def threshold_callback(value):\n        # Remove previous thresholded mesh if exists\n        if 'thresholded' in plotter.actors:\n            plotter.remove_actor('thresholded')\n        # Apply threshold to isolate regions with M > value\n        thresholded = grid.threshold(value=float(value), scalars=\"M\")\n        # Add the thresholded mesh\n        plotter.add_mesh(thresholded, name='thresholded', cmap=\"rainbow\", opacity=0.5, show_edges=True, label=f'Magnetization > {value:.2f}')\n        plotter.render()\n\n    # Initial threshold value\n    initial_threshold = 0.05\n\n    # Apply initial threshold and plot\n    thresholded = grid.threshold(value=initial_threshold, scalars=\"M\")\n    plotter.add_mesh(thresholded, name='thresholded', cmap=\"rainbow\", opacity=0.5, show_edges=True, label=f'Magnetization > {initial_threshold:.2f}')\n\n    # Add the slider widget\n    plotter.add_slider_widget(\n        threshold_callback,\n        [0.0, np.max(m_plot)],\n        value=initial_threshold,\n        title='Threshold Value',\n        pointa=(0.025, 0.1),\n        pointb=(0.225, 0.1),\n        style='modern',\n    )\n\n    # Show the plot\n    plotter.show()\n\ndef plot_model_with_forward(\n    mag_data,\n    forward_data,\n    spacing=(1.0, 1.0, 1.0),\n    height=0,\n    zoom=1.0,\n    plotter=None,\n    threshold_value=0.05,\n):\n    if plotter is None:\n        plotter = pv.Plotter()\n\n    # Remove unnecessary dimensions if present\n    M = mag_data.squeeze()\n    M = M.cpu().numpy()\n    D = forward_data.squeeze().cpu().numpy()\n\n    # Define grid parameters XYZ spacing\n    origin = (0.0, 0.0, 0.0)\n\n    # Create a PyVista Uniform Grid (ImageData) for the 3D volume\n    grid = pv.ImageData()\n\n    # Set grid dimensions (number of points = cells + 1)\n    grid.dimensions = np.array(M.shape) + 1\n    grid.spacing = spacing\n    grid.origin = origin\n\n    # Assign magnetization data to cell data\n    grid.cell_data[\"M\"] = M.flatten(order=\"F\")\n\n    # Apply threshold to isolate regions with M > threshold_value\n    thresholded = grid.threshold(value=threshold_value, scalars=\"M\").flip_z()\n    outline = grid.outline()\n\n    # Add the thresholded mesh\n    plotter.add_mesh(\n        thresholded,\n        cmap=\"rainbow\",\n        opacity=0.7,\n        show_edges=True,\n        label=\"Magnetization > {:.2f}\".format(threshold_value),\n    )\n\n    # Add the outline mesh\n    plotter.add_mesh(outline, color=\"black\", line_width=2, label=\"Model Bounds\")\n\n    # Create a structured grid for the 2D surface data\n    nx, ny = D.shape\n    x = np.linspace(0, nx * spacing[0], nx)\n    y = np.linspace(0, ny * spacing[1], ny)\n    X, Y = np.meshgrid(x, y)\n    Z = np.full_like(X, -height)  # Position the surface above the volume\n\n    # Create a PyVista mesh for the 2D surface\n    surface_mesh = pv.StructuredGrid(X, Y, Z)\n    surface_mesh.point_data[\"Forward Data\"] = D.flatten(order=\"F\")\n\n    # Add the 2D surface to the plotter\n    plotter.add_mesh(\n        surface_mesh,\n        cmap=\"coolwarm\",\n        show_edges=False,\n        opacity=0.3,\n        label=\"Forward Problem Data\",\n    )\n\n    # Add axes and a legend\n    plotter.add_axes()\n    plotter.add_legend()\n    plotter.camera_position = [\n        (-54507.19712327622, -49446.175185560685, -53221.11813207309),\n        (128.0, 128.0, 64.0),\n        (0.44554389292076074, 0.38017371952961604, -0.8105298158982375),\n    ]\n\n    # Adjust the zoom to fit data in window\n    plotter.camera.zoom(zoom)\n\n\n    return plotter\n```\n:::\n\n\n::: {#c2d7bd9b .cell execution_count=4}\n``` {.python .cell-code code-fold=\"false\"}\np = pv.Plotter()\nD = forMod(M)\nD = D.squeeze()\np = plot_model_with_forward(M, D, h, height=0, plotter=p)\noutput_file = \"imgs/magnetic_inversion_plot.png\"\np.show(screenshot=output_file)\n```\n\n::: {.cell-output .cell-output-display}\n![A simple model and forward data.](index_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\n## Inverting the Magnetic Readings\n\nThe magnetic anomaly readings above ground are but a 2D slice from the 3D data that is being recovered. This is like trying to reconstruct a 3D object from the shadow that it casts, a very ill conditioned problem. To get a gauge of the problem difficulty we attempt a simple inversion using the conjugate gradient method developed in earlier lectures.\n\nThe inversion problem is to recover the magnetization model $M$ from the observed magnetic data $D$ using the forward model $A$, its adjoint $A^*$, and the conjugate gradient method. The null space of the forward operator is quite large, meaning there are infinite possible solutions, so a regularization term is added to the objective function to constrain the solution. The model is\n\n$$D = A(M) + \\epsilon$$\n\nwhere $\\epsilon$ is the noise in the data. \n\n#### Objective Function\nThe objective function to minimize is\n\n$$ J(M) = \\frac{1}{2} \\| A(M) - D \\|_2^2 + \\alpha R(M) $$\n\nwhere $R(M)$ is the regularization term and $\\alpha$ is the regularization parameter. In this example a weight matrix $W$ is used on the flattened magnetic data.\n\n$$ R(M) = \\frac{1}{2} \\| W(M) \\|_2^2$$\n\nThe solution to this problem is given in [Lecture 4](../lecture4/index.qmd) using the normal equations.\n\n$$ (A^*A + \\alpha W^*W) M_{sol} = A^*D$$\n\nThe conjugate gradient method is used to solve the system of equations. The adjoint of the regularization term is also defined to speed up the process of inverting data when using techniques that make use of it. \n\n#### Regularization Implementation\n\nThe penalty or prior that this is to incur is the finite difference between neighboring cells in the model. This is an approximation of the gradient at each point in the magnetic data, functioning as a slope penalty regularization as discussed previously in [Lecture 8](../lecture8/index.qmd#choice-of-regularizer). \n\nThe finite difference matrix in 1D is\n\n$$W = \\begin{bmatrix} 1 & -1 & 0 & \\cdots & 0 \\\\0 & 1 & -1 & \\cdots & 0 \\\\\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\0 & 0 & 0 & \\cdots & -1 \\end{bmatrix}$$\n\nand it can be extended to 2D and 3D by applying the finite difference in each dimension over the flattened data, thus it is a linear operator. It is simpler to compute the value as a summation in practice, applied to the flattened data: $$\\|W(M)\\|^2 = \\sum_i^N\\sum_j^S\\sum_k^Q (M_{i,j,k} - M_{i+1,j,k})^2 + (M_{i,j,k} - M_{i,j+1,k})^2 + (M_{i,j,k} - M_{i,j,k+1})^2$$ We take the mean over each dimension to normalize across any small variations in the data size due to the loss of boundary cells when applying the finite difference.\n\nA generalized linear adjoint computation in PyTorch is also defined in code, so that the regularization term has a defined adjoint operation for the optimization algorithm. The regularization term to be defined is the operator before the squared norm is applied.\n\n::: {#290f5297 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"false\"}\ndef regularization(x):\n    gx = x[:,:,1:,:,:] - x[:,:,:-1,:,:]\n    gy = x[:,:,:,1:,:] - x[:,:,:,:-1,:]\n    gz = x[:,:,:,:,1:] - x[:,:,:,:,:-1]\n    # Concatenate the flattened gradients together and return as the W(M) vector    \n    return torch.cat([torch.flatten(gx), torch.flatten(gy), torch.flatten(gz)])\n\ndef adjoint(A, v, x_sample):\n    \"\"\" \n    Take the adjoint of the forward operator A with respect to the input vector v.\n    \n    Parameters:\n        A (callable): Forward operator.\n        v (torch.Tensor): Input vector.\n        x_sample (torch.Tensor): Sample input for dimensioning (dummy data).\n    \"\"\"\n    \n    x = torch.zeros_like(x_sample)\n    x.requires_grad = True\n    b = A(x)\n    # Compute the dot product of the forward operator with the input vector\n    h = torch.sum(b * v)\n    # Compute the gradient of the dot product with respect to the input image\n    adjoint = torch.autograd.grad(h, x, create_graph=True)[0]\n    return adjoint\n```\n:::\n\n\n#### Data Set for Fitting\n\nA new data set with three blocks is created for a more interesting test case.\n\n::: {#d6a4c6ff .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\ndef gen_true_data(dim):\n\n    # set the magnetization model\n    xtrue = torch.zeros(*dim)\n    xtrue[30:50, 40:50, 10:20] = 0.6\n    xtrue[20:40, 20:40, 5:15] = 0.2\n    xtrue[10:15, 10:15, 2:5] = 0.5\n    \n    return xtrue\n\n# Set the model parameters\nn1 = 64\nn2 = 64\nn3 = 32\ndim = torch.tensor([n1,n2,n3])\nh = torch.tensor([100.0, 100.0, 100.0])\ndirs = torch.tensor([np.pi/2, np.pi/2, np.pi/2, np.pi/2])\nforMod = Magnetics(dim, h, dirs)\n\n# Generate the true data\nxtrue = gen_true_data(dim)\nxtrue = xtrue.unsqueeze(0).unsqueeze(0)\n\nD = forMod(xtrue)\nnoise = torch.randn_like(D)\nD = D + 0.001*noise\n\n# Plot the true model and forward data\np = pv.Plotter()\np = plot_model_with_forward(xtrue, D, h, height=0, zoom = 4.0, plotter=p)\noutput_file = \"imgs/magnetic_inversion_proj.png\"\np.show(screenshot=output_file)\n```\n\n::: {.cell-output .cell-output-display}\n![Model with forward data](index_files/figure-html/cell-7-output-1.png){}\n:::\n:::\n\n\n#### Conjugate Gradient Method\n\nThe equation to solve is $(A^*A + \\alpha W^*W) M_{sol} = A^*D$ which should be defined in the standard form for a conjugate gradient problem, $Cx = b$. To fit this form a new linear operator $C$ is defined as $C := A^*A + \\alpha W^*W$ and the right hand side $b = A^*D$. The conjugate gradient method is then applied to solve the system of equations.\n\n::: {#78089c79 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"false\"}\nimport matplotlib.pyplot as plt\n\ndef conj_gradient(A, b, x0=None, niter=20, tol=1e-2, alpha=1e-2, verbose=True):\n    \"\"\"\n    Solve Ax=b using the conjugate gradient method.\n\n    Paramters:\n        A (callable): A function that computes the operator Ax.\n        b (torch.Tensor): The right-hand side vector.\n        x0 (torch.Tensor, optional): The initial guess. Defaults to None.\n        niter (int, optional): Maximum number of iterations. Defaults to 20.\n        tol (float, optional): Tolerance for the residual. Defaults to 1e-2.\n        alpha (float, optional): Step size for the conjugate gradient method. Defaults to 1e-2.\n    \"\"\"\n    if x0 is None:\n        r = b\n    else:\n        r = b - A(x0)\n\n    q = r\n    x = torch.zeros_like(b)\n    for i in range(niter):\n        Aq = A(q)\n        alpha = (r * r).sum() / (q * Aq).sum()\n        x = x + alpha * q\n        rnew = r - alpha * Aq\n        beta = (rnew**2).sum() / (r**2).sum()\n        q = rnew + beta * q\n        r = rnew.clone()\n        if verbose:\n            print(\"iter = %3d    res = %3.2e\" % (i, r.norm() / b.norm()))\n        if r.norm() / b.norm() < tol:\n            break\n    return x\n\n# Generate the true data\nxtrue = gen_true_data(dim)\nxtrue = xtrue.unsqueeze(0).unsqueeze(0)\n\nD = forMod(xtrue)\nnoise = torch.randn_like(D)\nD = D + 0.001*noise\n\n# Define the forward operator with regularization\nreg_param = 1e-4\nreg_func = lambda x: regularization(x)\ndef A(x, alpha=reg_param):\n    y1 = forMod(x)\n    y1 = forMod.adjoint(y1)\n    \n    y2 = reg_func(x)\n    y2 = adjoint(reg_func, y2, x)\n    return y1 + alpha*y2\n\nb = forMod.adjoint(D)\n\n# Check dimensions and functioning\ny = A(xtrue)\n\n# Solve the inverse problem using the conjugate gradient method\nx0 = torch.zeros_like(xtrue)\n\nxinv = conj_gradient(A, b, x0, niter=20, tol=1e-6, alpha=1e-2, verbose=True)\n    \n# Verify that the gradient is zero at the solution\nxtest = xinv.clone().detach().requires_grad_(True)\nloss = 0.5 * (D - forMod(xtest)).norm()**2 + 0.5 * reg_param * torch.sum(reg_func(xtest)**2)\nloss.backward()\n\n# Print the norm of the gradient\ngradient_norm = xtest.grad.norm().item()\nprint(f\"Gradient norm at xinv: {gradient_norm:.6e}\")\n\n# Get misfit and regularization\nmisfit = 0.5 * (D - forMod(xinv)).norm()**2\nreg = 0.5 * reg_param * torch.sum(reg_func(xtest)**2)\nprint(f\"Misfit: {misfit:.6e}, Regularization: {reg:.6e}\")\n\n\n# Optionally, set a tolerance and assert\ntolerance = 1e-4\nif gradient_norm < tolerance:\n    print(f\"Verification Passed: Gradient norm {gradient_norm:.2e} is below the tolerance {tolerance:.2e}.\")\nelse:\n    print(f\"Verification Failed: Gradient norm {gradient_norm:.2e} exceeds the tolerance {tolerance:.2e}.\")\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\niter =   0    res = 1.47e-01\niter =   1    res = 3.31e-02\niter =   2    res = 6.97e-03\niter =   3    res = 1.34e-03\niter =   4    res = 2.93e-04\niter =   5    res = 1.38e-04\niter =   6    res = 2.79e-04\niter =   7    res = 1.43e-03\niter =   8    res = 4.25e-03\niter =   9    res = 1.72e-03\niter =  10    res = 3.04e-04\niter =  11    res = 7.72e-05\niter =  12    res = 8.64e-05\niter =  13    res = 3.84e-04\niter =  14    res = 1.96e-03\niter =  15    res = 1.21e-03\niter =  16    res = 1.94e-04\niter =  17    res = 6.02e-05\niter =  18    res = 9.90e-05\niter =  19    res = 4.94e-04\nGradient norm at xinv: 4.354817e-04\nMisfit: 7.700652e-08, Regularization: 1.644024e-05\nVerification Failed: Gradient norm 4.35e-04 exceeds the tolerance 1.00e-04.\n```\n:::\n:::\n\n\nNow that the inversion is performed, the results can be checked against the true forward model to see if they match correctly.\n\n::: {#1ee5e04f .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\n# Plot the results   \npred = forMod(xinv) # predicted 2d data\n\nplt.subplot(1, 2, 1)\nplt.imshow(D.view(n1, n2).cpu().detach().numpy(), cmap='rainbow')\nplt.title('Data')\nplt.colorbar()\n\nplt.subplot(1, 2, 2)\nplt.imshow(pred.view(n1,n2).cpu().detach().numpy(), cmap='rainbow')\nplt.title('Inverted model')\nplt.colorbar()\n\nplt.show() \n```\n\n::: {.cell-output .cell-output-display}\n![Forward model comparison.](index_files/figure-html/cell-9-output-1.png){width=623 height=389}\n:::\n:::\n\n\nThis looks like a perfect fit, but as we will see, the method has done a poor job of recovering the true model. \n\n## Results\n\nThe inverted model can be plotted to see how well the inversion has performed.\n\n::: {#fig-inverted-comparison .cell fig-width='8' layout-ncol='2' execution_count=9}\n``` {.python .cell-code}\np = pv.Plotter()\np = plot_model_with_forward(xtrue, D, h, height=0, zoom = 4.0, threshold_value=0.05, plotter=p)\np.show()\n\np = pv.Plotter()\nxinv = xinv.detach().cpu()\np = plot_model_with_forward(xinv, D, h, height=0, zoom = 4.0, threshold_value=.05, plotter=p)\np.show()\n```\n\n::: {.cell-output .cell-output-display}\n![True Model](index_files/figure-html/fig-inverted-comparison-output-1.png){#fig-inverted-comparison-1}\n:::\n\n::: {.cell-output .cell-output-display}\n![Inverted Model](index_files/figure-html/fig-inverted-comparison-output-2.png){#fig-inverted-comparison-2}\n:::\n\nA comparison of true and inverted models.\n:::\n\n\n::: {#fig-inverted-comparison-2d .cell fig-height='4' fig-width='8' layout-ncol='2' execution_count=10}\n``` {.python .cell-code}\n# Plot the true and inverted models\nplot_model_2d(xtrue, figsize=(3,3))\n\nplot_model_2d(xinv, figsize=(3,3))\n```\n\n::: {.cell-output .cell-output-display}\n![True Model](index_files/figure-html/fig-inverted-comparison-2d-output-1.png){#fig-inverted-comparison-2d-1 width=275 height=278}\n:::\n\n::: {.cell-output .cell-output-display}\n![Inverted Model](index_files/figure-html/fig-inverted-comparison-2d-output-2.png){#fig-inverted-comparison-2d-2 width=277 height=278}\n:::\n\nA comparison of true and inverted models in 2D.\n:::\n\n\nThe inversion is clearly a failure. The magnetic distrbution for a material with a lower susceptibility that is close to the surface will produce an identical 2D magnetic field to the true data which is material buried much deeper. The model has a preference for shallow material, as material that is far away is more susceptible to noise and closer to the null space of the forward operator.\n\nTo try and recover the true magnetic model, a more sophisticated inversion method is required. One such method is decribed in a review paper by Crespo et. al [@CrespoMarques2019] and is a topic of exploration in the next project section, where a sparse recovery technique will be used to recover the true model.\n\n",
    "supporting": [
      "index_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}