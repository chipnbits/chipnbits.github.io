{
  "hash": "fd150c60ed3f9808fedbfb59c47442a9",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Lecture 3: Image Denoising with Gradient Descent and Early Stopping\"\nsubtitle: \"A derivation of least squares gradient descent and ODE analysis\"\ndate: 2024-09-17\nauthor: \"Simon Ghyselincks\"\ndescription: >-\n    In continuation of Lecture 2, we now look at an alternative approach to image denoising using gradient descent and early stopping. We will derive the least squares gradient descent algorithm and analyze it as an ordinary differential equation.\ncategories:\n  - Optimization\n  - Inverse Theory\n  - Python\n  - Torch\n  - Adjoint\n\nimage: imgs/gradient_flow.gif\ndraft: false\n\nexecute:\n  freeze: true\n\neditor: \n  render-on-save: false\n---\n\n\n\n\n## Derivations of Linear Algebra Gradients\n\nOften times we wish to find the gradient of a multi-variable function that is formulated as a linear algebra operation. In this case there are some useful \"vector\" derivatives and rules that can simplify the process of calculating more complex expressions. The gradient with respect to vector $\\mathbf{x}$ is generally denoted as $\\nabla_{\\mathbf{x}}$ or alternatively $\\partial_{\\mathbf{x}}$, somewhat of an abuse of notation.\n\n#### 1.  A Warmup\n\n$$\\phi(x) = a^\\top x = \\sum_i a_i x_i$$\n\nThis is a vector dotproduct and the gradient is simply the vector $a$. There is a subtlety here in that the vector is usually transposed to be a column vector, but this is not always the case. Some people in the field of statistics prefer to use row vector, this can cause some confusion. The general convention is a column vector.\n\n$$\\nabla_{\\mathbf{x}} \\phi = a$$\n\n#### 2. Matrix Vector Multiplication\n\n$$\\phi(x) = Ax$$\n\nBased on the previous process we are expecting to potentially get $A^\\top$ as the gradient, however the transpose does not occur in this case because we are not returning a vector that needs to be reshaped into a column form.\n\n$$\\nabla_{\\mathbf{x}} \\phi = A$$\n\n#### 3. Quadratic Forms\n\nOften we may encounter quadratic linear functions that are of the form:\n$$ \\phi(x) = x^\\top A x$$\n\nOne way to determine the gradient is to expand the expression and evaluate for a single $\\frac{\\partial}{\\partial x_i}$ term. This method can be found at [Mark Schmidt Notes](https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/linearQuadraticGradients.pdf) Instead we can apply a chain rule for matrix differentiation that is based on the product rule for differentiation. The chain rule for matrix differentiation is as follows:\n\n$$\\frac{d f(g,h)}{d x} = \\frac{d (g(x)^\\top)}{d x} \\frac{\\partial f(g,h)}{\\partial g} + \\frac{d (h(x)^\\top)}{d x} \\frac{\\partial f(g,h)}{\\partial h}$$\n\n$$ \\begin {align*}\n\\phi(x) &= x^\\top A x \\\\\n\\nabla_{\\mathbf{x}} \\phi &= \\nabla_{\\mathbf{x}} (x^\\top A x) \\\\\n&= \\nabla_{\\mathbf{x}} x^\\top (A x) =  \\nabla_{\\mathbf{x}} x^\\top y\\\\\n&= (\\nabla_{\\mathbf{x}} x) \\nabla_{\\mathbf{x}} x^\\top y + \\nabla_{\\mathbf{x}} y^\\top \\nabla_{\\mathbf{y}} x^\\top y\\\\\n&= I y + \\nabla_{\\mathbf{x}} (x^\\top A^\\top) x\\\\\n&= (A x) + A^\\top x\\\\\n&= (A + A^\\top) x\n\\end {align*}\n$$\n\nThis fits with the generalization for a scalar quadratic form where we end up with $(cx^2)' = (c + c^\\top)x = 2cx$ where $c$ is a scalar.\n\n#### 4. Hadamard Product\n\nAnother form of interest is the hadamard product of two vectors.\n$$\\phi(x) = (Ax)^2 = Ax \\odot Ax$$\n\nFor this one let $y=Ax$ and we can index each element of the vector $y$ as $y_i = \\sum_j A_{ij} x_j$. The hadamard product is a vector $z$ where $z_i = y_i^2$, we can compute the jacobian since now we are taking the gradient with respect to a vector.\n\nThe Jacobian will contain the partial derivatives:\n\n$$\\frac{d\\vec{z}}{d\\vec{x}} = \\begin{bmatrix} \\frac{\\partial z_1}{\\partial x_1} & \\frac{\\partial z_1}{\\partial x_2} & \\cdots & \\frac{\\partial z_1}{\\partial x_n} \\\\\n\\frac{\\partial z_2}{\\partial x_1} & \\frac{\\partial z_2}{\\partial x_2} & \\cdots & \\frac{\\partial z_2}{\\partial x_n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial z_n}{\\partial x_1} & \\frac{\\partial z_n}{\\partial x_2} & \\cdots & \\frac{\\partial z_n}{\\partial x_n} \\end{bmatrix}\n$$\n\nIf we can recover this then we have the gradient of the hadamard product.\n\n$$\n\\begin{align*}\nz_i &= y_i^2 = \\left( \\sum_j A_{ij} x_j \\right)^2\\\\\n\\frac{\\partial}{\\partial x_j} y_i^2 &= 2 y_i \\frac{\\partial y_i}{\\partial x_j} = 2 y_i A_{ij}\\\\\n\\frac{d\\vec{z}}{d\\vec{x}} &= 2 \\begin{bmatrix} y_1 A_{1j} & y_1 A_{2j} & \\cdots & y_1 A_{nj} \\\\\ny_2 A_{1j} & y_2 A_{2j} & \\cdots & y_2 A_{nj} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ny_n A_{1j} & y_n A_{2j} & \\cdots & y_n A_{nj} \\end{bmatrix}\\\\\n&= 2 \\cdot \\text{diag}(\\vec{y})A\\\\\n&= 2 \\cdot \\text{diag}(Ax)A\n\\end{align*}\n$$\n\n#### 5. Least Squares Gradient\n\nWe look at taking the gradient of the expansion of least squares to find the gradient for this optimization objective.\n\n$$\\phi(x) = \\frac{1}{2} ||Ax - b||^2 = \\frac{1}{2} (x^\\top A^\\top A x - 2 b^\\top A x + b^\\top b)$$\n\n$$ \\begin{align*}\n\\nabla_{\\mathbf{x}} \\phi &= \\nabla_{\\mathbf{x}} \\left( \\frac{1}{2} (x^\\top A^\\top A x - 2 b^\\top A x + b^\\top b) \\right)\\\\\n&= \\nabla_{\\mathbf{x}} \\left( \\frac{1}{2} x^\\top A^\\top A x \\right) - \\nabla_{\\mathbf{x}} \\left( b^\\top A x \\right)\\\\\n&= \\frac{1}{2} (A^\\top A + A^\\top A) x - A^\\top b\\\\\n&= A^\\top A x - A^\\top b\\\\\n\\end{align*}\n$$\n\nReturning to the first-order optimality condition we have:\n$$A^\\top A x = A^\\top b$$\n\nAt which point it is in question if $A^\\top A$ is invertible. The invertibility of $A^\\top A$ is determined by the rank of $A$. The rank of A for a non-square matrix is the number of independent columns. If we examine $A^\\top Ax = 0$ then we see that this is only true where the range of $A$ is in the nullspace of $A^\\top$. But $N(A^\\top) = R(A)^\\perp$ so they are orthogonal subspaces and will never coincide unless $Ax=0$. So then $A^\\top A x = 0$ implies that $Ax = 0$ which means that if the null space of $A=\\{0\\}$ then the null space of $A^\\top A = \\{0\\}$ and $A^\\top A$ is invertible. Since $A^\\top A$ is symmetric and positive definite, it is invertible.\n\n\n$A^\\top A$ is invertible $\\iff$ $A$ is full rank, that is all the columns are independent. For non-square matrices, an $m>n$ matrix that is wide will trivially not satisfy this condition. A tall matrix $m<n$ will satisfy the condition if the columns are independent. \n\n## Gradient Descent Analysis\n\nThe standard form of the gradient descent algorithm comes from the field of optimization and can be written as:\n\n$$ x_{k+1} = x_k - \\alpha \\nabla_x \\phi(x_k)$$\n\nWhere $\\alpha$ is the learning rate, which can be dependent on the problem and the gradient. Substituting the gradient of the least squares problem we have:\n\n$$ \\begin{align}\nx_{k+1} &= x_k - \\alpha (A^\\top A x_k - A^\\top b)\\\\\n\\frac{x_{k+1}-x_k}{\\alpha} &= A^\\top b - A^\\top A x_k\\\\\n\\lim_{\\alpha \\to 0} \\frac{x_{k+1}-x_k}{\\alpha} &= \\frac{dx}{dt} = A^\\top (b -A x), \\quad x(0) = x_0\n\\end{align}\n$$\n\nThis ODE is the continuous version of the gradient descent algorithm, also known as the *gradient flow*. Since this a linear first-order ODE we can solve it analytically. The general method for a linear system ODE would be to find the homogeneous solution and the particular solution:\n\n$$ \\begin{align}\nx' + A^\\top A x &= A^\\top b\\\\\n\\text{Guess:} x &= v e^{\\lambda t}\\\\\n\\lambda v e^{\\lambda t} + A^\\top A v e^{\\lambda t} &= A^\\top b e^{\\lambda t}\\\\\n\\lambda v + A^\\top A v &= 0 \\qquad \\text{Homogeneous}\\\\\n(\\lambda I + A^\\top A) v &= 0\\\\\n\\lambda &= \\text{eigenvalues of } A^\\top A, \\quad v = \\text{eigenvectors of } A^\\top A\n\\end{align}\n$$\n\nBefore continuing further with this line, we can see that the solutions will be closely related to the SVD because it contains the information on these eigenvalues and vectors. So we can try to solve the ODE with the SVD.\n\n#### Solving the ODE with SVD\n\n$$\\begin{align}\nA &= U \\Sigma V^\\top\\\\\nA^TA &= V \\Sigma^2 V^\\top\\\\\n\\frac{d}{dt}x &= V \\Sigma U^\\top b - V \\Sigma^2 V^\\top x\\\\\n\\end{align}\n$$\n\nNow let $z = V^\\top x$ and $\\hat b = U ^ \\top b$ then we have:\n\n$$\\begin{align}\n\\frac{d}{dt} (V^\\top x) &= \\Sigma \\hat b - \\Sigma^2 (V^\\top x)\\\\\n\\frac{d}{dt} z &= \\Sigma \\hat b - \\Sigma^2 z\\\\\nz' + \\Sigma^2 z &= \\Sigma \\hat b\\\\\n\\end{align}\n$$\n\nAt this stage since everything has been diagonalized, all of the equations are decoupled and independent so we can solve for the $\\lambda_i$ cases independently. We find the homogeneous $z_h$ and particular $z_p$ solutions:\n\n$$\n\\begin{align}\nz_h' + \\lambda^2 z_h &= 0\\\\\nz_h &= c e^{-\\lambda^2 t}\\\\\nz_p' + \\lambda^2 z_p &= \\lambda \\hat b\\\\\nz_p &= D \\hat b \\\\\n\\lambda^2 D \\hat b &= \\lambda \\hat b\\\\\nD &= \\frac{1}{\\lambda}\\\\\nz_p &= \\frac{1}{\\lambda} \\hat b\n\\end{align}\n$$\n\nSo the general solution for the $i^{th}$ component is:\n\n$$z_i = c_i e^{-\\lambda_i^2 t} + \\frac{1}{\\lambda_i} \\hat b_i$$\n\nSupposing that we start at $x=0$ then we have $z=0$ at all elements and can solve the coefficients $c_i$:\n\n$$c_i = -\\frac{1}{\\lambda_i} \\hat b_i$$\n\nThen putting it all back together with all the equations we have that \n\n$$Z = \\text{diag}\\left( \\lambda_i^{-1} (1 - \\exp (-\\lambda_i t)) \\right) \\hat b$$\n\nSubstituting back in for $x$ and $b$ we get:\n\n$$x = V \\text{diag}\\left( \\lambda_i^{-1} (1 - \\exp (-\\lambda_i t)) \\right) U^\\top b$$\n\nIf we stare at this long enough it begins to look a lot like the pseudoinverse of $A$ from earlier:\n\n$x = V \\Sigma^{-1} U^\\top b$ except in this case there is a time dependence. At the limit as $t \\rightarrow \\infty$ we have that the exponential term goes to zero and we are left with the pseudoinverse solution. This is a nice way to see that the pseudoinverse is the limit of the gradient descent algorithm. What we may be interested in is what happens at earlier stages since each decay term is dependent on the eigenvalues.\n\nFor a simple matrix problem we can create a matrix and plot out the time evolution of the diagonals of the matrix that are of interest. In a sense, we have singular values that are time evolving at different rates.\n\n::: {#57334044 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Seed for reproducibility\nnp.random.seed(4)\n# Create a 5x10 matrix A with random values\nA = np.random.randn(5, 10)\n# Create a vector b of size 5 with random values\nb = np.random.randn(5)\n\n# Compute the SVD of A\nU, S, Vt = np.linalg.svd(A, full_matrices=False)\n\n# Create a time dependent vector of the singular values\ndef St(t):\n    Sdim = S[:, np.newaxis]\n    return (1 - np.exp(-Sdim**2*t)) / Sdim\n\n# Compute the time evolution of the values and plot them on a log scale y axis with a linear time x axis\nt = np.linspace(0, .6, 100)\nT = t[np.newaxis, :]\n\nsingular_vals_t = St(T)\n\n# Initialize the plot\nplt.figure(figsize=(7.5, 4))\n\n# Create a color palette\npalette = sns.color_palette(\"husl\", len(S))\n\n# Plot the singular values and their asymptotes\nfor i in range(len(S)):\n    # Plot the time evolution of each singular value\n    sns.lineplot(x=t, y=singular_vals_t[i, :], color=palette[i], linewidth=2, label=f'$1/S_{i}$ ')\n    \n    Sinv = 1/S[i]\n\n    # Add a horizontal asymptote at the original singular value\n    plt.axhline(y=Sinv, color=palette[i], linestyle='--', linewidth=1)\n    \n    # Annotate the asymptote with the singular value\n    plt.text(t[-1] + 0.02, Sinv, f'{Sinv:.2f}', color=palette[i], va='center')\n\n# Configure plot aesthetics\nplt.xlabel('Time', fontsize=14)\nplt.ylabel('Inverse Singular Vals', fontsize=14)\nplt.title('Time Evolution of Pseudo Inverse in Gradient Flow', fontsize=16)\nplt.legend(title='Inverse Singular Vals', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.xlim(t[0], t[-1] + 0.1)\nplt.tight_layout()\nplt.savefig('imgs/pseudo_inverse_time_evolution.png')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=703 height=374}\n:::\n:::\n\n\nSo we can use early stopping to prevent the flow from reaching the optimal point, a very useful technique. When it comes to inverse theory, often we are not interested in the optimal solution, but more interested in getting somewhere close that is not too noisy. This method differs from the thresholded pseudoinverse from the previous lecture, in that it allows some blending of the the smaller singular values, but their propensity for blowing up is controlled by the time exponent and early stopping.\n\n### Example for Image Recovery using Analytic Solution\n\nReferring back to the problem of estimating the original image based on a noisy point spread function. We can monitor the time evolution of the estimate using gradient flow. Some code below defines the problem again, with recovery of the SVD decomposition for the 32x32 image, which will be used to solve the ODE for the gradient flow.\n\n::: {#2e0edef4 .cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport matplotlib\n#matplotlib.use('TkAgg')\nimport numpy as np\nimport torch.optim\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nimport copy\n\nimport seaborn as sns\n\nimport math\nimport os\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch.fft\n\nclass gaussianConv(nn.Module):\n    \"\"\"\n    A PyTorch module that applies a Gaussian convolution to an input image using \n    a parameterized Gaussian Point Spread Function (PSF). The PSF is derived \n    from a covariance matrix and the derivatives of the Gaussian are computed \n    for edge detection.\n\n    Args:\n        C (torch.Tensor): Inverse of covariance matrix used to define the shape of the Gaussian.\n        t (float, optional): Scaling factor for the Gaussian, default is np.exp(5).\n        n0 (float, optional): Scaling factor for the original PSF, default is 1.\n        nx (float, optional): Scaling factor for the derivative along the x-axis, default is 1.\n        ny (float, optional): Scaling factor for the derivative along the y-axis, default is 1.\n    \"\"\"\n    def __init__(self, C, t=np.exp(5), n0=1, nx=1, ny=1):\n        super(gaussianConv, self).__init__()\n\n        self.C = C\n        self.t = t\n        self.n0 = n0\n        self.nx = nx\n        self.ny = ny\n\n    def forward(self, image):\n        P, center = self.psfGauss(image.shape[-1], image.device)\n        P_shifted = torch.roll(P, shifts=center, dims=[2, 3])\n        S = torch.fft.fft2(P_shifted)\n        I_fft = torch.fft.fft2(image)\n        B_fft = S * I_fft\n        B = torch.real(torch.fft.ifft2(B_fft))\n\n        return B\n\n    def psfGauss(self, dim, device='cpu'):\n        m = dim\n        n = dim\n\n        # Create a meshgrid of (X, Y) coordinates\n        x = torch.arange(-m // 2 + 1, m // 2 + 1, device=device)\n        y = torch.arange(-n // 2 + 1, n // 2 + 1, device=device)\n        X, Y = torch.meshgrid(x, y, indexing='ij')\n        X = X.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, m, n)\n        Y = Y.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, m, n)\n\n        cx, cy, cxy = self.C[0, 0], self.C[1, 1], self.C[0, 1]\n\n        PSF = torch.exp(-self.t * (cx * X ** 2 + cy * Y ** 2 + 2 * cxy * X * Y))\n        PSF0 = PSF / torch.sum(PSF.abs())\n\n        Kdx = torch.tensor([[-1, 0, 1],\n                            [-2, 0, 2],\n                            [-1, 0, 1]], dtype=PSF0.dtype, device=device) / 4\n        Kdy = torch.tensor([[-1, -2, -1],\n                            [0, 0, 0],\n                            [1, 2, 1]], dtype=PSF0.dtype, device=device) / 4\n\n        Kdx = Kdx.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, 3, 3)\n        Kdy = Kdy.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, 3, 3)\n\n        PSFdx = F.conv2d(PSF0, Kdx, padding=1)\n        PSFdy = F.conv2d(PSF0, Kdy, padding=1)\n\n        PSF_combined = self.n0 * PSF0 + self.nx * PSFdx + self.ny * PSFdy\n\n        center = [1 - m // 2, 1 - n // 2]\n\n        return PSF_combined, center\n\ndim = 32\nx = torch.zeros(1, 1, dim, dim)\nx[:,:, 12:14, 12:14] = 1.0\nx[:,:, 10:12, 10:12] = -1.0\n\nC = torch.tensor([[1, 0],[0, 1]])\nAmv = gaussianConv(C, t=0.1,n0=1, nx=0.1,  ny=0.1)\n\nn=(len(x.flatten()))\nAmat = torch.zeros(n,n)\n\nk=0\nfor i in range(x.shape[-2]):\n  for j in range(x.shape[-1]):\n    e_ij = torch.zeros_like(x)\n    e_ij[:,:, i, j] = 1.0\n    y = Amv(e_ij)\n    Amat[:, k] = y.flatten()\n    k = k+1\n\nU, S, V = torch.svd(Amat.to(torch.float64))\nb = Amv(x)\n```\n:::\n\n\nNow that we have the matrix form of the forward operator `Amat` defined, along with the forward result `b` and the the decomposition `U, S, V` we can run the pseudo-inverse gradient flow method as before. So in this case we will be computing:\n\n$$ x = V \\text{diag}\\left( \\lambda_i^{-1} (1 - \\exp (-\\lambda_i t)) \\right) U^\\top b$$\n\nSince these represents an evolution over time, an animation can be created to show the time evolution of the image recovery, along with the effect of continuing into a region where noise is amplified and dominates.\n\nRecalling the original and distorted images with a small amount of noise $\\epsilon$ are as follows:\n\n::: {#f8cdd23c .cell execution_count=4}\n``` {.python .cell-code}\nplt.figure(figsize=(6, 3))\nplt.subplot(1, 2, 1)\nplt.imshow(x[0, 0], cmap='viridis', vmin=-1, vmax=1)\nplt.title('Original Image')\nplt.axis('off')\nplt.subplot(1, 2, 2)\n\nb_noisy = b+ 0.01 * torch.randn_like(b)\nplt.imshow(b_noisy[0, 0], cmap='viridis', vmin=-1, vmax=1)\nplt.title('Distorted Image')\nplt.axis('off')\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=549 height=288}\n:::\n:::\n\n\nThe distorted image has had much of its intensity spread out diffusely, so it is only visible as a faint outline. The noise is also visible in the image as a grainy texture. The gradient flow method will attempt to recover the original image from this distorted image.\n\n::: {#7febabf8 .cell execution_count=5}\n``` {.python .cell-code}\nfrom matplotlib import animation\n\nb_flat = b.flatten().to(torch.float64)\nx_flat = x.flatten().to(torch.float64)\nb_noisy = b_flat + 0.001 * torch.randn_like(b_flat)\n\ndef get_xhat(t):\n    Sinv_t = (1 - torch.exp(-S**2 * t)) / S\n    A_pinv = V @ torch.diag(Sinv_t) @ U.T\n    xhat = A_pinv @ b_noisy\n    return xhat\n\n# Time evolution parameters\nnum_frames = 50\nt_vals = np.logspace(0, 6, num_frames)\n\n# Prepare the plot\nfig, ax = plt.subplots(figsize=(6, 6))\nim = ax.imshow(np.zeros((dim, dim)), cmap='viridis', vmin=-1, vmax=1)\nax.set_title('Time Evolution of Pseudo-Inverse Gradient Flow')\nplt.axis('off')\n\n# Initialize the error text\nerror_text = ax.text(0.02, 0.95, '', transform=ax.transAxes, color='blue', fontsize=12,\n                     verticalalignment='top')\n\ntime_text = ax.text(0.5, 0.95, '', transform=ax.transAxes, color='blue', fontsize=12,\n                        verticalalignment='top')\n\n# Initialize containers to track min error and best time\ntracking = {'min_error': float('inf'), 'best_t': 0.0}\n\n# Animation update function\ndef update_frame(t):\n    # Compute time-dependent singular values\n    Sinv_t = (1 - torch.exp(-S ** 2 * t)) / S\n    # Construct the pseudoinverse of Amat at time t\n    A_pinv = V @ torch.diag(Sinv_t) @ U.t()\n    # Reconstruct the image estimate x(t)\n    xt = A_pinv @ b_noisy\n    # Compute the relative error\n    error = torch.norm(x_flat - xt) / torch.norm(x_flat)\n    \n    # Update min_error and best_t if current error is lower\n    if error.item() < tracking['min_error']:\n        tracking['min_error'] = error.item()\n        tracking['best_t'] = t\n\n    # Reshape to image dimensions\n    x_image = xt.reshape(dim, dim).detach().numpy()\n\n    # Update the image data\n    im.set_data(x_image)\n\n    # Update the error text\n    error_text.set_text(f'Relative Error: {error.item():.4f}')\n    time_text.set_text(f'Time: {t:.2f}')\n\n    return [im, error_text, time_text]\n\n# Create the animation\nani = animation.FuncAnimation(fig, update_frame, frames=t_vals, blit=True, interval=100)\n\nani.save('imgs/gradient_flow.gif', writer='pillow', fps=5)\nplt.close(fig)\n```\n:::\n\n\n![](imgs/gradient_flow.gif){width=600px}\n\nAnd we saved the best time that was discovered for the recovery (with prior knowledge of the ground truth). So we can inspect that image, this was the best that we could do with the gradient flow method.\n\n::: {#96468aea .cell execution_count=6}\n``` {.python .cell-code}\nbest_img = get_xhat(tracking['best_t']).reshape(dim, dim).detach().numpy()\n\nplt.figure(figsize=(6, 6))\nplt.imshow(best_img / np.max(np.abs(best_img)), cmap='viridis', vmin=-1, vmax=1)\nplt.title(f'Best Reconstruction at t={tracking[\"best_t\"]:.2f}\\nRelative Error: {tracking[\"min_error\"]:.4f}')\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=463 height=501}\n:::\n:::\n\n\n## Recovery of the Adjoint Operator using Autograd\n\nIn this case we were able to compute the matrix form of $A$ and use its transpose to compute the SVD, but in many cases this might be too expensive or there may not be a closed form analytic solution to the early stopping technique. In such cases we wish to recover the adjoint. The question then is how to recover the adjoint operator from the `Amv` operator? There are helpful tools available through the use of automatic differentiation to track the gradients of the forward operator and recover the adjoint operator. This is a very powerful tool that can be used to recover the adjoint operator in a very general way.\n\nBy definition the adjoint has the property that:\n$$\\langle Ax, v \\rangle = \\langle x, A^\\top v \\rangle$$\n\n### Explicit Computation of the Adjoint\n\nWe can compute the adjoint explicitly for the `Amv` operator based on its computation from earlier. The discrete fourier transform matrix operator $F$ has the property that $F^{-1} = F^\\top$ so we can use this to compute the adjoint.\n\n$$\n\\begin{align}\nA(x) &= \\mathcal{F}^-1 \\left( \\mathcal{F}(P) \\odot \\mathcal{F}(x) \\right)\\\\\n&= F^\\top \\left( \\text{diag} (F(P)) F(x) \\right)\\\\\nA^\\top(v) &= F^\\top \\text{diag} (F(P))^* F v\\\\\n\\end{align}\n$$\n\nWhere the hadamard operation of the two vectors has been modified to a matrix form by diagonalizing the vector $F(P)$ that is the Fourier transform of the point spread function. From this form it is posible to take the adjoint of the operator by taking the complex conjugate of the transpose of the entire operation. \n\n### Autograd Computation of the Adjoint\n\nWe start with a new function $h = v^\\top A(x)$ and we wish to compute the gradient of $h$ with respect to $x$. \n\n$$ \\nabla_x h = \\nabla_x (v^\\top A(x)) = A^\\top(v)$$\n\nThe gradient of $h$ with respect to $x$ is the adjoint operator $A^\\top(v)$. We can use the `torch.autograd.grad` function to compute the gradient of $h$ with respect to $x$.\n\n::: {#ae7a7b8e .cell execution_count=7}\n``` {.python .cell-code}\ndef Amv_adjoint(v):\n    x = torch.zeros(1, 1, dim, dim)\n    x.requires_grad = True\n    b = Amv(x)\n    # Compute the dot product of the forward operator with the input vector\n    h = torch.sum(b * v)\n    # Compute the gradient of the dot product with respect to the input image\n    adjoint = torch.autograd.grad(h, x, create_graph=True)[0]\n    return adjoint\n```\n:::\n\n\nWe can use this to recover $A^\\top$ for the general case if we run the operator on the set of basis vectors in the image space. This will give us the adjoint operator in the form of a matrix. We can also use it to confirm that it recovers the matrix transpose of the forward operator if we are working with a simple matrix, reusing the `Amat` matrix from earlier to take its transpose and compare it to the adjoint operator.\n\n::: {#4dbef633 .cell execution_count=8}\n``` {.python .cell-code}\nAmat_adj = torch.zeros(n,n)\n\ndim = 32 # Same as earlier\nk=0\nfor i in range(dim):\n  for j in range(dim):\n    e_ij = torch.zeros_like(x)\n    e_ij[:,:, i, j] = 1.0\n    y = Amv_adjoint(e_ij)\n    Amat_adj[:, k] = y.flatten()\n    k = k+1\n\ndiff = torch.norm(Amat_adj - Amat.T)\nprint(f'Norm of difference between adjoint and transpose: {diff:.2e}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNorm of difference between adjoint and transpose: 4.43e-07\n```\n:::\n:::\n\n\nSo the difference is within the bounds of numerical precison and the code appears to be working correctly.\n\n## Gradient Descent with Adjoint\n\nWe can now use the defined operators (functions) from earlier to setup a simple gradient descent algorithm with a step size and early stopping to produce a recovery image that bypasses the need to compute the SVD decomposition, which may be very expensive for large matrices.\n\n::: {#4d7ed9ba .cell execution_count=9}\n``` {.python .cell-code}\nfrom tqdm import tqdm\n\ndef least_squares_sol(x0, b, Amv, Amv_adjoint, max_iter=1000, alpha=1e-3, tol=1e-6, show_progress=True):\n    \"\"\"\n    Solves the least squares problem using gradient descent with optional progress tracking.\n\n    Parameters:\n    - x0 (torch.Tensor): Initial guess for the solution.\n    - b (torch.Tensor): Observation vector.\n    - Amv (callable): Function to compute A @ x.\n    - Amv_adjoint (callable): Function to compute A^T @ v.\n    - max_iter (int): Maximum number of iterations.\n    - alpha (float): Learning rate.\n    - tol (float): Tolerance for convergence.\n    - show_progress (bool): If True, display a progress bar; otherwise, suppress output.\n\n    Returns:\n    - x (torch.Tensor): Approximated solution vector.\n    \"\"\"\n    x = x0.clone()\n    x.requires_grad = True\n    b_noisy = b.clone() + 0.01 * torch.randn_like(b)\n\n    # Initialize progress bar or a placeholder for quiet mode\n    pbar = tqdm(total=max_iter, desc='Least Squares Iteration', unit='iter', disable=not show_progress) \n    for i in range(max_iter):\n        # Gradient descent update\n        residual = Amv(x) - b_noisy\n        gradient = Amv_adjoint(residual)\n        xnext = x - alpha * gradient\n\n        # Compute relative error\n        error = torch.norm(xnext - x) \n\n        # Update the progress bar with the current error\n        if show_progress:\n            pbar.set_postfix({'Error': f'{error.item():.4e}'})\n            pbar.update(1);\n\n        # Check for convergence\n        if error < tol:\n            if show_progress:\n                pbar.write(f'Converged at iteration {i+1} with error {error.item():.4e}')\n            x = xnext\n            break\n\n        x = xnext\n\n    pbar.close()\n    \n    return x\n\nb = Amv(x)\nx0 = torch.zeros_like(x)\nxhat = least_squares_sol(x0, b, Amv, Amv_adjoint, max_iter=1000, alpha=1, tol=1e-6, show_progress=False)\n\n# Display final images\nplt.figure(figsize=(6, 3))\nplt.subplot(1, 2, 1)\nplt.imshow(x[0, 0], cmap='viridis', vmin=-1, vmax=1)\nplt.title('Original Image')\nplt.axis('off')\nplt.subplot(1, 2, 2)\nplt.imshow(xhat.detach().numpy()[0, 0], cmap='viridis', vmin=-1, vmax=1)\nplt.title('Recovered Image')\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=549 height=288}\n:::\n:::\n\n\nNote that torch does have the framework to run autograd on the least squares objective itself, but for this general method we are using the adjoint to compute the gradient (and indirectly invoking autograd). This framework is the most general for when there might not be explicit analytic solutions to the least squares problem, but we have the forward operator and its adjoint.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}