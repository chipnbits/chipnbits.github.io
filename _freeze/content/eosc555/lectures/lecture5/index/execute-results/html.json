{
  "hash": "2b74c18b7e38df553ae658327dbfb63a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Lecture 5\"\nsubtitle: \"Gauss Newton for Non-Linear Least Squares Optimization\"\ndate: 2024-09-25\nauthor: \"Simon Ghyselincks\"\ndescription: >-\n    The Gauss-Newton method is a powerful optimization technique for solving non-linear least squares problems of the form $\\min_{p} \\frac{1}{2}\\|F(p)-d\\|^2$. In this lecture, a derivation of the method is presented along with a comparison to Newton's method. Finally an algorithm for solving the problem is given.\ncategories:\n  - Optimization\n  - Gauss-Newton\n\nimage: imgs/lotka_volterra_phase_space.png\ndraft: false\n\neditor: \n  render-on-save: false\n\nfilters:\n  - pseudocode\n\npseudocode:\n  caption-prefix: \"Algorithm\"\n  reference-prefix: \"Algorithm\"\n  caption-number: true\n---\n\n\n\n\n::: {.hidden}\n$$\n\\def\\argmin{\\operatorname*{argmin}}\n\\def\\bmat#1{\\begin{bmatrix}#1\\end{bmatrix}}\n\\def\\Diag{\\mathbf{Diag}}\n\\def\\ip#1{\\langle #1 \\rangle}\n\n\\def\\maximize#1{\\displaystyle\\maxim_{#1}}\n\n\\def\\minimize#1{\\displaystyle\\minim_{#1}}\n\\def\\norm#1{\\|#1\\|}\n\\def\\proj{\\mathbf{proj}}\n\\def\\R{\\mathbb R}\n\\def\\Re{\\mathbb R}\n\\def\\Rn{\\R^n}\n\\def\\rank{\\mathbf{rank}}\n\\def\\range{{\\mathbf{range}}}\n\\def\\span{{\\mathbf{span}}}\n\\def\\textt#1{\\quad\\text{#1}\\quad}\n\\def\\trace{\\mathbf{trace}}\n\\def\\bf#1{\\mathbf{#1}}\n$$\n:::\n\n\n\n## A Non-Linear Dynamics Problem\n\nA well studied problem in non-linear dynamics involves the predator-prey model that is described by the Lotka-Volterra equations. The equations are given by:\n\n$$\n\\begin{aligned}\n\\frac{dx}{dt} &= \\alpha x - \\beta xy \\\\\n\\frac{dy}{dt} &= \\delta xy - \\gamma y\n\\end{aligned}\n$$\n\nwhere $x$ and $y$ are the populations of the prey and predator respectively. The parameters $\\alpha, \\beta, \\gamma, \\delta$ are positive constants. The goal is to find the values of these parameters that best fit the data.\n\nThere is no closed form analytic solution that is known to this remarkably simple system of equations, which is why we must resort to numerical solutions to compute the model.\n\nMore information about the model can be found at the [Wikipedia](https://en.wikipedia.org/wiki/Lotka%E2%80%93Volterra_equations) page.\n\n### The Forward Problem\n\nWe start with an initial time $t_0$ and initial conditions $x_0, y_0$, with parameters $\\alpha, \\beta, \\gamma, \\delta$ to run a forward version of the problem using a variant of the forward Euler method, the RK4.\n\n::: {#cell-pred-prey-forward .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\n\nclass LotkaVolterraModel(nn.Module):\n    def __init__(self, alpha, beta, gamma, delta):\n        super(LotkaVolterraModel, self).__init__()\n        # Define parameters as torch tensors that require gradients\n        self.alpha = nn.Parameter(torch.tensor(alpha, dtype=torch.float32))\n        self.beta = nn.Parameter(torch.tensor(beta, dtype=torch.float32))\n        self.gamma = nn.Parameter(torch.tensor(gamma, dtype=torch.float32))\n        self.delta = nn.Parameter(torch.tensor(delta, dtype=torch.float32))\n\n    def forward(self, x, y):\n        # Ensure x and y are tensors\n        if not isinstance(x, torch.Tensor):\n            x = torch.tensor(x, dtype=torch.float32)\n        if not isinstance(y, torch.Tensor):\n            y = torch.tensor(y, dtype=torch.float32)\n\n        # Compute dx and dy based on the current parameters\n        dx = self.alpha * x - self.beta * x * y\n        dy = self.delta * x * y - self.gamma * y\n\n        return dx, dy\n\nclass RK4Solver:\n    def __init__(self, model):\n        self.model = model\n\n    def step(self, x, y, dt):\n        \"\"\"\n        Perform a single RK4 step.\n        \"\"\"\n        # Convert x and y to tensors if they are not already\n        if not isinstance(x, torch.Tensor):\n            x = torch.tensor(x, dtype=torch.float32)\n        if not isinstance(y, torch.Tensor):\n            y = torch.tensor(y, dtype=torch.float32)\n\n        # RK4 Step calculations\n        k1_x, k1_y = self.model.forward(x, y)\n        k2_x, k2_y = self.model.forward(x + 0.5 * dt * k1_x, y + 0.5 * dt * k1_y)\n        k3_x, k3_y = self.model.forward(x + 0.5 * dt * k2_x, y + 0.5 * dt * k2_y)\n        k4_x, k4_y = self.model.forward(x + dt * k3_x, y + dt * k3_y)\n\n        # Update x and y using weighted averages of the slopes\n        x_new = x + (dt / 6) * (k1_x + 2 * k2_x + 2 * k3_x + k4_x)\n        y_new = y + (dt / 6) * (k1_y + 2 * k2_y + 2 * k3_y + k4_y)\n\n        return x_new, y_new\n\n    def solve(self, x0, y0, time_steps):\n        \"\"\"\n        Solve the system over a serie of time steps.\n        Parameters:\n            x0: Initial value of prey population\n            y0: Initial value of predator population\n            time_steps: List or numpy array of time steps to solve over\n        \"\"\"\n        x, y = x0, y0\n        DT = time_steps[1:] - time_steps[:-1]\n        trajectory = torch.zeros(len(time_steps), 2)\n        trajectory[0] = torch.tensor([x, y])\n\n        for i, dt in enumerate(DT):\n            x, y = self.step(x, y, dt)\n            trajectory[i+1] = torch.tensor([x, y])            \n\n        return trajectory\n\n# Define the model parameters\nalpha = 1.0\nbeta = .1\ngamma = 1.5\ndelta = 0.1\n\n# Create the model and solver\nmodel = LotkaVolterraModel(alpha, beta, gamma, delta)\nsolver = RK4Solver(model)\n\n# Define the initial conditions and time steps\nx0 = 5\ny0 = 1\ntime_steps = np.linspace(0, 20, 1000)\n\n# Solve the system\ntrajectory = solver.solve(x0, y0, time_steps)\n\nx_values = trajectory[:, 0].detach().numpy()\ny_values = trajectory[:, 1].detach().numpy()\n\nplt.plot(time_steps, x_values, label='Prey')\nplt.plot(time_steps, y_values, label='Predator')\nplt.xlabel('Time')\nplt.ylabel('Population')\nplt.legend()\nplt.savefig('imgs/lotka_volterra.png')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![The time evolution of the prey and predator populations.](index_files/figure-html/pred-prey-forward-output-1.png){#pred-prey-forward width=622 height=429}\n:::\n:::\n\n\nWe can additionally look at the phase space of the system for various initial conditions to see how the different solutions are periodic.\n\n::: {#cell-pred-prey-phase-space .cell execution_count=3}\n``` {.python .cell-code}\n# Define the initial conditions\nx0 = 5\ny0 = [.2,.5,1, 2, 3, 4, 5]\n\n# Create the model and solver\nmodel = LotkaVolterraModel(alpha, beta, gamma, delta)\nsolver = RK4Solver(model)\n\n# Define the time steps\ntime_steps = np.linspace(0, 10, 1000)\n\n# Plot the phase space\nplt.figure(figsize=(6, 4.5))\nfor y in y0:\n    trajectory = solver.solve(x0, y, time_steps)\n    x_values = trajectory[:, 0].detach().numpy()\n    y_values = trajectory[:, 1].detach().numpy()\n    plt.plot(x_values, y_values, label=f'y0={y}')\n\nplt.xlabel('Prey Population')\nplt.ylabel('Predator Population')\nplt.legend()\nplt.title('Lotka-Volterra Phase Space')\nplt.savefig('imgs/lotka_volterra_phase_space.png')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![The phase space of the predator-prey model.](index_files/figure-html/pred-prey-phase-space-output-1.png){#pred-prey-phase-space width=510 height=413}\n:::\n:::\n\n\n## The Inverse Problem\n\nThe inverse problem in this case is to find the parameters $\\alpha, \\beta, \\gamma, \\delta$ that best fit the data. We suppose that we have a model with parameters that takes in the initial conditions and time steps and returns the trajectory of the system. For simplicity we vectorize the previous $\\begin{bmatrix} x \\\\ y \\end{bmatrix}$ into a single vector $\\vec{x}$. The forward model is then a function $F(\\vec{x}; \\vec{p})$ where $\\vec{p}$ are the parameters and is an approximator of the true system, where\n\n$$f(\\vec{x}; \\vec{p}) \\approx \\frac{d \\vec{x}}{dt} = \\begin{bmatrix} \\alpha x - \\beta xy \\\\ \\delta xy - \\gamma y \\end{bmatrix}, \\quad \\vec{x}_0 = \\vec{x}(0).$$\n\nwhere $\\vec{x}$ is the state of the system and $\\vec{p}$ are the parameters. \n\nThe goal is to form an estimate of $\\vec{p}$, while the data that we have collected may be sparse, noisy, or incomplete. We represent the incompleteness in the data using the $Q$ *sampling operator* which is applied to the true underlying data to give $Qx$. If $x$ is fully given then $Q=I$.\n\nA finite difference approximation of the derivative of the data can be used to approximate the derivative of the data,\n\n$$f(\\vec{x}; \\vec{p}) \\cong \\frac{\\vec{x}_{n+1} - \\vec{x}_n}{\\Delta t}.$$ \n\nSo in this case the forward model is a time derivative, and the observed data is computed from an intial condition $\\vec{x}_0$ and an ODE approximate solution such as Euler's method or RK4. The output of the forward process is then given as $F$, where\n $$F(\\vec{p}, x_0) = \\hat {\\vec{x}}(t, \\vec{p}),$$ \n \n is the application of the system dynamics to the initial conditions and the parameter to create an estimated trajectory $\\hat{\\vec{x}}(t, \\vec{p})$.\n\nThe observed data is $d = Q\\vec{x}(t)$. We also make an assumption here that $F$ does not depend on the particular solver that we are using for the forward ODE and that all of the $\\vec{p}$ are physical parameters, we assume that the parameters are faithful enough.\n\nFor the rest of the mathematical notation ahead, the explicit marking of vectors is ommitted to simplify the equations.\n\n#### Goodness of Fit\n\nThe goodness of fit is measured by using the L2 norm of the difference between the observed data and the model output, thus forming the non-linear least squares problem:\n\n$$ \\min_{p} \\frac{1}{2}\\|QF(p)-d\\|^2 = \\min_{p}\\|G(p)\\|^2.$$\n\nTo find the best fit, the objective is to minimize the mean squared error (MSE) of a function of the parameters $p$ and the data $d$. The data is fixed for a given problem, so it is only by varying $p$ that an optimal solution can be found. The entire MSE function is denoted as $G(p)$.\n\n## Minimization of the Objective Function\n\n$$ \\min_{p \\in \\mathbb{R}^m} \\biggl\\{ \\sum_{i=1}^n (QF_i(\\mathbf{p}) - d_i) ^2\\biggr\\}$$\n\nwhere $d_i$ are the observed data points. This is the same as \n\n$$ \\min_{p \\in \\mathbb{R}^m} \\|G(\\mathbf{p})\\|^2$$\n\nwhere $G(\\mathbf{p}) = QF(\\mathbf{p}) - d$ and $d \\in \\mathbb{R}^n$. We are minimizing the norm of a non-linear function of the parameters. Supposing that we want to find the minimizer, one approach would be by gradient descent. \n\n#### The Jacobian: A quick review\n\n---\n\nThe Jacobian is a multivariate extension of the derivative that extends to functions $f : \\mathbb{R}^m \\to \\mathbb{R}^n$. Because there are $n$ function outputs and $m$ input variables, the Jacobian is an $n \\times m$ matrix that contains the information of how each of the $n$ functions changes with respect to each of the $m$ variables. In an abuse of Leibniz's notation, it can be seen as:\n\n$$\n\\frac{\\partial \\vec{f}}{\\partial \\vec{x}} = \\mathbf{J_f} =\n\\left[\n    \\frac{\\partial f}{\\partial x_1} \\cdots \\frac{\\partial f}{\\partial x_n}\n\\right]\n=\n\\begin{bmatrix}\n    \\nabla^\\top f_1\n    \\\\\n    \\vdots\n    \\\\\n    \\nabla^\\top f_m\n\\end{bmatrix}\n=\n\\left[\n    \\begin{array}{ccc}\n    \\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    \\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n    \\end{array}\n\\right]\n$$\n\nNote that like the derivative, the Jacobian is a function of the input variables $\\vec{x}$. The Jacobian is a linear approximation of the function $f$ at a point $x_0$ and can be used to approximate the function at a point $x_0 + \\Delta x$.\n\n$$ f(x_0 + \\Delta x) \\approx f(x_0) + J_f(x_0) \\Delta x$$\n\nNoting that we are applying matrix multiplication using $J_f$ evaluated at $x_0$ and the vector $\\Delta x = \\vec{x} - \\vec{x_0}$. The quantity $J_f(x_0) \\Delta x$ is the directional derivative of the function $f$ at $x_0$ in the direction of $\\Delta x$.\n\n---\n\n\nThe gradient of $\\|G(\\mathbf{p})\\|^2$ can be computed as follows:\n\n$$\\begin{align}\n\\nabla_p \\|G(p)\\|^2 &= \\nabla_p G(p)^T G(p)\\\\\n&= \\sum_{i=1}^n \\nabla_p G_i(p)^2\\\\\n&= \\sum_{i=1}^n 2 G_i(p) \\nabla_p G_i(p)\\\\\n&= 2 J_G(p)^T G(p)\n\\end{align}\n$$\n\nFrom this stage, gradient descent can be applied to find the minimum of the function. However, the function $G(p)$ is non-linear and so the gradient descent method may not converge quickly or the problem may have poor conditioning. The celebrated [Newton's Method](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization#Higher_dimensions) addresses some of these issues, but requires computing the Hessian $\\nabla^2 \\|G(p)\\|^2$ of the function, which can be expensive.\n\nTo demonstrate, the true Hessian of the function is:\n$$\n\\nabla^2 \\|G(p)\\|^2 = 2 J_G(p)^T J_G(p) + 2 \\sum_{i=1}^n G_i(p) \\nabla^2 G_i(p)\n$$\n\nSo we'd have to compute the Hessian $\\nabla^2 G_i(p)$ of each of the $G_i(p)$ functions, of which there are $n$, not good in practice. If we did have this Hessian, the steps with Newton's method would be:\n\n$$ p_{k+1} = p_k - (\\nabla^2 \\|G(p_k)\\|^2)^{-1} \\nabla (\\|G(p_k)\\|^2)$$\n\n## Gauss-Newton Optimization\n\nRather than solve the problem directly with Newton's method, it can be approximated by linearizing inside of the norm and solving the linearized version using the normal equations. We approximate the function\n\n $$G(p) = QF(p) - d \\approx (QF(p_k) - d) + QJ_k(p-p_k)$$\n \n  where $J_k$ is the Jacobian of $F(p)$ at $p_k$.\n\n$$ \\min_{p \\in \\mathbb{R}^m} \\|QF(p_k) - d + QJ_k(p-p_k)\\|^2$$\n\nThen rearranging this we get a form that is a linear least squares problem:\n\n$$ \\begin{align}\n& \\min_{p \\in \\mathbb{R}^m} \\| QJ_kp - (d + QJ_k p_k- QF(p_k) )\\|^2\\\\\n=& \\min_{p \\in \\mathbb{R}^m} \\| Ap - r_k\\|^2\\\\\nA^T A p =& A^T r_k\n\\end{align}\n$$\n\nwhere $A = QJ_k$ and $r_k = d + QJ_k p_k - QF(p_k)$. This is the normal equations for the linear least squares problem. This gives us \n\n$$ \n\\begin{align}\np_{k+1} &= (A^T A)^{-1} A^T (r_k)\\\\\n&= (J_k^T Q^T Q J_k)^{-1} J_k^T Q^T (d + QJ_k p_k - QF(p_k))\\\\\n&= p_k + (J_k^T Q^T Q J_k)^{-1} J_k^T Q^T (d - QF(p_k))\\\\\np_{k+1} &= p_k - (J_k^T Q^T Q J_k)^{-1} J_k^T Q^T (QF(p_k) - d)\\\\\n\\end{align}\n$$\n\nThis could be written in a more tidy and general way, reacalling that $G(p) = QF(p) - d$ and let $J_G(p) = QJ(p)$, then we have:\n\n$$ p_{k+1} = p_k - (J_G(p_k)^TJ_G(p_k))^{-1} J_G(p_k) G(p_k)$$\n\n#### Comparison with Newton's Method\n\nSo this resembles a scaled gradient descent. In Newton's method we have the Hessian, in Gauss-Newton we have the Jacobian of the function. As a comparison:\n\n| **Method**            | **Description**                                           |\n|-----------------------|-----------------------------------------------------------|\n| **Newtonâ€™s Method**    | **Step Update**                                           |\n|                       | $p_{k+1} = p_k - (\\nabla^2 \\|G(p_k)\\|^2)^{-1} \\nabla \\|G(p_k)\\|^2$ |\n|                       | **Scaling Matrix**                                        |\n|                       | $\\nabla^2 \\|G(p_k)\\|^2 = 2 J_G(p_k)^T J_G(p_k) + 2 \\sum_{i=1}^n G_i(p_k) \\nabla^2 G_i(p_k)$ |\n| **Gauss-Newton Method**| **Step Update**                                           |\n|                       | $p_{k+1} = p_k - (J_G(p_k)^T J_G(p_k))^{-1} J_G(p_k)^T G(p_k)$ |\n|                       | **Scaling Matrix**                                        |\n|                       | $J_G(p_k)^T J_G(p_k)$                                 |\n\nThe direction of change between iterations in Newton's method can be rewritten as $$d_k = \\left(J_G(p_k)^T J(p_k) + \\sum_{i=1}^n G_i(p_k) \\nabla^2 G_i(p_k)\\right)^{-1} J_G(p_k)^T G(p_k)$$\n\nWhile the direction in the case of Gauss-Newton is \n$$d_k = \\left(J_G(p_k)^T J_G(p_k)\\right)^{-1} J_G(p_k)^T G(p_k)$$\n\nThe difference between the two is the omission of the computationally expensive $\\sum_{i=1}^n G_i(p) \\nabla^2 G_i(p)$ terms. The Gauss-Newton method is approximating the second-order approach of Newton's method by only considering the first-order terms inside of the norm.\n\n$$J_G(p_k)^T J(p_k) \\sum_{i=1}^n G_i(p_k) \\nabla^2 G_i(p_k) \\approx J_G(p_k)^T J(p_k)$$\n\nRecall that $G(p) = QF(p) - d$ which is the difference between the observed data and the model. If the difference is small then $G_i$ is also small and the approximation is good.\n\n### Algorithm for Gauss-Newton\n\nWe have derived the algorithm for the Gauss-Newton method for solving the non-linear least squares problem. The algorithm is as follows:\n\n```pseudocode\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"//\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n#| pdf-placement: \"htb!\"\n#| pdf-line-number: true\n\n\\begin{algorithm}\n\\caption{Gauss-Newton Algorithm for Non-linear Least Squares}\n\\begin{algorithmic}[1]\n\\State \\textbf{Input:} Initial guess $p_0$, maximum iterations $K$, tolerance $\\epsilon$\n\\State \\textbf{Initialize} $p_0$\n\\For{$k = 0, 1, 2, \\ldots$}\n    \\State Compute the Jacobian $J_G$ of $G(p)$ at $p_k$\n    \\State Compute the transpose $J_G^T$ of the Jacobian\n    \\State Compute the residual $r_k =G(p_k)$ (forward model)\n    \\State Compute the step $s_k = (J_G(p_k)^T J_G(p_k) )^{-1} J_G(p_k)^T r_k$\n    \\State Update the parameters $p_{k+1} = p_k + \\mu_k s_k$\n    \\If{$\\|s_k\\| < \\epsilon$}\n        \\State \\textbf{Stop}\n    \\EndIf\n\\EndFor\n\\State \\textbf{Output:} $p_{k+1}$ as the optimal solution\n\\end{algorithmic}\n\\end{algorithm}\n```\n\n### Matrix Inversions\n\nIn practice it may be computationally expensive to invert the matrix $J_k^T Q^T Q J_k$. We can use a conjugate gradient method to solve the normal equations instead. \n$$J_k^T Q^T Q J_k s_k = J_k^T Q^T r_k$$\n\nWe developed a conjugate gradient method in the last lecture, so we can use that along with the computed values for $J_k^T, J_k, r_k$ to solve the normal equations and get the step $s_k$.\n\n### Summary\n\n| **Component**        | **Description**                   | **Dimensions**              |\n|----------------------|-----------------------------------|-----------------------------|\n| $d$              | Observed data                     | $\\mathbb{R}^{n}$         |\n| $p_k$            | Parameters                        | $\\mathbb{R}^{m}$         |\n| $Q$              | Weight matrix                     | $\\mathbb{R}^{n \\times n}$|\n| $J_k$            | Jacobian of $F(p_k)$          | $\\mathbb{R}^{n \\times m}$|\n| $r_k$            | Residual $d - QF(p_k)$        | $\\mathbb{R}^{n}$         |\n| $F(p_k)$         | Forward model output              | $\\mathbb{R}^{n}$         |\n| $s_k$            | Step direction                    | $\\mathbb{R}^{m}$         |\n| $J_k^T$          | Transpose of the Jacobian         | $\\mathbb{R}^{m \\times n}$|\n| $J_k^T Q^T Q J_k$ | Normal equations matrix           | $\\mathbb{R}^{m \\times m}$|\n\n",
    "supporting": [
      "index_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}