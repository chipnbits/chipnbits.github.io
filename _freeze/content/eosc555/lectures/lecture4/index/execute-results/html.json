{
  "hash": "148035a864f95a889c0f0e833c4604ff",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Lecture 4\"\nsubtitle: \"Regularization and the Conjugate Gradient Methods\"\ndate: 2024-09-20\nauthor: \"Simon Ghyselincks\"\ndescription: >-\n    Tikhonov regularization is a common technique used in inverse theory to stabilize ill-posed problems. In this lecture, we derive the Tikhonov regularization technique, we also have a look at a least squares solution that does not require the computation of the full SVD of the matrix $A$, using the conjugate gradient method.\ncategories:\n  - Optimization\n  - Inverse Theory\n  - Python\n\nimage: imgs/tikhonov_regularization.png\ndraft: false\n\neditor: \n  render-on-save: false\n---\n\n\n\n\n::: {.hidden}\n$$\n\\def\\argmin{\\operatorname*{argmin}}\n\\def\\bmat#1{\\begin{bmatrix}#1\\end{bmatrix}}\n\\def\\Diag{\\mathbf{Diag}}\n\\def\\ip#1{\\langle #1 \\rangle}\n\n\\def\\maximize#1{\\displaystyle\\maxim_{#1}}\n\n\\def\\minimize#1{\\displaystyle\\minim_{#1}}\n\\def\\norm#1{\\|#1\\|}\n\\def\\proj{\\mathbf{proj}}\n\\def\\R{\\mathbb R}\n\\def\\Re{\\mathbb R}\n\\def\\Rn{\\R^n}\n\\def\\rank{\\mathbf{rank}}\n\\def\\range{{\\mathbf{range}}}\n\\def\\span{{\\mathbf{span}}}\n\\def\\textt#1{\\quad\\text{#1}\\quad}\n\\def\\trace{\\mathbf{trace}}\n\\def\\bf#1{\\mathbf{#1}}\n$$\n:::\n\n\n\n## Tikhnov Regularization\n\nWe have looked at the least squares formulation for solving inverse problems:\n\n$$ \\min \\frac{1}{2} \\norm{A x - b}^2 $$\n\nwhere $A \\in \\R^{m \\times n}$ is a linear operator, $x \\in \\R^n$ is the unknown model, and $b \\in \\R^m$ is the data.\n\nThe least squares problem is often ill-posed, meaning that the solution is not unique or stable. If there are more unknowns than equations, such as the case when $n > m$, then the problem is underdetermined and there are infinitely many solutions.\n\nWe can return to unique solutions by adding a regularization term to the selection of the $x$ that we want to minimize. The Tikhonov regularization technique adds a penalty term to the least squares problem:\n\n$$ \\min \\frac{1}{2} \\norm{A x - b}^2 + \\frac{1}{2}  \\lambda \\norm{Lx}^2 $$\n\nwhere $L \\in \\R^{n \\times n}$ is a regularization matrix. The regularization matrix $L$ is often chosen to be the identity matrix, but other choices are possible. \n\n#### Uniqueness\n\nTo check the uniqueness of the solution, we can rewrite the problem as a quadratic form:\n\n$$ \\min \\frac{1}{2} x^T A^T A x - b^T A x + \\frac{1}{2} \\lambda x^T L^T L x $$\n$$ = \\min \\frac{1}{2} x^T H x - b^T A x + \\frac{1}{2}\\norm{b}^2$$\n \nwhere $H = A^T A + \\lambda L^T L$ is the Hessian matrix which is symmetric and positive semi-definite by spectral theorem. If we choose an appropriate $\\lambda$, then the Hessian matrix is positive definite and the problem is well-posed. In the case where $L=I$, the Hessian becomes full rank for $\\lambda > 0$ and the problem is well-posed. The quality that $H \\succ 0$ means that the matrix is invertible.\n\n#### Solution\n\nThe unique solution is given by by the first order optimatility condition:\n\n$$ \\begin{align}\n(A^T A + \\lambda L^T L) \\bf{x}_{\\text{RLS}} - A^T b&= 0 \\\\ \n\\bf{x}_{\\text{RLS}} &= (A^T A + \\lambda L^T L)^{-1} A^T b\n\\end{align} \n$$\n\n#### SVD Decomposition\n\nThe solution can be written in terms of the singular value decomposition of $A$, and with the assumption that $L=I$:\n\n$$ \\begin{align}\nA &= U \\Sigma V^T \\\\\nA^T A &= V \\Sigma^T \\Sigma V^T \\\\\n\\bf{x}_{\\text{RLS}} &= \\left( V \\Sigma^2 V^T + \\lambda I \\right)^{-1} V \\Sigma^T U^T b \\\\\n&= \\left( V \\Sigma^2 V^T + \\lambda I V V^T \\right)^{-1} V \\Sigma^T U^T b\\\\\n&= V \\left( \\Sigma^2 + \\lambda I \\right)^{-1} \\Sigma^T U^T b\\\\\n&= V \\Diag \\left( \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda} \\right) U^T b\\\\\n&= \\sum _i ^ n \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda} \\ip{u_i, b} v_i\n\\end{align}\n$$\n\nThis form is more readily comparable to some of the other methods that we have see so far, which are presented in the table below:\n\n## Comparison of Least Squares Methods\n\n| Method | Solution |\n| --- | --- | \n| Tikhonov | $x_{\\text{RLS}} = \\sum _i ^ n \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda} \\ip{u_i, b} v_i$ | $L=I$ |\n| Thresholded SVD | $x_{\\text{TSVD}} = \\sum _i ^ n h(\\sigma_i) \\ip{u_i, b} v_i$ | $L=I$ |\n| Gradient Flow | $x_{\\text{SDF}} = \\sum _i ^ n \\frac{\\exp(-\\sigma_i^2 t) - 1}{\\sigma_i} \\ip{u_i, b} v_i$ | $L=I$ |\n\nAs we can see all three methods have a similar form and offer some mechanism for controlling the noise induced by the small singular values of $A$.\n\n::: {#cell-comp-plot .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef generate_ill_conditioned_matrix(m, n, condition_number):   \n    # Generate random orthogonal matrices U and V\n    U, _ = np.linalg.qr(np.random.randn(m, m))\n    V, _ = np.linalg.qr(np.random.randn(n, n))\n    \n    sigma = np.linspace(1, 1/condition_number, min(m, n))    \n    Sigma = np.diag(sigma)    \n    A = U @ Sigma @ V[:min(m, n), :]\n    \n    return A, sigma\n\n# Seed for reproducibility\nnp.random.seed(4)\nA, S = generate_ill_conditioned_matrix(8, 24, 1e3)\n\n# Create a vector b of size 5 with random values\nb = np.random.randn(8)\n\n# Compute the SVD of A\nU, S, Vt = np.linalg.svd(A, full_matrices=False)\nV = Vt.T\nU = U  # Already in proper shape\n\n# Number of singular values\nn = len(S)\n\n# Define parameters for each method\n# Gradient Flow\nt_values = np.linspace(0, 0.6, 100)\n\n# Tikhonov Regularization\nlambda_values = np.linspace(1e-4, 1, 100)\n\n# Thresholded SVD\nthreshold_values = np.linspace(0, max(S), 100)\n\n# Compute scaling factors for each method\n# Gradient Flow Scaling\ndef gradient_flow_scaling(sigma, t):\n    return (1 - np.exp(-sigma**2 * t)) / sigma\n\ngradient_scalings = np.array([gradient_flow_scaling(s, t_values) for s in S])\n\n# Tikhonov Scaling\ndef tikhonov_scaling(sigma, lambd):\n    return sigma / (sigma**2 + lambd)\n\ntikhonov_scalings = np.array([tikhonov_scaling(s, lambda_values) for s in S])\n\n# Thresholded SVD Scaling\ndef tsvd_scaling(sigma, threshold):\n    return np.where(sigma >= threshold, 1/sigma, 0)\n\ntsvd_scalings = np.array([tsvd_scaling(s, threshold_values) for s in S])\n\n# Initialize the plot with 3 subplots\nfig, axes = plt.subplots(3, 1, figsize=(5, 15))\n\n# Define a color palette\npalette = sns.color_palette(\"husl\", n)\n\n# Plot Gradient Flow\nax = axes[0]\nfor i in range(n):\n    ax.plot(t_values, gradient_scalings[i], color=palette[i], linewidth=2, label=f'$1/\\sigma_{i}$' )\n    ax.axhline(y=1/S[i], color=palette[i], linestyle='--', linewidth=1)\nax.set_yscale('log')\nax.set_xlabel('Time (t)', fontsize=14)\nax.set_ylabel('Scaling Factor', fontsize=14)\nax.set_title('Gradient Flow', fontsize=16)\nax.legend()\nax.grid(True)\n\n# Plot Tikhonov Regularization\nax = axes[1]\nfor i in range(n):\n    ax.plot(lambda_values, tikhonov_scalings[i], color=palette[i], linewidth=2, label=f'$1/\\sigma_{i}$' )\n    ax.axhline(y=1/S[i], color=palette[i], linestyle='--', linewidth=1)\nax.set_yscale('log')\nax.set_xlabel('Regularization Parameter (λ)', fontsize=14)\nax.set_ylabel('Scaling Factor', fontsize=14)\nax.set_title('Tikhonov Regularization', fontsize=16)\nax.legend()\nax.grid(True)\n\n# Plot Thresholded SVD\nax = axes[2]\nfor i in range(n):\n    ax.plot(threshold_values, tsvd_scalings[i], color=palette[i], linewidth=2, label=f'$1/\\sigma_{i}$')\n    ax.axhline(y=1/S[i], color=palette[i], linestyle='--', linewidth=1)\nax.set_yscale('log')\nax.set_xlabel('Threshold (τ)', fontsize=14)\nax.set_ylabel('Scaling Factor', fontsize=14)\nax.set_title('Thresholded SVD', fontsize=16)\nax.legend()\nax.grid(True)\n\n# Adjust layout and add a legend\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Evolution of scaling factors for three different methods](index_files/figure-html/comp-plot-output-1.png){#comp-plot width=469 height=1430}\n:::\n:::\n\n\n## Solving Least Squares Using Conjugate Gradient\n\nA detailed explanation of this method can be found at [Wikipedia](https://en.wikipedia.org/wiki/Conjugate_gradient_method)\n\n#### Conjugate Vectors Definition\n\nA set of vectors $\\{ p_1, p_2, \\ldots, p_n \\}$ is said to be **conjugate with respect to** a matrix $A$ if:\n\n$$\n\\langle p_i, A p_j \\rangle = 0 \\quad \\text{for all } i \\neq j\n$$\n\nThis is a generalization of the concept of orthoganality to non-symmetric matrices.\n\n**Standard Orthogonality:** When $ A = I $ (the identity matrix), the definition reduces to the standard concept of orthogonality. For a symmetric $A$ we also have an orthogonal decomposition of eigenvectors by spectral theorem.\n\n---\n\nBack to the problem of least squares, we can express the solution $ x $ as a linear combination of conjugate vectors:\n\n$$\nx = x_0 + \\sum_{i=1}^n \\alpha_i p_i\n$$\n\nwhere:\n\n- $x_0$ is an initial guess (can be zero).\n- $\\alpha_i$ are scalar coefficients.\n- $p_i$ are conjugate vectors with respect to $A$.\n\nTo recover the coefficients of $\\alpha_i$ we can use a projection in the weighted space of $A$:\n\n$$ \\begin{align}\nA x_0 + \\sum_{i=1}^n \\alpha_i A p_i &= b\\\\\nr &= b - A x_0\\\\\n\\sum_{i=1}^n \\alpha_i A p_i &= r\\\\\n\\ip{p_i, \\sum_{i=1}^n \\alpha_i A p_i} &= \\ip{p_i, r}\\\\\n\\alpha_i \\ip{p_i, A p_i} &= \\ip{p_i, r}\\\\\n\\alpha_i &= \\frac{\\ip{p_i, r}}{\\ip{p_i, A p_i}}\n\\end{align} \n$$\nIn the case where $x_0$ is zero, then this reduces to\n$$ \\alpha_i = \\frac{\\ip{p_i, b}}{\\ip{p_i, A p_i}} $$\n\n### Algorithm Steps\n\n**Initialize:**\n\n- $x = x_0$\n- $r_0 = b - A x_0$\n- $p_0 = r_0$\n\n**For $i = 0,1, 2, \\ldots$:**\n\n1. **Compute $\\alpha_i$:**\n\n   $$\n   \\alpha_i = \\frac{\\langle r_i, r_i \\rangle}{\\langle p_i, A p_i \\rangle}\n   $$\n\n2. **Update Solution $x$:**\n\n   $$\n   x_{i+1} = x_{i} + \\alpha_i p_i\n   $$\n\n3. **Update Residual $r$:**\n\n   $$\n   r_{i+1} = r_{i} - \\alpha_i A p_i\n   $$\n\n4. **Check for Convergence:**\n\n   - If $\\| r_{i+1} \\|$ is small enough, stop.\n\n5. **Compute $\\beta_i$:**\n\n   $$\n   \\beta_i = \\frac{\\langle r_{i+1}, r_{i+1}\\rangle}{\\langle r_i,r_i \\rangle}\n   $$\n\n6. **Update Conjugate Direction $p_{i+1}$:**\n\n   $$\n   p_{i+1} = r_{i+1} + \\beta_i p_i\n   $$\n\n---\n\nThe method can be seen better if we trace through the minimization problem for fixed $x$ and with variable $\\alpha$:\n\n$$\n\\begin{align}\n& \\min \\frac{1}{2} \\norm{A (x+\\alpha p) - b}^2  \\\\\n&= \\frac{1}{2}r^T r + \\alpha \\ip{r, A p} + \\frac{1}{2} \\alpha^2 \\ip{p, A^T A p} \\\\ \n0 &= \\ip{r, A p} + \\alpha \\ip{p, A^T A p} \\\\\n\\alpha &= -\\frac{\\ip{r, A p}}{\\norm{A p}^2}\n\\end{align}\n$$\n\nBut we can also trace this through using the expansion of lest squares and removing the $\\norm{b}^2$ term:\n\n$$\n\\begin{align}\n& \\min \\frac{1}{2} \\tilde x^T A x - \\tilde x^T b  \\\\\n&= \\frac{1}{2} \\left( x^T A x + 2 \\alpha x^T A p + \\alpha^2 p^T A p \\right) - x^T b - \\alpha p^T b\\\\\n0&= x^TAp + \\alpha p^T A p - p^T b \\\\\n\\alpha &= \\frac{p^T (Ax-b)}{p^T A p}\n\\end {align}\n$$\n\n",
    "supporting": [
      "index_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}