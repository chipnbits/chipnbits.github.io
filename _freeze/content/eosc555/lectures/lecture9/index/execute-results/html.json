{
  "hash": "29275716c15776aee514bf6d6cdd7992",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Lecture 9: Machine Learning and Neural Networks\"\nsubtitle: \"A mathematical approach.\"\ndate: 2024-11-05\nauthor: \"Simon Ghyselincks\"\ndescription: >-\n    Neural networks have revolutionized the field of machine learning, but how exactly do they work? In this lecture, we will explore the basic structure of these models from a mathematical perspective. We will also discuss the role of regularization and priors in solving inverse problems.\ncategories:\n  - Machine Learning\n  - Neural Networks\n  \n# image: \"imgs/gaussian_homotopy.gif\"\ndraft: false\n\neditor: \n  render-on-save: false\n\nfilters:\n  - pseudocode\n  - diagram\n\npseudocode:\n  caption-prefix: \"Algorithm\"\n  reference-prefix: \"Algorithm\"\n  caption-number: true\n---\n\n\n\n::: {.hidden}\n$$\n\\def\\argmin{\\operatorname*{argmin}}\n\\def\\bmat#1{\\begin{bmatrix}#1\\end{bmatrix}}\n\\def\\Diag{\\mathbf{Diag}}\n\\def\\ip#1{\\langle #1 \\rangle}\n\n\\def\\maximize#1{\\displaystyle\\maxim_{#1}}\n\n\\def\\minimize#1{\\displaystyle\\minim_{#1}}\n\\def\\norm#1{\\|#1\\|}\n\\def\\proj{\\mathbf{proj}}\n\\def\\R{\\mathbb R}\n\\def\\Re{\\mathbb R}\n\\def\\Rn{\\R^n}\n\\def\\rank{\\mathbf{rank}}\n\\def\\range{{\\mathbf{range}}}\n\\def\\span{{\\mathbf{span}}}\n\\def\\textt#1{\\quad\\text{#1}\\quad}\n\\def\\trace{\\mathbf{trace}}\n\\def\\bf#1{\\mathbf{#1}}\n$$\n:::\n\n\n\n## Motivation\n\nIn the previous lecture, we viewed different priors or regularizers and how they can be used to help solve inverse problems. A regularizer for least squares in the most general sense is given as:\n\n$$ \\min_{u} \\left\\{ \\frac{1}{2} \\left\\| Au(x) - b \\right\\|_2^2 + \\lambda R(u) \\right\\} $$\n\nwhere $u(x)$ is a distribution of the unknowns over the domain $x$, $A$ is the forward operator, $b$ is the data, and $R(u)$ is the regularizer. A neural network can be used as a universal approximator for the function $R: \\mathbb{R}^n \\rightarrow \\mathbb{R}$, where $n$ is the number of unknowns, or values of $u$.\n\n## Neural Networks\n\n### A Basic Neural Network: Single-Layer Perceptron (SLP)\n\nA basic neural network will have parameters $\\theta$ that can be trained or learned, along with the input, $u$.\n\n$$y = R(u; \\theta) = w^T \\sigma(Wu+a), \\quad \\theta := \\{w, W, a\\}$$\n\nThe function $R$ in this case is a function defined for fixed $\\theta$. The term $\\sigma$ is a non-linear activation function, of which there are many choices. \n\n- **$u$**: Input vector to the neural network.\n  \n- **$y$**: Output of the neural network, parameterized by $\\theta$, representing the learned function.\n\n- **$\\theta := \\{w, W, a\\}$**: Set of trainable parameters in the network, where:\n  - **$w$**: Weight vector for the output layer\n  - **$W$**: Weights matrix for the hidden layer\n  - **$a$**: Bias vector added to the hidden layer\n\n- **$\\sigma$**: Non-linear activation function applied element-wise to the affine transformation $Wu + a$. \n\nSo a single layer neural network can be seen as the affine transformation of the vector $u$ followed by a non-linear activation function and a weighting metric for the resultant vector.\n\nThis can be used as an approximator for the true regularizer $R(u) \\approx R_T(u)$ in the inverse problem.\n\nSuppose that we have a known set of mappings $u_i \\rightarrow y_i$, where $i = 1, \\ldots, N$. For example we might have some information about the regularizer $R(u)$ for a set of $u$ values. One possible technique is to train an SLP to approximate the true regularizer $R_T(u)$.\n\nThe function $y = R(u; \\theta)$ returns a scalar, taking its transpose will not change the output:\n\n$$y = w^T \\sigma(Wu+a) = \\sigma(u^TW + a)w$$\n\nThen using the squared loss function, we can define the loss function as:\n\n$$\\mathcal{L}(\\theta) = \\frac{1}{2} \\sum_{i=1}^N \\left \\| \\sigma(u^TW + a)w - y_i \\right \\|^2$$\n\nThe summation is reorganized to get rid of the summation term where $U$ is a matrix with the $u_i^T$ as the columns, A is a matrix with $a$ as the columns, and $y$ is the vector of $y_i$ values.\n\n$$\\mathcal{L}(\\theta) = \\frac{1}{2} \\left \\| \\sigma(U^TW + A)w - y \\right \\|^2$$\n\nFor simplicity of this analysis, we can assume without loss of generality for the problem at hand that $A = 0$ and $\\sigma$ is the identity operator. Then:\n\n$$\\hat\\theta = \\min_{\\theta} \\mathcal{L}(\\theta) = \\min_{\\hat w} \\frac{1}{2} \\left \\| U^T\\hat w - y \\right \\|^2.$$\n\nwhere $\\hat w = Ww$.\n\n### Non-linearity Analysis\n\nThis least squares problem will generally be ill-posed when the activation function is not present (the case with identity activation). $N>d$ means that there are more equations than there are unknowns, because $\\hat w$ is of dimension $d$, so there could be infinite solutions.\n\n$$\\hat{\\theta} = \\min_{\\theta} \\frac{1}{2} \\left\\|\n\\underbrace{\n\\begin{bmatrix}\n\\ & \\ & \\ \\\\\n\\ & U^T & \\ \\\\\n\\ & \\ & \\ \\\\\n\\end{bmatrix}\n}_{N \\times d}\n\\cdot\n\\underbrace{\n\\begin{bmatrix}\n\\ & \\ & \\ \\\\\n\\ & W & \\ \\\\\n\\ & \\ & \\ \\\\\n\\end{bmatrix}\n}_{N \\times k}\n- y \\right\\|^2\n$$\n\n\n\n**Idea 1:**\n\nIf we can increase the rank of the $Z = U^TW$ matrix, then perhaps it is possible to solve the problem batter. We select for there to be a larger weights matrix $W$ that is $N \\times m$ where $m > d$. In the resulting $z = U^TW$ matrix, the rank will still be $\\text{rank}(Z) \\le d$.\n\n**Idea 2:**\n\nUse a non-linear activation function $\\sigma$ that operates element-wise on the matrix $Z = U^TW$ to increase the rank of the matrix so that $\\text{rank}(\\sigma(Z)) = \\min (N,m)$.\n\nIn practice the exact activation function is not important. It may be the case that $\\text{rank}(\\sigma(Z)) = 3$ for example, but applying the activation function will increase the rank to the minimum dimension size of the weights matrix $W$. This can give a unique solution the least squares problem.\n\n$$\n\\hat{\\theta} = \\min_{\\theta} \\frac{1}{2} \\left\\|\n\\sigma \\left( \\underbrace{\n\\begin{bmatrix} \n\\ & \\ & \\ \\\\\n\\ & U^T & \\ \\\\\n\\ & \\ & \\ \\\\\n\\end{bmatrix}\n}_{N \\times d}\n\\cdot\n\\underbrace{\n\\begin{bmatrix}\n\\ & \\ & \\ & \\ & \\cdots & \\ \\\\\n\\ & W & \\ & \\ & \\ & \\ \\\\\n\\ & \\ & \\ & \\ & \\ & \\ \\\\\n\\end{bmatrix}\n}_{N \\times m}\n\\right ) w\n-\ny \\right\\|^2\n$$\n\n#### Non-linear Example\n\nTo illustrate the rank recovery property and the improvement for finding a unique solution to the least squares problem, we consider a simple example below.\n\n::: {#fig-nonlinear-rank-recovery .cell fig-width='3' layout-ncol='2' execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.linalg import lstsq, matrix_rank\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Parameters\nN = 10  # Number of samples\nd = 5   # Dimension of input u\nm = 10  # Increased dimension for W\n\n# Generate random input data U (d x N)\nU = np.random.randn(d, N)\n\n# True weight matrix W_true (d x d)\nW_true = np.random.randn(d, d)\nw_true = np.random.randn(d)\n\n# Generate nonlinear output to test with\ny_linear = (np.cos(U.T @ W_true)) @ w_true\n\n# Initialize random model weight matrix W (d x m)\nW = np.random.randn(d, m)\nZ = U.T @ W\nrank_Z = matrix_rank(Z)\n\nsigma = np.sin\nZ_nonlinear = sigma(Z)\nrank_Z_nl = matrix_rank(Z_nonlinear)\n\nw_linear, residuals_linear, _, _ = lstsq(Z, y_linear, rcond=None)\nw_nonlinear, residuals_nl, _, _ = lstsq(Z_nonlinear, y_linear, rcond=None)\n\n# Check reconstruction error for each case\nerror_linear = np.linalg.norm(Z @ w_linear - y_linear)\nerror_nonlinear = np.linalg.norm(Z_nonlinear @ w_nonlinear - y_linear)\n\n# Comparison of Reconstruction Errors\nlabels = ['Linear Least Squares', 'Non-linear Least Squares']\nerrors = [error_linear, error_nonlinear]\n\nplt.figure(figsize=(5,5))\n\nbars = plt.bar(labels, errors, color=['skyblue', 'salmon'])\nplt.ylabel('Reconstruction Error')\n\n# Annotate bars with error values\nfor bar in bars:\n    height = bar.get_height()\n    plt.annotate(f'{height:.4f}',\n                 xy=(bar.get_x() + bar.get_width() / 2, height),\n                 xytext=(0, 3),  # 3 points vertical offset\n                 textcoords=\"offset points\",\n                 ha='center', va='bottom')\n\nplt.ylim(0, max(errors)*1.2)\nplt.show()\n\nplt.figure(figsize=(5,5))\n\nranks = [rank_Z, rank_Z_nl]\nlabels_rank = ['Z (Linear)', 'Z_nonlinear (Non-linear)']\n\nbars_rank = plt.bar(labels_rank, ranks, color=['lightgreen', 'gold'])\nplt.ylabel('Matrix Rank')\n\n# Annotate bars with rank values\nfor bar in bars_rank:\n    height = bar.get_height()\n    plt.annotate(f'{int(height)}',\n                 xy=(bar.get_x() + bar.get_width() / 2, height),\n                 xytext=(0, 3),  # 3 points vertical offset\n                 textcoords=\"offset points\",\n                 ha='center', va='bottom')\n\nplt.ylim(0, m + 1)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Reconstruction Error Comparison](index_files/figure-html/fig-nonlinear-rank-recovery-output-1.png){#fig-nonlinear-rank-recovery-1 width=440 height=411}\n:::\n\n::: {.cell-output .cell-output-display}\n![Matrix Rank Comparison](index_files/figure-html/fig-nonlinear-rank-recovery-output-2.png){#fig-nonlinear-rank-recovery-2 width=436 height=411}\n:::\n\nA comparison least squares with non-linear activation function\n:::\n\n\n#### Notes on Scaling\n\nThe $W$ matrix will scale up in $O(N^2)$ so that with more data samples it can become too large to handle well. The problem however can be solved with a random $w$ and with $W$ alone under these conditions. A lower rank $W$ could help under conditions where the size of $N$ is large. \n\n$$ W = Q Z^T $$\n\nwhere $Q$ is a matrix of orthonormal columns and $Z$ is a matrix of size $d \\times N$. In this case the product $U^TW$ for any particular sample $u_i$ will be giben by $\\sigma((u_i^TQ)Z^T)$. This lower rank matrix leads to the topic of convolutional neural networks (CNNs) which make extensive use of a reduced rank matrix.\n\n",
    "supporting": [
      "index_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}