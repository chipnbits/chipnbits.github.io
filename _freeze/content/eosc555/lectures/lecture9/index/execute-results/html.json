{
  "hash": "153bd229909667e088a44406e0aa231a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Lecture 9\"\nsubtitle: \"Machine Learning and Neural Networks\"\ndate: 2024-11-05\nauthor: \"Simon Ghyselincks\"\ndescription: >-\n    Neural networks have revolutionized the field of machine learning, but how exactly do they work? In this lecture, we will explore the basic structure of these models from a mathematical perspective. We will also discuss the role of regularization and priors in solving inverse problems.\ncategories:\n  - Machine Learning\n  - Neural Networks\n  \n# image: \"imgs/gaussian_homotopy.gif\"\ndraft: false\n\neditor: \n  render-on-save: false\n\nfilters:\n  - pseudocode\n  - diagram\n\npseudocode:\n  caption-prefix: \"Algorithm\"\n  reference-prefix: \"Algorithm\"\n  caption-number: true\n---\n\n\n\n::: {.hidden}\n$$\n\\def\\argmin{\\operatorname*{argmin}}\n\\def\\bmat#1{\\begin{bmatrix}#1\\end{bmatrix}}\n\\def\\Diag{\\mathbf{Diag}}\n\\def\\ip#1{\\langle #1 \\rangle}\n\n\\def\\maximize#1{\\displaystyle\\maxim_{#1}}\n\n\\def\\minimize#1{\\displaystyle\\minim_{#1}}\n\\def\\norm#1{\\|#1\\|}\n\\def\\proj{\\mathbf{proj}}\n\\def\\R{\\mathbb R}\n\\def\\Re{\\mathbb R}\n\\def\\Rn{\\R^n}\n\\def\\rank{\\mathbf{rank}}\n\\def\\range{{\\mathbf{range}}}\n\\def\\span{{\\mathbf{span}}}\n\\def\\textt#1{\\quad\\text{#1}\\quad}\n\\def\\trace{\\mathbf{trace}}\n\\def\\bf#1{\\mathbf{#1}}\n$$\n:::\n\n\n\n## Motivation\n\nIn the previous lecture, we viewed different priors or regularizers and how they can be used to help solve inverse problems. A regularizer for least squares in the most general sense is given as:\n\n$$ \\min_{u} \\left\\{ \\frac{1}{2} \\left\\| Au(x) - b \\right\\|_2^2 + \\lambda R(u) \\right\\} $$\n\nwhere $u(x)$ is a distribution of the unknowns over the domain $x$, $A$ is the forward operator, $b$ is the data, and $R(u)$ is the regularizer. A neural network can be used as a universal approximator for the function $R: \\mathbb{R}^n \\rightarrow \\mathbb{R}$, where $n$ is the number of unknowns, or values of $u$.\n\n## Neural Networks\n\n### A Basic Neural Network: Single-Layer Perceptron (SLP)\n\nA basic neural network will have parameters $\\theta$ that can be trained or learned, along with the input, $u$.\n\n$$y = R(u; \\theta) = w^T \\sigma(Wu+a), \\quad \\theta := \\{w, W, a\\}$$\n\nThe function $R$ in this case is a function defined for fixed $\\theta$. The term $\\sigma$ is a non-linear activation function, of which there are many choices. \n\n- **$u$**: Input vector to the neural network.\n  \n- **$y$**: Output of the neural network, parameterized by $\\theta$, representing the learned function.\n\n- **$\\theta := \\{w, W, a\\}$**: Set of trainable parameters in the network, where:\n  - **$w$**: Weight vector for the output layer\n  - **$W$**: Weights matrix for the hidden layer\n  - **$a$**: Bias vector added to the hidden layer\n\n- **$\\sigma$**: Non-linear activation function applied element-wise to the affine transformation $Wu + a$. \n\nSo a single layer neural network can be seen as the affine transformation of the vector $u$ followed by a non-linear activation function and a weighting metric for the resultant vector.\n\nThis can be used as an approximator for the true regularizer $R(u) \\approx R_T(u)$ in the inverse problem.\n\nSuppose that we have a known set of mappings $u_i \\rightarrow y_i$, where $i = 1, \\ldots, N$. For example we might have some information about the regularizer $R(u)$ for a set of $u$ values. One possible technique is to train an SLP to approximate the true regularizer $R_T(u)$.\n\nThe function $y = R(u; \\theta)$ returns a scalar, taking its transpose will not change the output:\n\n$$y = w^T \\sigma(Wu+a) = \\sigma(u^TW + a)w$$\n\nThen using the squared loss function, we can define the loss function as:\n\n$$\\mathcal{L}(\\theta) = \\frac{1}{2} \\sum_{i=1}^N \\left \\| \\sigma(u^TW + a)w - y_i \\right \\|^2$$\n\nThe summation is reorganized to get rid of the summation term where $U$ is a matrix with the $u_i^T$ as the columns, A is a matrix with $a$ as the columns, and $y$ is the vector of $y_i$ values.\n\n$$\\mathcal{L}(\\theta) = \\frac{1}{2} \\left \\| \\sigma(U^TW + A)w - y \\right \\|^2$$\n\nFor simplicity of this analysis, we can assume without loss of generality for the problem at hand that $A = 0$ and $\\sigma$ is the identity operator. Then:\n\n$$\\hat\\theta = \\min_{\\theta} \\mathcal{L}(\\theta) = \\min_{\\hat w} \\frac{1}{2} \\left \\| U^T\\hat w - y \\right \\|^2.$$\n\nwhere $\\hat w = Ww$.\n\n### Non-linearity Analysis\n\nThis least squares problem will generally be ill-posed when the activation function is not present (the case with identity activation). $N>d$ means that there are more equations than there are unknowns, because $\\hat w$ is of dimension $d$, so there could be infinite solutions.\n\n$$\\hat{\\theta} = \\min_{\\theta} \\frac{1}{2} \\left\\|\n\\underbrace{\n\\begin{bmatrix}\n\\ & \\ & \\ \\\\\n\\ & U^T & \\ \\\\\n\\ & \\ & \\ \\\\\n\\end{bmatrix}\n}_{N \\times d}\n\\cdot\n\\underbrace{\n\\begin{bmatrix}\n\\ & \\ & \\ \\\\\n\\ & W & \\ \\\\\n\\ & \\ & \\ \\\\\n\\end{bmatrix}\n}_{N \\times k}\n- y \\right\\|^2\n$$\n\n\n\n**Idea 1:**\n\nIf we can increase the rank of the $Z = U^TW$ matrix, then perhaps it is possible to solve the problem batter. We select for there to be a larger weights matrix $W$ that is $N \\times m$ where $m > d$. In the resulting $z = U^TW$ matrix, the rank will still be $\\text{rank}(Z) \\le d$.\n\n**Idea 2:**\n\nUse a non-linear activation function $\\sigma$ that operates element-wise on the matrix $Z = U^TW$ to increase the rank of the matrix so that $\\text{rank}(\\sigma(Z)) = \\min (N,m)$.\n\nIn practice the exact activation function is not important. It may be the case that $\\text{rank}(\\sigma(Z)) = 3$ for example, but applying the activation function will increase the rank to the minimum dimension size of the weights matrix $W$. This can give a unique solution the least squares problem.\n\n$$\n\\hat{\\theta} = \\min_{\\theta} \\frac{1}{2} \\left\\|\n\\sigma \\left( \\underbrace{\n\\begin{bmatrix} \n\\ & \\ & \\ \\\\\n\\ & U^T & \\ \\\\\n\\ & \\ & \\ \\\\\n\\end{bmatrix}\n}_{N \\times d}\n\\cdot\n\\underbrace{\n\\begin{bmatrix}\n\\ & \\ & \\ & \\ & \\cdots & \\ \\\\\n\\ & W & \\ & \\ & \\ & \\ \\\\\n\\ & \\ & \\ & \\ & \\ & \\ \\\\\n\\end{bmatrix}\n}_{N \\times m}\n\\right ) w\n-\ny \\right\\|^2\n$$\n\n#### Non-linear Example\n\nTo illustrate the rank recovery property and the improvement for finding a unique solution to the least squares problem, we consider a simple example below.\n\n::: {#fig-nonlinear-rank-recovery .cell fig-width='3' layout-ncol='2' execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.linalg import lstsq, matrix_rank\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Parameters\nN = 10  # Number of samples\nd = 5   # Dimension of input u\nm = 10  # Increased dimension for W\n\n# Generate random input data U (d x N)\nU = np.random.randn(d, N)\n\n# True weight matrix W_true (d x d)\nW_true = np.random.randn(d, d)\nw_true = np.random.randn(d)\n\n# Generate nonlinear output to test with\ny_linear = (np.cos(U.T @ W_true)) @ w_true\n\n# Initialize random model weight matrix W (d x m)\nW = np.random.randn(d, m)\nZ = U.T @ W\nrank_Z = matrix_rank(Z)\n\nsigma = np.sin\nZ_nonlinear = sigma(Z)\nrank_Z_nl = matrix_rank(Z_nonlinear)\n\nw_linear, residuals_linear, _, _ = lstsq(Z, y_linear, rcond=None)\nw_nonlinear, residuals_nl, _, _ = lstsq(Z_nonlinear, y_linear, rcond=None)\n\n# Check reconstruction error for each case\nerror_linear = np.linalg.norm(Z @ w_linear - y_linear)\nerror_nonlinear = np.linalg.norm(Z_nonlinear @ w_nonlinear - y_linear)\n\n# Comparison of Reconstruction Errors\nlabels = ['Linear Least Squares', 'Non-linear Least Squares']\nerrors = [error_linear, error_nonlinear]\n\nplt.figure(figsize=(5,5))\n\nbars = plt.bar(labels, errors, color=['skyblue', 'salmon'])\nplt.ylabel('Reconstruction Error')\n\n# Annotate bars with error values\nfor bar in bars:\n    height = bar.get_height()\n    plt.annotate(f'{height:.4f}',\n                 xy=(bar.get_x() + bar.get_width() / 2, height),\n                 xytext=(0, 3),  # 3 points vertical offset\n                 textcoords=\"offset points\",\n                 ha='center', va='bottom')\n\nplt.ylim(0, max(errors)*1.2)\nplt.show()\n\nplt.figure(figsize=(5,5))\n\nranks = [rank_Z, rank_Z_nl]\nlabels_rank = ['Z (Linear)', 'Z_nonlinear (Non-linear)']\n\nbars_rank = plt.bar(labels_rank, ranks, color=['lightgreen', 'gold'])\nplt.ylabel('Matrix Rank')\n\n# Annotate bars with rank values\nfor bar in bars_rank:\n    height = bar.get_height()\n    plt.annotate(f'{int(height)}',\n                 xy=(bar.get_x() + bar.get_width() / 2, height),\n                 xytext=(0, 3),  # 3 points vertical offset\n                 textcoords=\"offset points\",\n                 ha='center', va='bottom')\n\nplt.ylim(0, m + 1)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Reconstruction Error Comparison](index_files/figure-html/fig-nonlinear-rank-recovery-output-1.png){#fig-nonlinear-rank-recovery-1 width=440 height=411}\n:::\n\n::: {.cell-output .cell-output-display}\n![Matrix Rank Comparison](index_files/figure-html/fig-nonlinear-rank-recovery-output-2.png){#fig-nonlinear-rank-recovery-2 width=436 height=411}\n:::\n\nA comparison least squares with non-linear activation function\n:::\n\n\n#### Notes on Scaling\n\nThe $W$ matrix will scale up in $O(N^2)$ so that with more data samples it can become too large to handle well. The problem however can be solved with a random $w$ and with $W$ alone under these conditions. A lower rank $W$ could help under conditions where the size of $N$ is large. \n\n$$ W = Q Z^T $$\n\nwhere $Q$ is a matrix of orthonormal columns and $Z$ is a matrix of size $d \\times N$. In this case the product $U^TW$ for any particular sample $u_i$ will be giben by $\\sigma((u_i^TQ)Z^T)$. This lower rank matrix leads to the topic of convolutional neural networks (CNNs) which make extensive use of a reduced rank matrix. The benefit is that it can improve the computational speed by exploiting a sparse structure in the matrix $W$.\n\nThis becomes more important when the layers of a SLP are combined into a deep neural network (DNN).  \n\n$$y = R(u; \\theta) = w^T \\sigma(W^{(L)} \\sigma(W^{(L-1)} \\cdots \\sigma(W^{(1)}u + a^{(1)})) + a^{(L-1)}) + a^{(L)}$$\n\nwhere $L$ is the number of layers in the network. This is a chain of affine transformations followed by non-linear activation functions and can be expensive to compute in the case where $N$ is large.\n\n## Convolutional Neural Networks (CNNs)\n\nA convolutional neural network makes use of a matrix operator that produces the same result as a discrete convolution. \n\n### Convolution Operator\n\nThe 1D convolutional operator $\\ast$ in the discrete case is defined as:\n\n$$ (f \\ast g)[n] = \\sum_{m=-\\infty}^{\\infty} f[m]g[n-m] $$\n\nIn the case of a 2D convolution, the operator is defined as:\n\n$$ (f \\ast g)[n,m] = \\sum_{i=-\\infty}^{\\infty} \\sum_{j=-\\infty}^{\\infty} f[i,j]g[n-i,m-j] $$\n\nIt is also an operation defined in the continuous domain as:\n\n$$ (f \\ast g)(x) = \\int_{-\\infty}^{\\infty} f(y)g(x-y)dy $$\n\nThe operation is one that is fundamental to mathematics and shows up in many different applications including, signal processing, image processing, control theory, probability theory, solutions to ordinary and partial differential equations where it is known as the Green's function, and in the solution of integral equations. Another such home that is has found is in deep learning. The convolution has some intuitive properties that make it useful in any system that is linear and time/shift invariant (LTI). \n\n**Properties of Convolution**\n\n1. Linearity: $f \\ast (\\alpha g + \\beta h) = \\alpha f \\ast g + \\beta f \\ast h$\n2. Commutativity: $f \\ast g = g \\ast f$\n3. Associativity: $f \\ast (g \\ast h) = (f \\ast g) \\ast h$\n\nRather than explain convolution at length here, the interested reader is encouraged to look at the [Convolution Wikipedia page](https://en.wikipedia.org/wiki/Convolution) for some excellent properties and visual examples to build intuition. \n\nIn the context of image and data processing, the convolution is closely related to a correlation filter, the two only differe by a rotation of 180 in the convolutional kernel (the function being convolved with the input). This is an important consideration when it comes to working with learned convolutional kernels, since they can be equally interpreted as correlation filters.\n\nAnother important property to know is that the convolution operation has a close relationship with the fourier transform. The convolution in the spatial domain is equivalent to a pointwise multiplication in the frequency domain. This is known as the convolution theorem:\n\n$$ \\mathcal{F}(f \\ast g) = \\mathcal{F}(f) \\cdot \\mathcal{F}(g) $$\n\nWhen it comes to computing large convolutions for two function $f(x)$ and $g(x)$, the convolution theorem can be used to compute the convolution in the frequency domain, which is much faster than the spatial domain.\n\n$$ f \\ast g = \\mathcal{F}^{-1}(\\mathcal{F}(f) \\cdot \\mathcal{F}(g)) $$\n\nFor more details with visual explanations, another good resource is the UBC CPSC 425 course on (Computer Vision)[https://www.cs.ubc.ca/~lsigal/teaching.html] with slides from (Lecture 3b)[https://www.cs.ubc.ca/~lsigal/425_2024W1/101/Lecture3b.pdf] and (Lecture 4)[https://www.cs.ubc.ca/~lsigal/425_2024W1/101/Lecture4.pdf].\n\n### Convolution in CNNs\n\n",
    "supporting": [
      "index_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}